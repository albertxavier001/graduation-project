{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  2.  3.]\n",
      " [ 4.  5. -1.]]\n",
      "Variable containing:\n",
      "-1  2  3\n",
      " 4  5 -1\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      "-1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "-1\n",
      "[torch.FloatTensor of size 6x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "a = Variable(a)\n",
    "a_np = a.data.numpy()\n",
    "row = [0,1]\n",
    "col = [0,2]\n",
    "a_np[row, col] = -1\n",
    "print a_np\n",
    "a = Variable(torch.from_numpy(a_np))\n",
    "print a\n",
    "data = a.view(-1,1)\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 4\n",
      "[torch.LongTensor of size 3]\n",
      "\n",
      "\n",
      " 1\n",
      " 2\n",
      " 4\n",
      "[torch.LongTensor of size 3]\n",
      " \n",
      " 3\n",
      " 4\n",
      " 4\n",
      "[torch.LongTensor of size 3]\n",
      "\n",
      "[2 2 3]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(1,4,3).type(torch.LongTensor)\n",
    "print x\n",
    "y = x.clone()\n",
    "y[0] = 0\n",
    "y = (y+2).clamp(3,4)\n",
    "print x,y\n",
    "a = np.linspace(1,3,3, dtype=np.uint32)\n",
    "print a.clip(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "6\n",
      "[ 6 15]\n",
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n",
      "3.14159265359\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "print a.transpose()\n",
    "print a.size\n",
    "print a.sum(axis=1)\n",
    "print np.diag([1,2,3])\n",
    "print np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        a = torch.zeros(2,3)\n",
    "        self.save_for_backward(input,a)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input,a, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
