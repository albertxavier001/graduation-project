{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, platform, datetime, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "# from torch import functional as F\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as denseinv\n",
    "from scipy import sparse\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import inv as spinv\n",
    "import scipy.misc\n",
    "\n",
    "from myimagefoldereccv import MyImageFolder\n",
    "from mymodel import GradientNet\n",
    "from myargs import Args\n",
    "from myutils import MyUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('debian', 'jessie/sid', '')\n"
     ]
    }
   ],
   "source": [
    "myutils = MyUtils()\n",
    "\n",
    "args = Args()\n",
    "args.arch = \"densenet121\"\n",
    "args.epoches = 500\n",
    "args.epoches_unary_threshold = 0\n",
    "args.image_h = 256\n",
    "args.image_w = 256\n",
    "args.img_extentions = [\"png\"]\n",
    "args.training_thresholds = [250,200,150,50,0,300]\n",
    "args.base_lr = 1\n",
    "args.lr = args.base_lr\n",
    "args.snapshot_interval = 5000\n",
    "args.debug = True\n",
    "\n",
    "\n",
    "# growth_rate = (4*(2**(args.gpu_num)))\n",
    "transition_scale=2\n",
    "pretrained_scale=4\n",
    "growth_rate = 32\n",
    "\n",
    "#######\n",
    "args.test_scene = ['alley_1', 'bamboo_1', 'bandage_1', 'cave_2', 'market_2', 'market_6', 'shaman_2', 'sleeping_1', 'temple_2']\n",
    "gradient=False\n",
    "args.gpu_num = 0\n",
    "#######\n",
    "\n",
    "writer_comment = 'eccv_albedo'\n",
    "\n",
    "\n",
    "offset = 0.\n",
    "if gradient == True: offset = 0.5\n",
    "\n",
    "args.display_interval = 50\n",
    "args.display_curindex = 0\n",
    "\n",
    "system_ = platform.system()\n",
    "system_dist, system_version, _ = platform.dist()\n",
    "if system_ == \"Darwin\": \n",
    "    args.train_dir = '/Volumes/Transcend/dataset/sintel2'\n",
    "    args.pretrained = False\n",
    "    args.image_w, args.image_h = 32, 32\n",
    "elif platform.dist() ==  ('debian', 'jessie/sid', ''):\n",
    "    args.train_dir = '/home/albertxavier/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "elif platform.dist() == ('debian', 'stretch/sid', ''):\n",
    "    args.train_dir = '/home/cad/lwp/workspace/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "\n",
    "if platform.system() == 'Linux': use_gpu = True\n",
    "else: use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "    \n",
    "\n",
    "print(platform.dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset = MyImageFolder(args.train_dir, 'train',\n",
    "                       transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ), random_crop=True, \n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "test_dataset = MyImageFolder(args.train_dir, 'test', \n",
    "                       transforms.Compose(\n",
    "        [transforms.CenterCrop((args.image_h, args.image_w)),\n",
    "         transforms.ToTensor()]\n",
    "    ), random_crop=False,\n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,1,True,num_workers=1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,1,True,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "[Defination](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)\n",
    "* DenseNet-121: num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16)\n",
    "    * First Convolution: 32M -> 16M -> 8M\n",
    "    * every transition: 8M -> 4M -> 2M (downsample 1/2, except the last block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "densenet = models.__dict__[args.arch](pretrained=args.pretrained)\n",
    "\n",
    "for param in densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu: densenet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args.display_curindex = 0\n",
    "args.base_lr = 0.01\n",
    "args.display_interval = 20\n",
    "args.momentum = 0.9\n",
    "args.epoches = int(60*4)\n",
    "#args.training_thresholds = 240//4\n",
    "args.power = 0.5\n",
    "\n",
    "\n",
    "\n",
    "net = GradientNet(densenet=densenet, growth_rate=growth_rate, \n",
    "                  transition_scale=transition_scale, pretrained_scale=pretrained_scale,\n",
    "                 gradient=gradient)\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "mse_loss = nn.MSELoss().cuda() if use_gpu==True else nn.MSELoss()\n",
    "mse_crf_loss = nn.MSELoss().cuda() if use_gpu==True else nn.MSELoss()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=args.base_lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_y(predict_unary, predict_dx, predict_dy, gt, predict_alpha, predict_beta, max_iter=100, eps=1.e-4, use_gpu=True, volatile=False):\n",
    "    def generate_y_(last_y, predict_unary, predict_dx, predict_dy, gt, predict_alpha, predict_beta, use_gpu=True):\n",
    "        def prepare_fileters(direction='up'):\n",
    "            filters = torch.Tensor(torch.zeros(3,3,3,3))\n",
    "            if direction == 'up': \n",
    "                for i in range(3): filters[i,i,0,1] = 1.\n",
    "            elif direction == 'down': \n",
    "                for i in range(3): filters[i,i,2,1] = 1.\n",
    "            elif direction == 'left': \n",
    "                for i in range(3): filters[i,i,1,0] = 1.\n",
    "            else: \n",
    "                for i in range(3): filters[i,i,1,2] = 1.\n",
    "            filters = Variable(filters)\n",
    "            if use_gpu == True: filters = filters.cuda()\n",
    "            return filters\n",
    "\n",
    "        f_up = prepare_fileters(direction='up')\n",
    "        f_down = prepare_fileters(direction='down')\n",
    "        f_left = prepare_fileters(direction='left')\n",
    "        f_right = prepare_fileters(direction='right')\n",
    "\n",
    "        last_y_up = F.conv2d(last_y, f_up, padding=1)\n",
    "        last_y_down = F.conv2d(last_y, f_down, padding=1)\n",
    "        last_y_left = F.conv2d(last_y, f_left, padding=1)\n",
    "        last_y_right = F.conv2d(last_y, f_right, padding=1)\n",
    "        \n",
    "        t_up = F.conv2d(predict_dy, f_up, padding=1)\n",
    "        t_down = -predict_dy\n",
    "        t_left = F.conv2d(predict_dx, f_left, padding=1)\n",
    "        t_right = -predict_dx\n",
    "        \n",
    "        beta_up = predict_beta[:,0:1,:,:]\n",
    "        beta_down = predict_beta[:,1:2,:,:]\n",
    "        beta_left = predict_beta[:,2:3,:,:]\n",
    "        beta_right = predict_beta[:,3:4,:,:]\n",
    "        \n",
    "        sum_beta = beta_up + beta_down + beta_left + beta_right\n",
    "        constant = predict_alpha + sum_beta\n",
    "        #print('constant', constant)\n",
    "        \n",
    "        # y = (predict_alpha * predict_unary + \\\n",
    "        #     beta_up * (last_y_up + t_up) + \\\n",
    "        #     beta_down * (last_y_down + t_down) + \\\n",
    "        #      beta_left * (last_y_left + t_left) + beta_right * (last_y_right + t_right))/constant\n",
    "\n",
    "        y = predict_alpha * predict_unary\n",
    "        y = y + last_y_up    + beta_up    * t_up\n",
    "        y = y + last_y_down  + beta_down  * t_down\n",
    "        y = y + last_y_left  + beta_left  * t_left\n",
    "        y = y + last_y_right + beta_right * t_right\n",
    "        y = y / 5.\n",
    "        return y\n",
    "    \n",
    "    predict_unary = predict_unary.clone()\n",
    "    predict_dx = predict_dx.clone()\n",
    "    predict_dy = predict_dy.clone()\n",
    "    \n",
    "    #y = Variable(predict_unary.data.cpu().clone()+torch.rand(predict_unary.size())/10., volatile=True).cuda()\n",
    "    y = Variable(predict_unary.data.clone())\n",
    "        \n",
    "    if use_gpu == True: y = y.cuda()\n",
    "    iters = 0\n",
    "    while 1:\n",
    "        last_y = y.clone()\n",
    "        y = generate_y_(y, predict_unary, predict_dx, predict_dy, gt, predict_alpha, predict_beta, use_gpu=use_gpu)\n",
    "        cur_loss = myutils.mse_loss_scalar(y, last_y)\n",
    "        if cur_loss <= eps: \n",
    "            #print('cur loss', cur_loss)\n",
    "            #print('cur iter', iters)\n",
    "            #print('y min', y.min(), 'max', y.max())\n",
    "            break\n",
    "        if iters >= max_iter: \n",
    "            #print('@break at max iter', cur_loss)\n",
    "            break\n",
    "        iters += 1\n",
    "        #print('y min', y.min(), 'max', y.max())\n",
    "        break\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crf_loss(y, predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta, volatile=False):\n",
    "#     return torch.cat([y],1)\n",
    "    def filter_gen(direction='x'):\n",
    "        filters = torch.Tensor(torch.zeros(3,3,3,3))\n",
    "        if use_gpu == True: filters = filters.cuda()\n",
    "        for i in range(3):\n",
    "            filters[i,i,1,1] = -1.\n",
    "        if direction == 'x':\n",
    "            for i in range(3):\n",
    "                filters[i,i,1,2] = 1.\n",
    "        else:\n",
    "            for i in range(3):\n",
    "                filters[i,i,2,1] = 1.\n",
    "        filters = Variable(filters)\n",
    "        return filters\n",
    "    \n",
    "    def prepare_fileters(direction='up'):\n",
    "            filters = torch.Tensor(torch.zeros(3,3,3,3))\n",
    "            if direction == 'up': \n",
    "                for i in range(3): filters[i,i,0,1] = 1.\n",
    "            elif direction == 'down': \n",
    "                for i in range(3): filters[i,i,2,1] = 1.\n",
    "            elif direction == 'left': \n",
    "                for i in range(3): filters[i,i,1,0] = 1.\n",
    "            else: \n",
    "                for i in range(3): filters[i,i,1,2] = 1.\n",
    "            filters = Variable(filters)\n",
    "            if use_gpu == True: filters = filters.cuda()\n",
    "            return filters\n",
    "\n",
    "    predict_unary = predict_unary.clone()\n",
    "    predict_dx = predict_dx.clone()\n",
    "    predict_dy = predict_dy.clone()\n",
    "    \n",
    "    if volatile ==True:\n",
    "        predict_alpha = predict_alpha.clone()\n",
    "        predict_beta = predict_beta.clone()\n",
    "\n",
    "\n",
    "    f_up = prepare_fileters(direction='up')\n",
    "    f_down = prepare_fileters(direction='down')\n",
    "    f_left = prepare_fileters(direction='left')\n",
    "    f_right = prepare_fileters(direction='right')\n",
    "\n",
    "    beta_up = predict_beta[:,0,:,:]\n",
    "    beta_down = predict_beta[:,1,:,:]\n",
    "    beta_left = predict_beta[:,2,:,:]\n",
    "    beta_right = predict_beta[:,3,:,:]\n",
    "    \n",
    "    f_dx = filter_gen(direction='x')\n",
    "    f_dy = filter_gen(direction='y')\n",
    "    \n",
    "    J1 = (y - predict_alpha * predict_unary)**2\n",
    "    J2 = (((y - F.conv2d(y, f_up, padding=1) - beta_up * F.conv2d(predict_dy, f_up, padding=1))**2))\n",
    "    J3 = (((y - F.conv2d(y, f_down, padding=1) + beta_down * predict_dy)**2))\n",
    "    J4 = (((y - F.conv2d(y, f_left, padding=1) - beta_left * F.conv2d(predict_dx, f_left, padding=1))**2))\n",
    "    J5 = (((y - F.conv2d(y, f_right, padding=1) + beta_right * predict_dx)**2))\n",
    "    J = torch.cat([J1,J2,J3,J4,J5],1) \n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_eval_model_per_epoch(epoch, net, args, train_loader, test_loader, phase='train'):\n",
    "    if phase == 'train':\n",
    "        volatile = False\n",
    "        net.train()\n",
    "    else:\n",
    "        volatile = True\n",
    "#         net.eval()\n",
    "        net.train()\n",
    "    \n",
    "    print('epoch: {} [{}]'.format(epoch, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    \"\"\"adjust learning rate\"\"\"\n",
    "    myutils.adjust_learning_rate(optimizer, args, epoch, beg=0, end=args.epoches)\n",
    "    #if epoch < args.training_thresholds: \n",
    "    #    myutils.adjust_learning_rate(optimizer, args, epoch, beg=0, end=args.training_thresholds-1)\n",
    "    #else:\n",
    "    #    myutils.adjust_learning_rate(optimizer, args, epoch, beg=args.training_thresholds, end=args.epoches)\n",
    "    writer.add_scalar('learning rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "\n",
    "    \"\"\"init statics\"\"\"\n",
    "    run_loss_unary = 0.\n",
    "    run_loss_dx = 0.\n",
    "    run_loss_dy = 0.\n",
    "    run_loss_y = 0.\n",
    "    run_loss_crf = 0.\n",
    "    run_cnt   = 0.00001\n",
    "\n",
    "    \"\"\"for all training/test data\"\"\"\n",
    "    loader = train_loader if phase == 'train' else test_loader\n",
    "    \n",
    "    for ind, data in enumerate(loader, 0):\n",
    "        \"\"\"prepare data\"\"\"\n",
    "        input_img, gt_albedo, gt_shading, cur_scene, img_path = data\n",
    "        (cur_scene,) = cur_scene\n",
    "        (img_path,) = img_path\n",
    "        cur_frame = img_path.split('/')[-1]\n",
    "        input_img = Variable(input_img, volatile=volatile)\n",
    "        gt_albedo = Variable(gt_albedo, requires_grad=False)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu: \n",
    "            input_img, gt_albedo, gt_shading = input_img.cuda(), gt_albedo.cuda(), gt_shading.cuda()\n",
    "        \n",
    "        \"\"\"prepare gradient\"\"\"\n",
    "        gt_dx = myutils.makeGradientTorch(gt_albedo, direction='x', use_gpu=use_gpu)\n",
    "        gt_dy = myutils.makeGradientTorch(gt_albedo, direction='y', use_gpu=use_gpu)\n",
    "        \n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        predict_all = net(input_img)\n",
    "        predict_unary = predict_all[:,0:3,:,:]\n",
    "        predict_dx = predict_all[:,3:6,:,:]\n",
    "        predict_dy = predict_all[:,6:9,:,:]\n",
    "        predict_alpha = predict_all[:,9:10,:,:]\n",
    "        predict_beta = predict_all[:,9:13,:,:]\n",
    "        \n",
    "        #print('alpha', predict_alpha.min(), predict_beta.max())\n",
    "        #print('beta ', predict_beta.min(), predict_beta.max())\n",
    "        \n",
    "        y = None\n",
    "        crf_loss_y = None\n",
    "        crf_loss_gt = None\n",
    "        \n",
    "        \"\"\"prepare crf y\"\"\"\n",
    "        y = generate_y(predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta, use_gpu=use_gpu, volatile=volatile)\n",
    "        #y = Variable(y.data.clone(), requires_grad=False).cuda()\n",
    "\n",
    "        #print('y', y.min(), y.max())\n",
    "\n",
    "\n",
    "        \"\"\"prepare crf loss\"\"\"\n",
    "        #crf_loss_y = crf_loss(y, predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta)\n",
    "        # crf_loss_y = crf_loss(predict_dx, predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta)\n",
    "        #crf_loss_gt = crf_loss(gt_albedo, predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta, volatile=True)\n",
    "        # crf_loss_gt = crf_loss(predict_dy, predict_unary, predict_dx, predict_dy, gt_albedo, predict_alpha, predict_beta)\n",
    "        #crf_loss_gt = Variable(crf_loss_gt.data.cpu(), requires_grad=False).cuda()\n",
    "        \n",
    "        \"\"\"prepare final gt\"\"\"\n",
    "        predict_final = None\n",
    "        gt_final = None\n",
    "        predict_final = torch.cat([predict_all[:,0:3+6,:,:], y], 1)\n",
    "        gt_final = torch.cat([gt_albedo, gt_dx, gt_dy, gt_albedo], 1)\n",
    "            \n",
    "        \n",
    "        \"\"\"compute loss\"\"\"\n",
    "        loss = mse_loss(predict_final, gt_final)\n",
    "        # c_loss = mse_crf_loss(predict_dx, predict_dy)\n",
    "        \n",
    "        run_loss_unary += myutils.mse_loss_scalar(predict_unary, gt_albedo)\n",
    "        run_loss_dx += myutils.mse_loss_scalar(predict_dx, gt_dx)\n",
    "        run_loss_dy += myutils.mse_loss_scalar(predict_dy, gt_dy)\n",
    "        run_loss_y += myutils.mse_loss_scalar(y, gt_albedo)\n",
    "        #run_loss_crf += myutils.mse_loss_scalar(crf_loss_y, 0)\n",
    "        run_cnt += 1\n",
    "\n",
    "        \"\"\"backward\"\"\"\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            # c_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \"\"\"generate display img\"\"\"\n",
    "        display_im = myutils.tensor2Numpy(input_img)[:,:,::-1]*255\n",
    "        display_gt_albedo = myutils.tensor2Numpy(gt_albedo)[:,:,::-1]*255\n",
    "        display_gt_dx = (myutils.tensor2Numpy(gt_dx)[:,:,::-1]+0.5)*255\n",
    "        display_gt_dy = (myutils.tensor2Numpy(gt_dy)[:,:,::-1]+0.5)*255\n",
    "        display_unary = myutils.tensor2Numpy(predict_unary)[:,:,::-1]*255\n",
    "        display_dx = (myutils.tensor2Numpy(predict_dx)[:,:,::-1]+0.5)*255\n",
    "        display_dy = (myutils.tensor2Numpy(predict_dy)[:,:,::-1]+0.5)*255\n",
    "        display_y = (myutils.tensor2Numpy(y)[:,:,::-1])*255\n",
    "\n",
    "        \"\"\"display\"\"\"\n",
    "        if (phase == 'train' and args.display_curindex % args.display_interval == 0) or \\\n",
    "        (phase == 'test' and cur_scene == 'alley_1' and cur_frame == 'frame_0001.png'):\n",
    "            # print('display ', phase, img_path, display_im.shape)\n",
    "            cv2.imwrite('snapshot{}/input.png'.format(args.gpu_num), display_im)\n",
    "            cv2.imwrite('snapshot{}/{}-gt-{}-unary.png'.format(args.gpu_num, phase, epoch), display_gt_albedo) \n",
    "            cv2.imwrite('snapshot{}/{}-gt-{}-dx.png'.format(args.gpu_num, phase, epoch), display_gt_dx) \n",
    "            cv2.imwrite('snapshot{}/{}-gt-{}-dy.png'.format(args.gpu_num, phase, epoch), display_gt_dy) \n",
    "            cv2.imwrite('snapshot{}/{}-rs-{}-unary.png'.format(args.gpu_num, phase, epoch), display_unary)\n",
    "            cv2.imwrite('snapshot{}/{}-rs-{}-dx.png'.format(args.gpu_num, phase, epoch), display_dx)\n",
    "            cv2.imwrite('snapshot{}/{}-rs-{}-dy.png'.format(args.gpu_num, phase, epoch), display_dy)\n",
    "            cv2.imwrite('snapshot{}/{}-rs-{}-y.png'.format(args.gpu_num, phase, epoch), display_y)\n",
    "        \n",
    "        args.display_curindex += 1\n",
    "    \n",
    "    \"\"\"output loss\"\"\"\n",
    "    loss_output = ''\n",
    "    loss_output += '{} loss: '.format(phase)\n",
    "    loss_output += 'unary: %6f ' % (run_loss_unary/run_cnt)\n",
    "    loss_output += 'pairwise: %6f ' % ((run_loss_dx+run_loss_dy)/run_cnt)\n",
    "    #loss_output += 'crf: %6f ' % (run_loss_crf/run_cnt)\n",
    "    loss_output += 'y: %6f ' % (run_loss_y/run_cnt)\n",
    "    \n",
    "    print(loss_output)\n",
    "    \n",
    "    \"\"\"write to tensorboard\"\"\"\n",
    "    writer.add_scalars('loss', {\n",
    "        '%s unary loss'% (phase): np.array([run_loss_unary/run_cnt]),\n",
    "        '%s dx loss'% (phase): np.array([run_loss_dx/run_cnt]),\n",
    "        '%s dy loss'% (phase): np.array([run_loss_dy/run_cnt]),\n",
    "        '%s pairwise loss'% (phase): np.array([(run_loss_dx+run_loss_dy)/run_cnt])\n",
    "        #'%s y loss'% (phase): np.array([run_loss_y/run_cnt]),\n",
    "    }, global_step=epoch)\n",
    "    \n",
    "    \"\"\"save snapshot\"\"\"\n",
    "    if phase == 'train':\n",
    "        myutils.save_snapshot(epoch, args, net, optimizer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [2018-02-19 00:55:26]\n",
      "train loss: unary: 0.064454 pairwise: 0.011692 y: 0.064844 \n",
      "epoch: 1 [2018-02-19 00:58:49]\n",
      "train loss: unary: 0.051616 pairwise: 0.007777 y: 0.051788 \n",
      "epoch: 2 [2018-02-19 01:02:00]\n",
      "train loss: unary: 0.040032 pairwise: 0.006333 y: 0.040420 \n",
      "epoch: 3 [2018-02-19 01:05:11]\n",
      "train loss: unary: 0.035519 pairwise: 0.005698 y: 0.036175 \n",
      "epoch: 4 [2018-02-19 01:08:21]\n",
      "test loss: unary: 0.037808 pairwise: 0.005209 y: 0.042884 \n",
      "epoch: 5 [2018-02-19 01:09:22]\n",
      "train loss: unary: 0.029033 pairwise: 0.005583 y: 0.029579 \n",
      "epoch: 6 [2018-02-19 01:12:32]\n",
      "train loss: unary: 0.024924 pairwise: 0.005270 y: 0.025641 \n",
      "epoch: 7 [2018-02-19 01:15:43]\n",
      "train loss: unary: 0.022993 pairwise: 0.004912 y: 0.023620 \n",
      "epoch: 8 [2018-02-19 01:18:53]\n",
      "train loss: unary: 0.022130 pairwise: 0.004810 y: 0.022798 \n",
      "epoch: 9 [2018-02-19 01:22:04]\n",
      "test loss: unary: 0.034786 pairwise: 0.004502 y: 0.037226 \n",
      "epoch: 10 [2018-02-19 01:23:04]\n",
      "train loss: unary: 0.019437 pairwise: 0.004842 y: 0.019980 \n",
      "epoch: 11 [2018-02-19 01:26:16]\n",
      "train loss: unary: 0.018624 pairwise: 0.004606 y: 0.019164 \n",
      "epoch: 12 [2018-02-19 01:29:27]\n",
      "train loss: unary: 0.017020 pairwise: 0.004606 y: 0.017508 \n",
      "epoch: 13 [2018-02-19 01:32:43]\n",
      "train loss: unary: 0.015812 pairwise: 0.004319 y: 0.016262 \n",
      "epoch: 14 [2018-02-19 01:35:56]\n",
      "test loss: unary: 0.038959 pairwise: 0.004338 y: 0.038295 \n",
      "epoch: 15 [2018-02-19 01:36:58]\n",
      "train loss: unary: 0.015894 pairwise: 0.004522 y: 0.016352 \n",
      "epoch: 16 [2018-02-19 01:40:12]\n",
      "train loss: unary: 0.015843 pairwise: 0.004507 y: 0.016267 \n",
      "epoch: 17 [2018-02-19 01:43:22]\n",
      "train loss: unary: 0.014506 pairwise: 0.004282 y: 0.014909 \n",
      "epoch: 18 [2018-02-19 01:46:33]\n",
      "train loss: unary: 0.014626 pairwise: 0.004421 y: 0.014977 \n",
      "epoch: 19 [2018-02-19 01:49:44]\n",
      "test loss: unary: 0.032142 pairwise: 0.004235 y: 0.032984 \n",
      "epoch: 20 [2018-02-19 01:50:45]\n",
      "train loss: unary: 0.013665 pairwise: 0.004298 y: 0.013982 \n",
      "epoch: 21 [2018-02-19 01:53:54]\n",
      "train loss: unary: 0.013887 pairwise: 0.004153 y: 0.014206 \n",
      "epoch: 22 [2018-02-19 01:57:05]\n",
      "train loss: unary: 0.013268 pairwise: 0.004413 y: 0.013602 \n",
      "epoch: 23 [2018-02-19 02:00:15]\n",
      "train loss: unary: 0.012401 pairwise: 0.004017 y: 0.012682 \n",
      "epoch: 24 [2018-02-19 02:03:25]\n",
      "test loss: unary: 0.032619 pairwise: 0.004185 y: 0.032430 \n",
      "epoch: 25 [2018-02-19 02:04:26]\n",
      "train loss: unary: 0.012296 pairwise: 0.004235 y: 0.012580 \n",
      "epoch: 26 [2018-02-19 02:07:36]\n",
      "train loss: unary: 0.011755 pairwise: 0.004283 y: 0.011994 \n",
      "epoch: 27 [2018-02-19 02:10:46]\n",
      "train loss: unary: 0.011246 pairwise: 0.003955 y: 0.011520 \n",
      "epoch: 28 [2018-02-19 02:13:57]\n",
      "train loss: unary: 0.011001 pairwise: 0.003887 y: 0.011236 \n",
      "epoch: 29 [2018-02-19 02:17:07]\n",
      "test loss: unary: 0.032908 pairwise: 0.004155 y: 0.032573 \n",
      "epoch: 30 [2018-02-19 02:18:07]\n",
      "train loss: unary: 0.010710 pairwise: 0.003944 y: 0.010942 \n",
      "epoch: 31 [2018-02-19 02:21:17]\n",
      "train loss: unary: 0.010725 pairwise: 0.004150 y: 0.010945 \n",
      "epoch: 32 [2018-02-19 02:24:27]\n",
      "train loss: unary: 0.009918 pairwise: 0.004060 y: 0.010128 \n",
      "epoch: 33 [2018-02-19 02:27:37]\n",
      "train loss: unary: 0.010042 pairwise: 0.004191 y: 0.010246 \n",
      "epoch: 34 [2018-02-19 02:30:48]\n",
      "test loss: unary: 0.031742 pairwise: 0.004139 y: 0.031920 \n",
      "epoch: 35 [2018-02-19 02:31:50]\n",
      "train loss: unary: 0.010007 pairwise: 0.003980 y: 0.010221 \n",
      "epoch: 36 [2018-02-19 02:35:00]\n",
      "train loss: unary: 0.009593 pairwise: 0.004011 y: 0.009775 \n",
      "epoch: 37 [2018-02-19 02:38:10]\n",
      "train loss: unary: 0.010444 pairwise: 0.003972 y: 0.010620 \n",
      "epoch: 38 [2018-02-19 02:41:20]\n",
      "train loss: unary: 0.009438 pairwise: 0.003893 y: 0.009644 \n",
      "epoch: 39 [2018-02-19 02:44:31]\n",
      "test loss: unary: 0.032075 pairwise: 0.004120 y: 0.031675 \n",
      "epoch: 40 [2018-02-19 02:45:32]\n",
      "train loss: unary: 0.009646 pairwise: 0.003968 y: 0.009843 \n",
      "epoch: 41 [2018-02-19 02:48:42]\n",
      "train loss: unary: 0.009105 pairwise: 0.003921 y: 0.009266 \n",
      "epoch: 42 [2018-02-19 02:51:53]\n",
      "train loss: unary: 0.008974 pairwise: 0.003974 y: 0.009151 \n",
      "epoch: 43 [2018-02-19 02:55:03]\n",
      "train loss: unary: 0.008620 pairwise: 0.003845 y: 0.008753 \n",
      "epoch: 44 [2018-02-19 02:58:14]\n",
      "test loss: unary: 0.032987 pairwise: 0.004094 y: 0.032545 \n",
      "epoch: 45 [2018-02-19 02:59:15]\n",
      "train loss: unary: 0.008482 pairwise: 0.003794 y: 0.008651 \n",
      "epoch: 46 [2018-02-19 03:02:25]\n",
      "train loss: unary: 0.008537 pairwise: 0.003914 y: 0.008682 \n",
      "epoch: 47 [2018-02-19 03:05:36]\n",
      "train loss: unary: 0.008432 pairwise: 0.003852 y: 0.008597 \n",
      "epoch: 48 [2018-02-19 03:08:47]\n",
      "train loss: unary: 0.008045 pairwise: 0.003812 y: 0.008183 \n",
      "epoch: 49 [2018-02-19 03:11:58]\n",
      "test loss: unary: 0.030748 pairwise: 0.004072 y: 0.031192 \n",
      "epoch: 50 [2018-02-19 03:13:00]\n",
      "train loss: unary: 0.008031 pairwise: 0.003857 y: 0.008152 \n",
      "epoch: 51 [2018-02-19 03:16:10]\n",
      "train loss: unary: 0.007907 pairwise: 0.003685 y: 0.008035 \n",
      "epoch: 52 [2018-02-19 03:19:21]\n",
      "train loss: unary: 0.008208 pairwise: 0.003792 y: 0.008339 \n",
      "epoch: 53 [2018-02-19 03:22:31]\n",
      "train loss: unary: 0.007747 pairwise: 0.003730 y: 0.007898 \n",
      "epoch: 54 [2018-02-19 03:25:41]\n",
      "test loss: unary: 0.031580 pairwise: 0.004088 y: 0.032315 \n",
      "epoch: 55 [2018-02-19 03:26:42]\n",
      "train loss: unary: 0.007623 pairwise: 0.003626 y: 0.007751 \n",
      "epoch: 56 [2018-02-19 03:29:53]\n",
      "train loss: unary: 0.007226 pairwise: 0.003672 y: 0.007362 \n",
      "epoch: 57 [2018-02-19 03:33:03]\n",
      "train loss: unary: 0.007292 pairwise: 0.003652 y: 0.007400 \n",
      "epoch: 58 [2018-02-19 03:36:14]\n",
      "train loss: unary: 0.007242 pairwise: 0.003651 y: 0.007340 \n",
      "epoch: 59 [2018-02-19 03:39:25]\n",
      "test loss: unary: 0.031157 pairwise: 0.004058 y: 0.031746 \n",
      "epoch: 60 [2018-02-19 03:40:25]\n",
      "train loss: unary: 0.007210 pairwise: 0.003564 y: 0.007328 \n",
      "epoch: 61 [2018-02-19 03:43:35]\n",
      "train loss: unary: 0.007046 pairwise: 0.003593 y: 0.007149 \n",
      "epoch: 62 [2018-02-19 03:46:46]\n",
      "train loss: unary: 0.007343 pairwise: 0.003784 y: 0.007462 \n",
      "epoch: 63 [2018-02-19 03:49:57]\n",
      "train loss: unary: 0.006828 pairwise: 0.003608 y: 0.006939 \n",
      "epoch: 64 [2018-02-19 03:53:07]\n",
      "test loss: unary: 0.030526 pairwise: 0.004042 y: 0.031024 \n",
      "epoch: 65 [2018-02-19 03:54:09]\n",
      "train loss: unary: 0.006591 pairwise: 0.003653 y: 0.006690 \n",
      "epoch: 66 [2018-02-19 03:57:19]\n",
      "train loss: unary: 0.006651 pairwise: 0.003572 y: 0.006750 \n",
      "epoch: 67 [2018-02-19 04:00:29]\n",
      "train loss: unary: 0.006696 pairwise: 0.003529 y: 0.006797 \n",
      "epoch: 68 [2018-02-19 04:03:39]\n",
      "train loss: unary: 0.006886 pairwise: 0.003535 y: 0.006970 \n",
      "epoch: 69 [2018-02-19 04:06:50]\n",
      "test loss: unary: 0.029552 pairwise: 0.004028 y: 0.029892 \n",
      "epoch: 70 [2018-02-19 04:07:51]\n",
      "train loss: unary: 0.006687 pairwise: 0.003618 y: 0.006783 \n",
      "epoch: 71 [2018-02-19 04:11:01]\n",
      "train loss: unary: 0.006374 pairwise: 0.003638 y: 0.006460 \n",
      "epoch: 72 [2018-02-19 04:14:11]\n",
      "train loss: unary: 0.006503 pairwise: 0.003596 y: 0.006590 \n",
      "epoch: 73 [2018-02-19 04:17:21]\n",
      "train loss: unary: 0.006337 pairwise: 0.003581 y: 0.006422 \n",
      "epoch: 74 [2018-02-19 04:20:32]\n",
      "test loss: unary: 0.028311 pairwise: 0.004017 y: 0.028397 \n",
      "epoch: 75 [2018-02-19 04:21:32]\n",
      "train loss: unary: 0.006365 pairwise: 0.003570 y: 0.006442 \n",
      "epoch: 76 [2018-02-19 04:24:44]\n",
      "train loss: unary: 0.006228 pairwise: 0.003481 y: 0.006310 \n",
      "epoch: 77 [2018-02-19 04:27:54]\n",
      "train loss: unary: 0.006069 pairwise: 0.003417 y: 0.006152 \n",
      "epoch: 78 [2018-02-19 04:31:04]\n",
      "train loss: unary: 0.006078 pairwise: 0.003419 y: 0.006156 \n",
      "epoch: 79 [2018-02-19 04:34:14]\n",
      "test loss: unary: 0.029924 pairwise: 0.004013 y: 0.030213 \n",
      "epoch: 80 [2018-02-19 04:35:15]\n",
      "train loss: unary: 0.006004 pairwise: 0.003365 y: 0.006079 \n",
      "epoch: 81 [2018-02-19 04:38:26]\n",
      "train loss: unary: 0.006375 pairwise: 0.003509 y: 0.006457 \n",
      "epoch: 82 [2018-02-19 04:41:36]\n",
      "train loss: unary: 0.006195 pairwise: 0.003375 y: 0.006279 \n",
      "epoch: 83 [2018-02-19 04:44:47]\n",
      "train loss: unary: 0.006029 pairwise: 0.003493 y: 0.006104 \n",
      "epoch: 84 [2018-02-19 04:47:57]\n",
      "test loss: unary: 0.029689 pairwise: 0.003990 y: 0.030034 \n",
      "epoch: 85 [2018-02-19 04:49:00]\n",
      "train loss: unary: 0.005857 pairwise: 0.003490 y: 0.005933 \n",
      "epoch: 86 [2018-02-19 04:52:10]\n",
      "train loss: unary: 0.005934 pairwise: 0.003491 y: 0.006006 \n",
      "epoch: 87 [2018-02-19 04:55:20]\n",
      "train loss: unary: 0.005899 pairwise: 0.003468 y: 0.005961 \n",
      "epoch: 88 [2018-02-19 04:58:31]\n",
      "train loss: unary: 0.005982 pairwise: 0.003451 y: 0.006063 \n",
      "epoch: 89 [2018-02-19 05:01:43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: unary: 0.030090 pairwise: 0.003995 y: 0.030248 \n",
      "epoch: 90 [2018-02-19 05:02:45]\n",
      "train loss: unary: 0.006016 pairwise: 0.003481 y: 0.006087 \n",
      "epoch: 91 [2018-02-19 05:05:56]\n",
      "train loss: unary: 0.005805 pairwise: 0.003390 y: 0.005878 \n",
      "epoch: 92 [2018-02-19 05:09:06]\n",
      "train loss: unary: 0.005785 pairwise: 0.003550 y: 0.005856 \n",
      "epoch: 93 [2018-02-19 05:12:16]\n",
      "train loss: unary: 0.005539 pairwise: 0.003371 y: 0.005606 \n",
      "epoch: 94 [2018-02-19 05:15:27]\n",
      "test loss: unary: 0.032007 pairwise: 0.004011 y: 0.032381 \n",
      "epoch: 95 [2018-02-19 05:16:27]\n",
      "train loss: unary: 0.005583 pairwise: 0.003254 y: 0.005653 \n",
      "epoch: 96 [2018-02-19 05:19:38]\n",
      "train loss: unary: 0.005578 pairwise: 0.003397 y: 0.005648 \n",
      "epoch: 97 [2018-02-19 05:22:48]\n",
      "train loss: unary: 0.005732 pairwise: 0.003474 y: 0.005798 \n",
      "epoch: 98 [2018-02-19 05:25:59]\n",
      "train loss: unary: 0.005426 pairwise: 0.003378 y: 0.005483 \n",
      "epoch: 99 [2018-02-19 05:29:10]\n",
      "test loss: unary: 0.030520 pairwise: 0.003968 y: 0.030823 \n",
      "epoch: 100 [2018-02-19 05:30:11]\n",
      "train loss: unary: 0.005473 pairwise: 0.003371 y: 0.005538 \n",
      "epoch: 101 [2018-02-19 05:33:21]\n",
      "train loss: unary: 0.005365 pairwise: 0.003518 y: 0.005429 \n",
      "epoch: 102 [2018-02-19 05:36:31]\n",
      "train loss: unary: 0.005258 pairwise: 0.003345 y: 0.005316 \n",
      "epoch: 103 [2018-02-19 05:39:41]\n",
      "train loss: unary: 0.005527 pairwise: 0.003341 y: 0.005591 \n",
      "epoch: 104 [2018-02-19 05:42:51]\n",
      "test loss: unary: 0.030625 pairwise: 0.003974 y: 0.030770 \n",
      "epoch: 105 [2018-02-19 05:43:52]\n",
      "train loss: unary: 0.005425 pairwise: 0.003449 y: 0.005483 \n",
      "epoch: 106 [2018-02-19 05:47:02]\n",
      "train loss: unary: 0.005320 pairwise: 0.003206 y: 0.005380 \n",
      "epoch: 107 [2018-02-19 05:50:12]\n",
      "train loss: unary: 0.005383 pairwise: 0.003332 y: 0.005445 \n",
      "epoch: 108 [2018-02-19 05:53:23]\n",
      "train loss: unary: 0.005262 pairwise: 0.003213 y: 0.005317 \n",
      "epoch: 109 [2018-02-19 05:56:33]\n",
      "test loss: unary: 0.029994 pairwise: 0.003954 y: 0.030208 \n",
      "epoch: 110 [2018-02-19 05:57:35]\n",
      "train loss: unary: 0.005045 pairwise: 0.003272 y: 0.005102 \n",
      "epoch: 111 [2018-02-19 06:00:45]\n",
      "train loss: unary: 0.005170 pairwise: 0.003221 y: 0.005231 \n",
      "epoch: 112 [2018-02-19 06:03:56]\n",
      "train loss: unary: 0.005155 pairwise: 0.003232 y: 0.005214 \n",
      "epoch: 113 [2018-02-19 06:07:06]\n",
      "train loss: unary: 0.005059 pairwise: 0.003350 y: 0.005115 \n",
      "epoch: 114 [2018-02-19 06:10:16]\n",
      "test loss: unary: 0.029811 pairwise: 0.003946 y: 0.029949 \n",
      "epoch: 115 [2018-02-19 06:11:17]\n",
      "train loss: unary: 0.005236 pairwise: 0.003247 y: 0.005290 \n",
      "epoch: 116 [2018-02-19 06:14:28]\n",
      "train loss: unary: 0.005135 pairwise: 0.003267 y: 0.005190 \n",
      "epoch: 117 [2018-02-19 06:17:39]\n",
      "train loss: unary: 0.004883 pairwise: 0.003083 y: 0.004925 \n",
      "epoch: 118 [2018-02-19 06:20:50]\n",
      "train loss: unary: 0.005009 pairwise: 0.003216 y: 0.005058 \n",
      "epoch: 119 [2018-02-19 06:24:00]\n",
      "test loss: unary: 0.029132 pairwise: 0.003955 y: 0.029207 \n",
      "epoch: 120 [2018-02-19 06:25:01]\n",
      "train loss: unary: 0.004882 pairwise: 0.003136 y: 0.004944 \n",
      "epoch: 121 [2018-02-19 06:28:11]\n",
      "train loss: unary: 0.004922 pairwise: 0.003187 y: 0.004973 \n",
      "epoch: 122 [2018-02-19 06:31:21]\n",
      "train loss: unary: 0.004940 pairwise: 0.003157 y: 0.004992 \n",
      "epoch: 123 [2018-02-19 06:34:31]\n",
      "train loss: unary: 0.004944 pairwise: 0.003313 y: 0.004996 \n",
      "epoch: 124 [2018-02-19 06:37:42]\n",
      "test loss: unary: 0.029464 pairwise: 0.003934 y: 0.029605 \n",
      "epoch: 125 [2018-02-19 06:38:44]\n",
      "train loss: unary: 0.004909 pairwise: 0.003152 y: 0.004959 \n",
      "epoch: 126 [2018-02-19 06:41:54]\n",
      "train loss: unary: 0.004987 pairwise: 0.003301 y: 0.005045 \n",
      "epoch: 127 [2018-02-19 06:45:05]\n",
      "train loss: unary: 0.004818 pairwise: 0.003186 y: 0.004869 \n",
      "epoch: 128 [2018-02-19 06:48:15]\n",
      "train loss: unary: 0.004857 pairwise: 0.003189 y: 0.004906 \n",
      "epoch: 129 [2018-02-19 06:51:25]\n",
      "test loss: unary: 0.029915 pairwise: 0.003932 y: 0.029952 \n",
      "epoch: 130 [2018-02-19 06:52:26]\n",
      "train loss: unary: 0.004872 pairwise: 0.003091 y: 0.004919 \n",
      "epoch: 131 [2018-02-19 06:55:36]\n",
      "train loss: unary: 0.004811 pairwise: 0.003129 y: 0.004863 \n",
      "epoch: 132 [2018-02-19 06:58:46]\n",
      "train loss: unary: 0.004879 pairwise: 0.003129 y: 0.004926 \n",
      "epoch: 133 [2018-02-19 07:01:56]\n",
      "train loss: unary: 0.004579 pairwise: 0.003012 y: 0.004627 \n",
      "epoch: 134 [2018-02-19 07:05:07]\n",
      "test loss: unary: 0.029291 pairwise: 0.003927 y: 0.029311 \n",
      "epoch: 135 [2018-02-19 07:06:07]\n",
      "train loss: unary: 0.004740 pairwise: 0.003046 y: 0.004794 \n",
      "epoch: 136 [2018-02-19 07:09:18]\n",
      "train loss: unary: 0.004681 pairwise: 0.003140 y: 0.004726 \n",
      "epoch: 137 [2018-02-19 07:12:28]\n",
      "train loss: unary: 0.004672 pairwise: 0.002971 y: 0.004722 \n",
      "epoch: 138 [2018-02-19 07:15:39]\n",
      "train loss: unary: 0.004696 pairwise: 0.003135 y: 0.004744 \n",
      "epoch: 139 [2018-02-19 07:18:49]\n",
      "test loss: unary: 0.030010 pairwise: 0.003923 y: 0.030243 \n",
      "epoch: 140 [2018-02-19 07:19:50]\n",
      "train loss: unary: 0.004665 pairwise: 0.003121 y: 0.004716 \n",
      "epoch: 141 [2018-02-19 07:23:00]\n",
      "train loss: unary: 0.004638 pairwise: 0.003070 y: 0.004683 \n",
      "epoch: 142 [2018-02-19 07:26:11]\n",
      "train loss: unary: 0.004664 pairwise: 0.003108 y: 0.004708 \n",
      "epoch: 143 [2018-02-19 07:29:20]\n",
      "train loss: unary: 0.004605 pairwise: 0.003098 y: 0.004640 \n",
      "epoch: 144 [2018-02-19 07:32:31]\n",
      "test loss: unary: 0.029872 pairwise: 0.003915 y: 0.029995 \n",
      "epoch: 145 [2018-02-19 07:33:32]\n",
      "train loss: unary: 0.004638 pairwise: 0.003144 y: 0.004691 \n",
      "epoch: 146 [2018-02-19 07:36:42]\n",
      "train loss: unary: 0.004516 pairwise: 0.003064 y: 0.004561 \n",
      "epoch: 147 [2018-02-19 07:39:52]\n",
      "train loss: unary: 0.004635 pairwise: 0.003117 y: 0.004678 \n",
      "epoch: 148 [2018-02-19 07:43:03]\n",
      "train loss: unary: 0.004536 pairwise: 0.003164 y: 0.004581 \n",
      "epoch: 149 [2018-02-19 07:46:13]\n",
      "test loss: unary: 0.028460 pairwise: 0.003897 y: 0.028532 \n",
      "epoch: 150 [2018-02-19 07:47:14]\n",
      "train loss: unary: 0.004542 pairwise: 0.003021 y: 0.004581 \n",
      "epoch: 151 [2018-02-19 07:50:24]\n",
      "train loss: unary: 0.004409 pairwise: 0.002950 y: 0.004454 \n",
      "epoch: 152 [2018-02-19 07:53:34]\n",
      "train loss: unary: 0.004459 pairwise: 0.003126 y: 0.004501 \n",
      "epoch: 153 [2018-02-19 07:56:46]\n",
      "train loss: unary: 0.004346 pairwise: 0.002940 y: 0.004389 \n",
      "epoch: 154 [2018-02-19 07:59:57]\n",
      "test loss: unary: 0.029598 pairwise: 0.003903 y: 0.029602 \n",
      "epoch: 155 [2018-02-19 08:00:57]\n",
      "train loss: unary: 0.004485 pairwise: 0.003080 y: 0.004530 \n",
      "epoch: 156 [2018-02-19 08:04:08]\n",
      "train loss: unary: 0.004407 pairwise: 0.003012 y: 0.004446 \n",
      "epoch: 157 [2018-02-19 08:07:19]\n",
      "train loss: unary: 0.004574 pairwise: 0.003061 y: 0.004621 \n",
      "epoch: 158 [2018-02-19 08:10:29]\n",
      "train loss: unary: 0.004436 pairwise: 0.002984 y: 0.004472 \n",
      "epoch: 159 [2018-02-19 08:13:39]\n",
      "test loss: unary: 0.029477 pairwise: 0.003889 y: 0.029550 \n",
      "epoch: 160 [2018-02-19 08:14:40]\n",
      "train loss: unary: 0.004436 pairwise: 0.002929 y: 0.004477 \n",
      "epoch: 161 [2018-02-19 08:17:50]\n",
      "train loss: unary: 0.004563 pairwise: 0.003088 y: 0.004609 \n",
      "epoch: 162 [2018-02-19 08:21:00]\n",
      "train loss: unary: 0.004275 pairwise: 0.002978 y: 0.004317 \n",
      "epoch: 163 [2018-02-19 08:24:12]\n",
      "train loss: unary: 0.004293 pairwise: 0.002986 y: 0.004334 \n",
      "epoch: 164 [2018-02-19 08:27:22]\n",
      "test loss: unary: 0.029329 pairwise: 0.003886 y: 0.029427 \n",
      "epoch: 165 [2018-02-19 08:28:22]\n",
      "train loss: unary: 0.004317 pairwise: 0.003016 y: 0.004361 \n",
      "epoch: 166 [2018-02-19 08:31:33]\n",
      "train loss: unary: 0.004309 pairwise: 0.002958 y: 0.004348 \n",
      "epoch: 167 [2018-02-19 08:34:43]\n",
      "train loss: unary: 0.004276 pairwise: 0.003019 y: 0.004317 \n",
      "epoch: 168 [2018-02-19 08:37:55]\n",
      "train loss: unary: 0.004368 pairwise: 0.002992 y: 0.004407 \n",
      "epoch: 169 [2018-02-19 08:41:06]\n",
      "test loss: unary: 0.028758 pairwise: 0.003879 y: 0.028814 \n",
      "epoch: 170 [2018-02-19 08:42:07]\n",
      "train loss: unary: 0.004392 pairwise: 0.003012 y: 0.004432 \n",
      "epoch: 171 [2018-02-19 08:45:17]\n",
      "train loss: unary: 0.004259 pairwise: 0.002953 y: 0.004296 \n",
      "epoch: 172 [2018-02-19 08:48:27]\n",
      "train loss: unary: 0.004264 pairwise: 0.002934 y: 0.004309 \n",
      "epoch: 173 [2018-02-19 08:51:41]\n",
      "train loss: unary: 0.004307 pairwise: 0.002996 y: 0.004343 \n",
      "epoch: 174 [2018-02-19 08:54:52]\n",
      "test loss: unary: 0.029309 pairwise: 0.003872 y: 0.029391 \n",
      "epoch: 175 [2018-02-19 08:55:53]\n",
      "train loss: unary: 0.004200 pairwise: 0.002920 y: 0.004232 \n",
      "epoch: 176 [2018-02-19 08:59:08]\n",
      "train loss: unary: 0.004202 pairwise: 0.002977 y: 0.004240 \n",
      "epoch: 177 [2018-02-19 09:02:22]\n",
      "train loss: unary: 0.004212 pairwise: 0.002971 y: 0.004251 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 178 [2018-02-19 09:05:36]\n",
      "train loss: unary: 0.004292 pairwise: 0.002879 y: 0.004328 \n",
      "epoch: 179 [2018-02-19 09:08:50]\n",
      "test loss: unary: 0.028424 pairwise: 0.003869 y: 0.028637 \n",
      "epoch: 180 [2018-02-19 09:09:53]\n",
      "train loss: unary: 0.004193 pairwise: 0.002962 y: 0.004233 \n",
      "epoch: 181 [2018-02-19 09:13:06]\n",
      "train loss: unary: 0.004229 pairwise: 0.002988 y: 0.004269 \n",
      "epoch: 182 [2018-02-19 09:16:21]\n",
      "train loss: unary: 0.004241 pairwise: 0.002984 y: 0.004274 \n",
      "epoch: 183 [2018-02-19 09:19:32]\n",
      "train loss: unary: 0.004110 pairwise: 0.002916 y: 0.004145 \n",
      "epoch: 184 [2018-02-19 09:22:43]\n",
      "test loss: unary: 0.027925 pairwise: 0.003858 y: 0.027968 \n",
      "epoch: 185 [2018-02-19 09:23:43]\n",
      "train loss: unary: 0.004265 pairwise: 0.003111 y: 0.004305 \n",
      "epoch: 186 [2018-02-19 09:26:54]\n",
      "train loss: unary: 0.004259 pairwise: 0.003013 y: 0.004298 \n",
      "epoch: 187 [2018-02-19 09:30:04]\n",
      "train loss: unary: 0.004117 pairwise: 0.002973 y: 0.004153 \n",
      "epoch: 188 [2018-02-19 09:33:15]\n",
      "train loss: unary: 0.004191 pairwise: 0.002958 y: 0.004230 \n",
      "epoch: 189 [2018-02-19 09:36:26]\n",
      "test loss: unary: 0.028722 pairwise: 0.003867 y: 0.028838 \n",
      "epoch: 190 [2018-02-19 09:37:26]\n",
      "train loss: unary: 0.004073 pairwise: 0.002982 y: 0.004109 \n",
      "epoch: 191 [2018-02-19 09:40:37]\n",
      "train loss: unary: 0.004311 pairwise: 0.003042 y: 0.004348 \n",
      "epoch: 192 [2018-02-19 09:43:47]\n",
      "train loss: unary: 0.004208 pairwise: 0.002999 y: 0.004246 \n",
      "epoch: 193 [2018-02-19 09:46:57]\n",
      "train loss: unary: 0.004051 pairwise: 0.002808 y: 0.004084 \n",
      "epoch: 194 [2018-02-19 09:50:08]\n",
      "test loss: unary: 0.028287 pairwise: 0.003847 y: 0.028350 \n",
      "epoch: 195 [2018-02-19 09:51:08]\n",
      "train loss: unary: 0.004143 pairwise: 0.002913 y: 0.004172 \n",
      "epoch: 196 [2018-02-19 09:54:18]\n",
      "train loss: unary: 0.004075 pairwise: 0.002929 y: 0.004114 \n",
      "epoch: 197 [2018-02-19 09:57:29]\n",
      "train loss: unary: 0.003966 pairwise: 0.002930 y: 0.004006 \n",
      "epoch: 198 [2018-02-19 10:00:39]\n",
      "train loss: unary: 0.004009 pairwise: 0.002828 y: 0.004046 \n",
      "epoch: 199 [2018-02-19 10:03:49]\n",
      "test loss: unary: 0.029193 pairwise: 0.003850 y: 0.029210 \n",
      "epoch: 200 [2018-02-19 10:04:49]\n",
      "train loss: unary: 0.004121 pairwise: 0.002971 y: 0.004152 \n",
      "epoch: 201 [2018-02-19 10:08:00]\n",
      "train loss: unary: 0.003992 pairwise: 0.002890 y: 0.004029 \n",
      "epoch: 202 [2018-02-19 10:11:10]\n",
      "train loss: unary: 0.004132 pairwise: 0.002934 y: 0.004164 \n",
      "epoch: 203 [2018-02-19 10:14:20]\n",
      "train loss: unary: 0.003929 pairwise: 0.002893 y: 0.003961 \n",
      "epoch: 204 [2018-02-19 10:17:30]\n",
      "test loss: unary: 0.028823 pairwise: 0.003853 y: 0.028855 \n",
      "epoch: 205 [2018-02-19 10:18:31]\n",
      "train loss: unary: 0.004088 pairwise: 0.002963 y: 0.004121 \n",
      "epoch: 206 [2018-02-19 10:21:41]\n",
      "train loss: unary: 0.004014 pairwise: 0.002954 y: 0.004049 \n",
      "epoch: 207 [2018-02-19 10:24:51]\n",
      "train loss: unary: 0.003990 pairwise: 0.002900 y: 0.004024 \n",
      "epoch: 208 [2018-02-19 10:28:01]\n",
      "train loss: unary: 0.004041 pairwise: 0.002965 y: 0.004081 \n",
      "epoch: 209 [2018-02-19 10:31:10]\n",
      "test loss: unary: 0.029025 pairwise: 0.003850 y: 0.029077 \n",
      "epoch: 210 [2018-02-19 10:32:11]\n",
      "train loss: unary: 0.003929 pairwise: 0.002861 y: 0.003963 \n",
      "epoch: 211 [2018-02-19 10:35:20]\n",
      "train loss: unary: 0.004040 pairwise: 0.002957 y: 0.004073 \n",
      "epoch: 212 [2018-02-19 10:38:30]\n",
      "train loss: unary: 0.003936 pairwise: 0.002889 y: 0.003969 \n",
      "epoch: 213 [2018-02-19 10:41:40]\n",
      "train loss: unary: 0.003875 pairwise: 0.002833 y: 0.003908 \n",
      "epoch: 214 [2018-02-19 10:44:50]\n",
      "test loss: unary: 0.028741 pairwise: 0.003846 y: 0.028818 \n",
      "epoch: 215 [2018-02-19 10:45:51]\n",
      "train loss: unary: 0.003878 pairwise: 0.002845 y: 0.003912 \n",
      "epoch: 216 [2018-02-19 10:49:01]\n",
      "train loss: unary: 0.003969 pairwise: 0.002905 y: 0.004003 \n",
      "epoch: 217 [2018-02-19 10:52:12]\n",
      "train loss: unary: 0.003891 pairwise: 0.002777 y: 0.003921 \n",
      "epoch: 218 [2018-02-19 10:55:22]\n",
      "train loss: unary: 0.003868 pairwise: 0.002770 y: 0.003899 \n",
      "epoch: 219 [2018-02-19 10:58:32]\n",
      "test loss: unary: 0.028358 pairwise: 0.003843 y: 0.028416 \n",
      "epoch: 220 [2018-02-19 10:59:33]\n",
      "train loss: unary: 0.003990 pairwise: 0.002828 y: 0.004025 \n",
      "epoch: 221 [2018-02-19 11:02:43]\n",
      "train loss: unary: 0.003997 pairwise: 0.002940 y: 0.004031 \n",
      "epoch: 222 [2018-02-19 11:05:52]\n",
      "train loss: unary: 0.003896 pairwise: 0.002757 y: 0.003929 \n",
      "epoch: 223 [2018-02-19 11:09:02]\n",
      "train loss: unary: 0.003856 pairwise: 0.002854 y: 0.003890 \n",
      "epoch: 224 [2018-02-19 11:12:12]\n",
      "test loss: unary: 0.028414 pairwise: 0.003835 y: 0.028479 \n",
      "epoch: 225 [2018-02-19 11:13:13]\n",
      "train loss: unary: 0.003900 pairwise: 0.002858 y: 0.003930 \n",
      "epoch: 226 [2018-02-19 11:16:23]\n",
      "train loss: unary: 0.003871 pairwise: 0.002863 y: 0.003906 \n",
      "epoch: 227 [2018-02-19 11:19:34]\n",
      "train loss: unary: 0.003758 pairwise: 0.002840 y: 0.003792 \n",
      "epoch: 228 [2018-02-19 11:22:44]\n",
      "train loss: unary: 0.003822 pairwise: 0.002863 y: 0.003855 \n",
      "epoch: 229 [2018-02-19 11:25:54]\n",
      "test loss: unary: 0.028485 pairwise: 0.003843 y: 0.028507 \n",
      "epoch: 230 [2018-02-19 11:26:54]\n",
      "train loss: unary: 0.003866 pairwise: 0.002950 y: 0.003899 \n",
      "epoch: 231 [2018-02-19 11:30:04]\n",
      "train loss: unary: 0.003918 pairwise: 0.002890 y: 0.003954 \n",
      "epoch: 232 [2018-02-19 11:33:16]\n",
      "train loss: unary: 0.003978 pairwise: 0.002950 y: 0.004008 \n",
      "epoch: 233 [2018-02-19 11:36:26]\n",
      "train loss: unary: 0.003758 pairwise: 0.002782 y: 0.003787 \n",
      "epoch: 234 [2018-02-19 11:39:37]\n",
      "test loss: unary: 0.028857 pairwise: 0.003837 y: 0.028919 \n",
      "epoch: 235 [2018-02-19 11:40:37]\n",
      "train loss: unary: 0.003896 pairwise: 0.002961 y: 0.003929 \n",
      "epoch: 236 [2018-02-19 11:43:47]\n",
      "train loss: unary: 0.003685 pairwise: 0.002804 y: 0.003715 \n",
      "epoch: 237 [2018-02-19 11:46:57]\n",
      "train loss: unary: 0.003867 pairwise: 0.002945 y: 0.003904 \n",
      "epoch: 238 [2018-02-19 11:50:07]\n",
      "train loss: unary: 0.003787 pairwise: 0.002822 y: 0.003822 \n",
      "epoch: 239 [2018-02-19 11:53:17]\n",
      "test loss: unary: 0.028848 pairwise: 0.003838 y: 0.028907 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"training loop\"\"\"\n",
    "writer = SummaryWriter(comment='-{}'.format(writer_comment))\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    phase = 'test' if (epoch+1) % 5 == 0 else 'train'\n",
    "    train_eval_model_per_epoch(epoch, net, args, train_loader, test_loader, phase=phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = Variable(torch.zeros(1,3,256,256))\n",
    "# y = net(x.cuda())\n",
    "# g = make_dot(y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# g.render('net-transition_scale_{}'.format(transition_scale)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
