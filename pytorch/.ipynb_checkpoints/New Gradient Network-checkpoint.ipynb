{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, platform, datetime, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import functional as F\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as denseinv\n",
    "from scipy import sparse\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import inv as spinv\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('debian', 'stretch/sid', '')\n"
     ]
    }
   ],
   "source": [
    "class Args(object):\n",
    "    pass\n",
    "args = Args()\n",
    "args.test_scene = 'alley_1'\n",
    "args.arch = \"densenet121\"\n",
    "args.epoches = 500\n",
    "args.epoches_unary_threshold = 0\n",
    "args.image_h = 256\n",
    "args.image_w = 256\n",
    "args.img_extentions = [\"png\"]\n",
    "args.training_thresholds = [250,200,150,50,0,300]\n",
    "args.base_lr = 1\n",
    "args.lr = args.base_lr\n",
    "args.snapshot_interval = 5000\n",
    "args.debug = True\n",
    "args.gpu_num = 1\n",
    "args.display_interval = 50\n",
    "args.display_curindex = 0\n",
    "\n",
    "system_ = platform.system()\n",
    "system_dist, system_version, _ = platform.dist()\n",
    "if system_ == \"Darwin\": \n",
    "    args.train_dir = '/Volumes/Transcend/dataset/sintel2'\n",
    "    args.pretrained = False\n",
    "elif platform.dist() ==  ('debian', 'jessie/sid', ''):\n",
    "    args.train_dir = '/home/albertxavier/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "elif platform.dist() == ('debian', 'stretch/sid', ''):\n",
    "    args.train_dir = '/home/cad/lwp/workspace/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "\n",
    "if platform.system() == 'Linux': use_gpu = True\n",
    "else: use_gpu = False\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "    \n",
    "\n",
    "print(platform.dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Netxxx(nn.Module):\n",
    "#     def build_blocks(self, num_block, num_init_features):\n",
    "#         bn_size = 4\n",
    "#         growth_rate = 32\n",
    "#         drop_rate = 0\n",
    "#         num_features = num_init_features\n",
    "#         features = nn.Sequential()\n",
    "#         for i, num_layers in enumerate(num_block):\n",
    "#             block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "#                                 bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "#             features.add_module('denseblock%d' % (i + 1), block)\n",
    "#             num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "#             trans = _MyTransition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "#             features.add_module('transition%d' % (i + 1), trans)\n",
    "#             num_features = num_features // 2\n",
    "#         print(num_features)\n",
    "#         return features\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Netxxx, self).__init__()\n",
    "#         self.bn = nn.Conv2d(kernel_size=3, in_channels=1, out_channels=1, padding=1)\n",
    "# #         self.bn = nn.BatchNorm2d(1)\n",
    "# #         self.fns = nn.Sequential(self.build_blocks([3,3],1),self.build_blocks([3,3],88))\n",
    "# #         self.fns.add_module(self.build_blocks([3,3],64))\n",
    "# #         self.fns.add_module(self.build_blocks([3,3],96))\n",
    "#     def forward(self,x):\n",
    "#         x = self.bn(x)\n",
    "# #         x = self.fns(x)\n",
    "#         return x\n",
    "    \n",
    "# netxxx = Netxxx()\n",
    "# netxxx.cuda()\n",
    "\n",
    "# cre = nn.MSELoss().cuda()\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=1, momentum=0)\n",
    "\n",
    "\n",
    "# x = torch.randn(1,1,3,3)\n",
    "# g = torch.randn(1,1,3,3)\n",
    "# x = Variable(x.cuda())\n",
    "# g = Variable(g.cuda())\n",
    "# y = netxxx(x)\n",
    "# optimizer.zero_grad()\n",
    "# loss = cre(y,g)\n",
    "# loss.backward()\n",
    "# print(netxxx.bn.weight.grad)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# y = netxxx(x)\n",
    "# optimizer.zero_grad()\n",
    "# loss = cre(y,g)\n",
    "# loss.backward()\n",
    "# print(netxxx.bn.weight.grad)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# y = netxxx(x)\n",
    "# optimizer.zero_grad()\n",
    "# loss = cre(y,g)\n",
    "# loss.backward()\n",
    "# print(netxxx.bn.weight.grad)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# y = netxxx(x)\n",
    "# optimizer.zero_grad()\n",
    "# loss = cre(y,g)\n",
    "# loss.backward()\n",
    "# print(netxxx.bn.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def make_dataset(dir, phase):\n",
    "    images_paths = glob.glob(os.path.join(dir, 'clean', '*', '*.png'))\n",
    "    albedo_paths = images_paths[:]\n",
    "    shading_paths = images_paths[:]\n",
    "    pathes = []\n",
    "    for img_path in images_paths:\n",
    "        sp = img_path.split('/'); \n",
    "        if phase == 'train':\n",
    "            if sp[-2] == args.test_scene: continue\n",
    "        else:\n",
    "            if sp[-2] != args.test_scene: continue\n",
    "            \n",
    "        sp[-3] = 'albedo'; \n",
    "        sp = ['/'] + sp; \n",
    "        albedo_path = os.path.join(*sp)\n",
    "        \n",
    "        sp = img_path.split('/'); \n",
    "        sp[-3] = 'shading'; \n",
    "        sp[-1] = sp[-1].replace('frame', 'out')\n",
    "        sp = ['/'] + sp; \n",
    "        shading_path = os.path.join(*sp)\n",
    "        \n",
    "        pathes.append((img_path, albedo_path, shading_path))\n",
    "    return pathes\n",
    "\n",
    "class MyImageFolder(data_utils.Dataset):\n",
    "    def __init__(self, root, phase='train', transform=None, target_transform=None, random_crop=True,\n",
    "                loader=default_loader):\n",
    "        imgs = make_dataset(root, phase)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(args.img_extentions)))\n",
    "\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.random_crop = random_crop\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path, albedo_path, shading_path = self.imgs[index]\n",
    "        \n",
    "        img = self.loader(img_path)\n",
    "        albedo = self.loader(albedo_path)\n",
    "        shading = self.loader(shading_path)\n",
    "        \n",
    "        i, j, h, w = self.get_params(img, (int(args.image_h), int(args.image_w)))\n",
    "\n",
    "        if self.random_crop == True:\n",
    "            img = img.crop((j, i, j + w, i + h))\n",
    "            albedo = albedo.crop((j, i, j + w, i + h))\n",
    "            shading = shading.crop((j, i, j + w, i + h))\n",
    "#         print(img.size)\n",
    "#         print((i, j, h, w))\n",
    "        \n",
    "        if self.transform is not None: img = self.transform(img)\n",
    "        if self.transform is not None: albedo = self.transform(albedo)\n",
    "        if self.transform is not None: shading = self.transform(shading)\n",
    "        \n",
    "        scene = img_path.split('/')[-2]\n",
    "        return img, albedo, shading, scene, img_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def get_params(self, img, output_size):\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "\n",
    "    \n",
    "#             transforms.RandomCrop((args.image_h, args.image_w)),\n",
    "    \n",
    "train_dataset = MyImageFolder(args.train_dir, 'train',\n",
    "                       transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ), random_crop=True)\n",
    "test_dataset = MyImageFolder(args.train_dir, 'test', \n",
    "                       transforms.Compose(\n",
    "        [transforms.CenterCrop((args.image_h, args.image_w)),\n",
    "         transforms.ToTensor()]\n",
    "    ), random_crop=False)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,1,True,num_workers=1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,1,False,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "[Defination](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)\n",
    "* DenseNet-121: num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16)\n",
    "    * First Convolution: 32M -> 16M -> 8M\n",
    "    * every transition: 8M -> 4M -> 2M (downsample 1/2, except the last block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.__dict__[args.arch](pretrained=args.pretrained)\n",
    "\n",
    "for param in densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu:\n",
    "    densenet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for k,v in enumerate(densenet.parameters()):\n",
    "#     print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet (\n",
      "  (features): Sequential (\n",
      "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu0): ReLU (inplace)\n",
      "    (pool0): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "    (denseblock1): _DenseBlock (\n",
      "      (denselayer1): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition1): _Transition (\n",
      "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (denseblock2): _DenseBlock (\n",
      "      (denselayer1): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition2): _Transition (\n",
      "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (denseblock3): _DenseBlock (\n",
      "      (denselayer1): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer17): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer18): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer19): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer20): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer21): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer22): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer23): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer24): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (transition3): _Transition (\n",
      "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (denseblock4): _DenseBlock (\n",
      "      (denselayer1): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer2): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer3): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer4): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer5): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer6): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer7): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer8): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer9): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer10): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer11): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer12): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer13): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer14): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer15): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (denselayer16): _DenseLayer (\n",
      "        (norm.1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.1): ReLU (inplace)\n",
      "        (conv.1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (norm.2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu.2): ReLU (inplace)\n",
      "        (conv.2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (classifier): Linear (1024 -> 1000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PreTrainedModel(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(PreTrainedModel, self).__init__()\n",
    "        common_features_net = nn.Sequential(*list(pretrained.children())[0:1])\n",
    "        self.net_16M = nn.Sequential(OrderedDict([\n",
    "            ('conv0', common_features_net[0].conv0),\n",
    "            ('norm0', common_features_net[0].norm0),\n",
    "            ('relu0', common_features_net[0].relu0)\n",
    "        ]))\n",
    "        self.net_8M = nn.Sequential(OrderedDict([\n",
    "            ('pool0', common_features_net[0].pool0)\n",
    "        ]))\n",
    "        self.net_4M = nn.Sequential(OrderedDict([\n",
    "            ('denseblock1', common_features_net[0].denseblock1),\n",
    "            ('transition1', common_features_net[0].transition1)\n",
    "        ]))\n",
    "        self.net_2M = nn.Sequential(OrderedDict([\n",
    "            ('denseblock2', common_features_net[0].denseblock2),\n",
    "            ('transition2', common_features_net[0].transition2)\n",
    "        ]))\n",
    "        self.net_1M = nn.Sequential(OrderedDict([\n",
    "            ('denseblock3', common_features_net[0].denseblock3),\n",
    "            ('transition3', common_features_net[0].transition3),\n",
    "            ('denseblock4', common_features_net[0].denseblock4)\n",
    "        ]))\n",
    "    def forward(self, ft_32M):\n",
    "        \n",
    "        pretrained_features = [0]*5\n",
    "        pretrained_features[0] = self.net_16M(ft_32M)\n",
    "        pretrained_features[1]  = self.net_8M(pretrained_features[0])\n",
    "        pretrained_features[2]  = self.net_4M(pretrained_features[1])\n",
    "        pretrained_features[3]  = self.net_2M(pretrained_features[2])\n",
    "        pretrained_features[4]  = self.net_1M(pretrained_features[3])\n",
    "        return pretrained_features\n",
    "\n",
    "if args.debug == True:\n",
    "#     pass\n",
    "    print(densenet)\n",
    "#     common_features_net = nn.Sequential(*list(densenet.children())[0:1])\n",
    "#     net_x = nn.Sequential(OrderedDict([\n",
    "#             ('conv0', common_features_net[0].conv0),\n",
    "#     ]))\n",
    "    \n",
    "#     \"\"\"\n",
    "#         debug: copy/clone\n",
    "# #     \"\"\"\n",
    "# #     print net_x\n",
    "#     t = nn.Sequential(*list(net_x.children()))\n",
    "\n",
    "#     for param in _8M.parameters():\n",
    "#         print param.data\n",
    "\n",
    "#     for param in t.parameters():\n",
    "#         param.data = (param.data*2)\n",
    "#     print \"@@@@@@\"\n",
    "\n",
    "#     for param in t.parameters():\n",
    "#         print param.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "#         self.add_module('relu1', nn.Sigmoid()),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "#         self.add_module('relu2', nn.Sigmoid()),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "class _MyTransition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_MyTransition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "#         self.add_module('relu', nn.Sigmoid())\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "\n",
    "    \n",
    "class GradientNet(nn.Module):\n",
    "    def build_blocks(self, num_block, num_init_features):\n",
    "        bn_size = 4\n",
    "        growth_rate = 32\n",
    "        drop_rate = 0\n",
    "        num_features = num_init_features\n",
    "        features = nn.Sequential()\n",
    "        for i, num_layers in enumerate(num_block):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            features.add_module('mydenseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            trans = _MyTransition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "            features.add_module('mytransition%d' % (i + 1), trans)\n",
    "            num_features = num_features // 2\n",
    "#         return features.cuda()\n",
    "        return features\n",
    "    \n",
    "    def __init__(self, pretrained_model):\n",
    "        super(GradientNet, self).__init__()\n",
    "        self.block_config = [(3,3,3),(6,6,6),(12,12,12),(16,16,16),(24,24,24)]\n",
    "        self.num_input_features = [64,64,128,256,1024]\n",
    "        self.upsample_config = [2,4,8,16,32]\n",
    "        \n",
    "        self.pretrained_model = pretrained_model\n",
    "        \n",
    "#         self.denseblocks = [nn.Sequential()] * len(self.block_config)\n",
    "#         for i in range(0, len(self.block_config)):\n",
    "        i=0; self.denseblock16 = self.build_blocks(self.block_config[i], self.num_input_features[i])\n",
    "        i=1; self.denseblock08 = self.build_blocks(self.block_config[i], self.num_input_features[i])\n",
    "        i=2; self.denseblock04 = self.build_blocks(self.block_config[i], self.num_input_features[i])\n",
    "        i=3; self.denseblock02 = self.build_blocks(self.block_config[i], self.num_input_features[i])\n",
    "        i=4; self.denseblock01 = self.build_blocks(self.block_config[i], self.num_input_features[i])\n",
    "        \n",
    "        if use_gpu: self.denseblock16.cuda()\n",
    "        if use_gpu: self.denseblock08.cuda()\n",
    "        if use_gpu: self.denseblock04.cuda()\n",
    "        if use_gpu: self.denseblock02.cuda()\n",
    "        if use_gpu: self.denseblock01.cuda()\n",
    "            \n",
    "        self.num_upsample_input_features = [92,176,352,480,800]\n",
    "#         for i in range(0, len(self.block_config)):\n",
    "\n",
    "        i=0; self.upsample16 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=2, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        i=1; self.upsample08 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=4, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        i=2; self.upsample04 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=8, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        i=3; self.upsample02 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=16, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        i=4; self.upsample01 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=32, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "        \n",
    "#         i=0; self.upsample16 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=2, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "#         i=1; self.upsample08 = nn.ConvTranspose2d(in_channels=self.num_upsample_input_features[i], out_channels=3, kernel_size=self.upsample_config[i], stride=4, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "#         i=2; self.upsample04 = nn.Sequential(OrderedDict([\n",
    "#             ('norm', nn.BatchNorm2d(self.num_upsample_input_features[i])),\n",
    "#             ('relu', nn.Sigmoid()),\n",
    "#             ('conv04', nn.Conv2d(self.num_upsample_input_features[i], 3, kernel_size=1, stride=1, bias=False))\n",
    "#             , \n",
    "#             ('upsample04', nn.Upsample(scale_factor=8, mode='bilinear'))\n",
    "#         ]))\n",
    "#         i=3; self.upsample02 = nn.Sequential(OrderedDict([\n",
    "#             ('norm', nn.BatchNorm2d(self.num_upsample_input_features[i])),\n",
    "#             ('relu', nn.Sigmoid()),\n",
    "#             ('conv02', nn.Conv2d(self.num_upsample_input_features[i], 3, kernel_size=1, stride=1, bias=False))\n",
    "#             , \n",
    "#             ('upsample02', nn.Upsample(scale_factor=16, mode='bilinear'))\n",
    "#         ]))\n",
    "#         i=4; self.upsample01 = nn.Sequential(OrderedDict([\n",
    "#             ('norm', nn.BatchNorm2d(self.num_upsample_input_features[i])),\n",
    "#             ('relu', nn.Sigmoid()),\n",
    "#             ('conv01', nn.Conv2d(self.num_upsample_input_features[i], 3, kernel_size=1, stride=1, bias=False))\n",
    "#             , \n",
    "#             ('upsample01', nn.Upsample(scale_factor=32, mode='bilinear'))\n",
    "#         ]))\n",
    "        \n",
    "        if use_gpu: self.upsample16.cuda()\n",
    "        if use_gpu: self.upsample08.cuda()\n",
    "        if use_gpu: self.upsample04.cuda()\n",
    "        if use_gpu: self.upsample02.cuda()\n",
    "        if use_gpu: self.upsample01.cuda()\n",
    "        \n",
    "#             if platform.system() == 'Linux': \n",
    "#                 self.upsamples[i] = self.upsamples[i].cuda()\n",
    "#             stride = stride * 2\n",
    "        \n",
    "        self.merge = nn.Sequential()\n",
    "        self.merge_in_channels =  (3*len(self.block_config), 64, 32, 16)\n",
    "        self.merge_out_channels = (                      64, 32, 16,  3)\n",
    "        for i in range(0, len(self.merge_out_channels)): \n",
    "            self.merge.add_module('merge.conv{}'.format(i), \n",
    "                                  nn.Conv2d(in_channels=self.merge_in_channels[i], \n",
    "                                            out_channels=self.merge_out_channels[i], kernel_size=1))\n",
    "            self.merge.add_module('merge.relu{}'.format(i), nn.ReLU(inplace=True))\n",
    "                                    \n",
    "    def forward(self, ft_input):\n",
    "        ft_pretrained = self.pretrained_model(ft_input)\n",
    "\n",
    "#         ft_predict = [b(ft_pretrained[i]) for i, b in enumerate(self.denseblocks)]\n",
    "#         ft_upsampled = [up(ft_predict[i]) for i, up in enumerate(self.upsamples)]\n",
    "        \n",
    "        ft_predict   = [0]*len(ft_pretrained)\n",
    "        ft_upsampled = [0]*len(ft_pretrained)\n",
    "        \n",
    "        i = 0; ft_predict[i] = self.denseblock16(ft_pretrained[i])\n",
    "        i = 1; ft_predict[i] = self.denseblock08(ft_pretrained[i])\n",
    "        i = 2; ft_predict[i] = self.denseblock04(ft_pretrained[i])\n",
    "        i = 3; ft_predict[i] = self.denseblock02(ft_pretrained[i])\n",
    "        i = 4; ft_predict[i] = self.denseblock01(ft_pretrained[i])\n",
    "        \n",
    "        i = 0; ft_upsampled[i] = self.upsample16(ft_predict[i])\n",
    "        i = 1; ft_upsampled[i] = self.upsample08(ft_predict[i])\n",
    "        i = 2; ft_upsampled[i] = self.upsample04(ft_predict[i])\n",
    "        i = 3; ft_upsampled[i] = self.upsample02(ft_predict[i])\n",
    "        i = 4; ft_upsampled[i] = self.upsample01(ft_predict[i])\n",
    "        \n",
    "        ft_concated = torch.cat(ft_upsampled, 1)\n",
    "        ft_merged = self.merge(ft_concated)\n",
    "#         ft_merged = None\n",
    "        ft_output = ft_upsampled + [ft_merged]\n",
    "        return ft_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [2017-11-13 17:53]\n",
      "lr 0.05\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.046645 merged: 0.000000\n",
      "epoch: 1 [2017-11-13 17:54]\n",
      "lr 0.04873397172404482\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.044607 merged: 0.000000\n",
      "epoch: 2 [2017-11-13 17:55]\n",
      "lr 0.047434164902525694\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.039879 merged: 0.000000\n",
      "epoch: 3 [2017-11-13 17:56]\n",
      "lr 0.04609772228646444\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.039625 merged: 0.000000\n",
      "epoch: 4 [2017-11-13 17:57]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.038451 merged: 0.000000\n",
      "epoch: 5 [2017-11-13 17:59]\n",
      "lr 0.04330127018922193\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.037699 merged: 0.000000\n",
      "epoch: 6 [2017-11-13 18:00]\n",
      "lr 0.041833001326703784\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.037119 merged: 0.000000\n",
      "epoch: 7 [2017-11-13 18:01]\n",
      "lr 0.04031128874149276\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.037034 merged: 0.000000\n",
      "epoch: 8 [2017-11-13 18:02]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.035009 merged: 0.000000\n",
      "epoch: 9 [2017-11-13 18:04]\n",
      "lr 0.03708099243547832\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.034026 merged: 0.000000\n",
      "epoch: 10 [2017-11-13 18:06]\n",
      "lr 0.03535533905932738\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.030263 merged: 0.000000\n",
      "epoch: 11 [2017-11-13 18:08]\n",
      "lr 0.03354101966249685\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.026351 merged: 0.000000\n",
      "epoch: 12 [2017-11-13 18:09]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.025058 merged: 0.000000\n",
      "epoch: 13 [2017-11-13 18:11]\n",
      "lr 0.02958039891549808\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.022450 merged: 0.000000\n",
      "epoch: 14 [2017-11-13 18:12]\n",
      "lr 0.02738612787525831\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.020268 merged: 0.000000\n",
      "epoch: 15 [2017-11-13 18:14]\n",
      "lr 0.025\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.019283 merged: 0.000000\n",
      "epoch: 16 [2017-11-13 18:15]\n",
      "lr 0.022360679774997894\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.017131 merged: 0.000000\n",
      "epoch: 17 [2017-11-13 18:17]\n",
      "lr 0.019364916731037088\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.016279 merged: 0.000000\n",
      "epoch: 18 [2017-11-13 18:18]\n",
      "lr 0.015811388300841896\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.015615 merged: 0.000000\n",
      "epoch: 19 [2017-11-13 18:20]\n",
      "lr 0.011180339887498954\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.015108 merged: 0.000000\n",
      "epoch: 20 [2017-11-13 18:21]\n",
      "lr 0.011180339887498954\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.042547  1M: 0.018688 merged: 0.000000\n",
      "epoch: 21 [2017-11-13 18:23]\n",
      "lr 0.04873397172404482\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.038745  1M: 0.018766 merged: 0.000000\n",
      "epoch: 22 [2017-11-13 18:25]\n",
      "lr 0.047434164902525694\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.035039  1M: 0.017464 merged: 0.000000\n",
      "epoch: 23 [2017-11-13 18:27]\n",
      "lr 0.04609772228646444\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.035038  1M: 0.017288 merged: 0.000000\n",
      "epoch: 24 [2017-11-13 18:29]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.029652  1M: 0.015394 merged: 0.000000\n",
      "epoch: 25 [2017-11-13 18:31]\n",
      "lr 0.04330127018922193\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.025176  1M: 0.015156 merged: 0.000000\n",
      "epoch: 26 [2017-11-13 18:33]\n",
      "lr 0.041833001326703784\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.019411  1M: 0.014098 merged: 0.000000\n",
      "epoch: 27 [2017-11-13 18:34]\n",
      "lr 0.04031128874149276\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.016552  1M: 0.014172 merged: 0.000000\n",
      "epoch: 28 [2017-11-13 18:36]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.013928  1M: 0.013639 merged: 0.000000\n",
      "epoch: 29 [2017-11-13 18:38]\n",
      "lr 0.03708099243547832\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.012752  1M: 0.013340 merged: 0.000000\n",
      "epoch: 30 [2017-11-13 18:40]\n",
      "lr 0.03535533905932738\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.012293  1M: 0.013027 merged: 0.000000\n",
      "epoch: 31 [2017-11-13 18:42]\n",
      "lr 0.03354101966249685\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.010832  1M: 0.012319 merged: 0.000000\n",
      "epoch: 32 [2017-11-13 18:44]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.010887  1M: 0.012526 merged: 0.000000\n",
      "epoch: 33 [2017-11-13 18:46]\n",
      "lr 0.02958039891549808\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.010495  1M: 0.011846 merged: 0.000000\n",
      "epoch: 34 [2017-11-13 18:48]\n",
      "lr 0.02738612787525831\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.010196  1M: 0.012202 merged: 0.000000\n",
      "epoch: 35 [2017-11-13 18:50]\n",
      "lr 0.025\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.010004  1M: 0.011826 merged: 0.000000\n",
      "epoch: 36 [2017-11-13 18:52]\n",
      "lr 0.022360679774997894\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.009629  1M: 0.011676 merged: 0.000000\n",
      "epoch: 37 [2017-11-13 18:54]\n",
      "lr 0.019364916731037088\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.009181  1M: 0.011442 merged: 0.000000\n",
      "epoch: 38 [2017-11-13 18:56]\n",
      "lr 0.015811388300841896\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.009043  1M: 0.011004 merged: 0.000000\n",
      "epoch: 39 [2017-11-13 18:58]\n",
      "lr 0.011180339887498954\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.008814  1M: 0.011139 merged: 0.000000\n",
      "epoch: 40 [2017-11-13 19:00]\n",
      "lr 0.011180339887498954\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.042433  2M: 0.009971  1M: 0.012449 merged: 0.000000\n",
      "epoch: 41 [2017-11-13 19:02]\n",
      "lr 0.04873397172404482\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.037068  2M: 0.009783  1M: 0.012074 merged: 0.000000\n",
      "epoch: 42 [2017-11-13 19:04]\n",
      "lr 0.047434164902525694\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.031343  2M: 0.009643  1M: 0.012356 merged: 0.000000\n",
      "epoch: 43 [2017-11-13 19:07]\n",
      "lr 0.04609772228646444\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.023266  2M: 0.009575  1M: 0.012322 merged: 0.000000\n",
      "epoch: 44 [2017-11-13 19:09]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.018232  2M: 0.009045  1M: 0.011871 merged: 0.000000\n",
      "epoch: 45 [2017-11-13 19:11]\n",
      "lr 0.04330127018922193\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.016424  2M: 0.008932  1M: 0.011548 merged: 0.000000\n",
      "epoch: 46 [2017-11-13 19:13]\n",
      "lr 0.041833001326703784\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.015106  2M: 0.008749  1M: 0.011691 merged: 0.000000\n",
      "epoch: 47 [2017-11-13 19:16]\n",
      "lr 0.04031128874149276\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.013529  2M: 0.008578  1M: 0.011612 merged: 0.000000\n",
      "epoch: 48 [2017-11-13 19:18]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.011605  2M: 0.008273  1M: 0.011115 merged: 0.000000\n",
      "epoch: 49 [2017-11-13 19:20]\n",
      "lr 0.03708099243547832\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.010990  2M: 0.008214  1M: 0.011244 merged: 0.000000\n",
      "epoch: 50 [2017-11-13 19:23]\n",
      "lr 0.03535533905932738\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.010528  2M: 0.007922  1M: 0.010960 merged: 0.000000\n",
      "epoch: 51 [2017-11-13 19:25]\n",
      "lr 0.03354101966249685\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.009907  2M: 0.007746  1M: 0.010884 merged: 0.000000\n",
      "epoch: 52 [2017-11-13 19:27]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.009251  2M: 0.007499  1M: 0.010738 merged: 0.000000\n",
      "epoch: 53 [2017-11-13 19:29]\n",
      "lr 0.02958039891549808\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.008772  2M: 0.007342  1M: 0.010892 merged: 0.000000\n",
      "epoch: 54 [2017-11-13 19:32]\n",
      "lr 0.02738612787525831\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.008598  2M: 0.007348  1M: 0.010792 merged: 0.000000\n",
      "epoch: 55 [2017-11-13 19:34]\n",
      "lr 0.025\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.008292  2M: 0.006998  1M: 0.010245 merged: 0.000000\n",
      "epoch: 56 [2017-11-13 19:36]\n",
      "lr 0.022360679774997894\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.008165  2M: 0.007079  1M: 0.010547 merged: 0.000000\n",
      "epoch: 57 [2017-11-13 19:38]\n",
      "lr 0.019364916731037088\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "ss = 20\n",
    "\n",
    "args.display_curindex = 0\n",
    "args.base_lr = 0.05\n",
    "args.display_interval = 10\n",
    "args.momentum = 0.9\n",
    "args.epoches = 500\n",
    "args.training_thresholds = [ss*4,ss*3,ss*2,ss*1,ss*0,ss*5]\n",
    "args.power = 0.5\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "pretrained = PreTrainedModel(densenet)\n",
    "if use_gpu: \n",
    "    pretrained.cuda()\n",
    "    \n",
    "net = GradientNet(pretrained)\n",
    "if use_gpu: \n",
    "    net.cuda()\n",
    "\n",
    "if use_gpu: \n",
    "    mse_losses = [nn.MSELoss().cuda()] * 6\n",
    "else:\n",
    "    mse_losses = [nn.MSELoss()] * 6\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=args.base_lr, momentum=args.momentum)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, beg, end, reset_lr=None):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if reset_lr != None:\n",
    "            param_group['lr'] = reset_lr\n",
    "            continue\n",
    "        if epoch != 0: \n",
    "            # linear\n",
    "#             param_group['lr'] *= (end-epoch) / (end-beg)\n",
    "#             poly base_lr (1 - iter/max_iter) ^ (power)\n",
    "            param_group['lr'] = args.base_lr * (1. - 1.*epoch/(end-beg)) ** (args.power)\n",
    "            if param_group['lr'] < 1.0e-8: param_group['lr'] = 1.0e-8\n",
    "        print('lr', param_group['lr'])\n",
    "        \n",
    "# def findLargerInd(target, arr):\n",
    "#     res = list(filter(lambda x: x>target, arr))\n",
    "#     print('res',res)\n",
    "#     if len(res) == 0: return -1\n",
    "#     return res[0]\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    net.train()\n",
    "    print('epoch: {} [{}]'.format(epoch, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "    \n",
    "    if epoch < args.training_thresholds[-1]: adjust_learning_rate(optimizer, epoch%20, beg=0, end=20)\n",
    "    else: adjust_learning_rate(optimizer, epoch, beg=args.training_thresholds[-1], end=args.epoches)\n",
    "    \n",
    "    run_losses = [0] * len(args.training_thresholds)\n",
    "    run_cnts   = [0.00001] * len(args.training_thresholds)\n",
    "    if (epoch in args.training_thresholds) == True: \n",
    "        adjust_learning_rate(optimizer, epoch, reset_lr=args.base_lr, beg=-1, end=-1)\n",
    "    writer.add_scalar('learning rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    for ind, data in enumerate(train_loader, 0):\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        im = input_img[0,:,:,:].numpy(); im = im.transpose(1,2,0); im = im[:,:,::-1]*255\n",
    "        \n",
    "        if test_scene[0] == 'alley_1':\n",
    "            print('alley_1 yes')\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if platform.system() == 'Linux':\n",
    "            input_img = input_img.cuda()\n",
    "            gt_albedo = gt_albedo.cuda()\n",
    "            gt_shading = gt_shading.cuda()\n",
    "#         run_losses = [0] * len(mse_losses)\n",
    "#         run_cnts = [0.00001] * len(mse_losses)\n",
    "        if args.display_curindex % args.display_interval == 0:\n",
    "            cv2.imwrite('snapshot/input.png', im)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ft_predict = net(input_img)\n",
    "        for (i,threshold) in enumerate(args.training_thresholds):\n",
    "            if epoch >= threshold:\n",
    "                if i == 5: s = 1\n",
    "                else: s = (2**(i+1))\n",
    "                gt = gt_albedo.cpu().data.numpy()\n",
    "                n,c,h,w = gt.shape\n",
    "                gt = gt[0,:,:,:]\n",
    "                gt = gt.transpose((1,2,0))\n",
    "                gt = cv2.resize(gt, (h//s, w//s))\n",
    "                gt = cv2.resize(gt, (h,w))\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "#                     cv2.imwrite('snapshot/input.png'.format(epoch, i), im)\n",
    "                    cv2.imwrite('snapshot/gt-{}-{}.png'.format(epoch, i), gt[:,:,::-1]*255)\n",
    "                gt = gt.transpose((2,0,1))\n",
    "                gt = gt[np.newaxis, :]\n",
    "                gt = Variable(torch.from_numpy(gt))\n",
    "                if platform.system() == \"Linux\": gt = gt.cuda()\n",
    "                loss = mse_losses[i](ft_predict[i], gt)\n",
    "                loss_data = loss.data.cpu().numpy()\n",
    "                writer.add_scalar('{}th training iters loss'.format(i), loss_data, global_step=args.display_curindex)\n",
    "                ma_ = ft_predict[i].max().cpu().data.numpy()\n",
    "                mi_ = ft_predict[i].min().cpu().data.numpy()\n",
    "#                 print('mi', mi_, 'ma', ma_)\n",
    "                writer.add_scalars('{}th training predict'.format(i), {'max': ma_, 'min': mi_}, global_step=args.display_curindex)\n",
    "#                 run_cnts[i] += 1\n",
    "                run_losses[i] += loss.data.cpu().numpy()[0]\n",
    "                loss.backward(retain_graph=True)\n",
    "                run_cnts[i] += 1\n",
    "#                 print('i = ', i, '; weig\\n', net.upsample01.weight[0,0,0:4,0:4].data.cpu().numpy())\n",
    "#                 print('i = ', i, '; grad\\n', net.upsample01.weight.grad[0,0,0:4,0:4].data.cpu().numpy())\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    im = ft_predict[i].cpu().data.numpy()[0].transpose((1,2,0)) * 255\n",
    "                    cv2.imwrite('snapshot/train-{}-{}.png'.format(epoch, i), im[:,:,::-1])\n",
    "        \n",
    "        optimizer.step()\n",
    "        args.display_curindex += 1\n",
    "\n",
    "    \"\"\" every epoch \"\"\"\n",
    "          \n",
    "    \n",
    "#     loss_output = 'ind: ' + str(args.display_curindex)\n",
    "    loss_output = ''\n",
    "    for i,v in enumerate(run_losses):\n",
    "        writer.add_scalar('{}th epoches'.format(i), run_losses[i]/ run_cnts[i], global_step=epoch)  \n",
    "        if i == len(run_losses)-1: \n",
    "            loss_output += ' merged: %6f' % (run_losses[i] / run_cnts[i])\n",
    "            continue\n",
    "        loss_output += ' %2dM: %6f' % ((2**(4-i)), (run_losses[i] / run_cnts[i]))\n",
    "    print(loss_output)\n",
    "    # save at every epoch\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'args' : args,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, 'snapshot/snapshot-{}.pth.tar'.format(epoch))\n",
    "    \n",
    "    # test \n",
    "    net.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if platform.system() == 'Linux':\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "        ft_test = net(input_img)\n",
    "            \n",
    "        for k,v in enumerate(ft_test):\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "            gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            \n",
    "            test_loss = np.mean((gt - v)**2)\n",
    "            writer.add_scalar('{}th test loss'.format(k), test_loss, global_step=epoch)\n",
    "            \n",
    "            cv2.imwrite('snapshot/test-{}-{}.png'.format(epoch, k), v[:,:,::-1]*255)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# net.upsample01.weight.grad[0,0,0:4,0:4].data[0]\n",
    "for k,v in enumerate(net.parameters()):\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-7784240a69a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.creator)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
