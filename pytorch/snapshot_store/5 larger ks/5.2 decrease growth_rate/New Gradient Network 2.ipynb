{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, platform, datetime, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import functional as F\n",
    "# import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as denseinv\n",
    "from scipy import sparse\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import inv as spinv\n",
    "import scipy.misc\n",
    "\n",
    "from myimagefolder import MyImageFolder\n",
    "from mymodel import PreTrainedModel, GradientNet\n",
    "from myargs import Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('debian', 'jessie/sid', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Args()\n",
    "args.test_scene = 'alley_1'\n",
    "args.arch = \"densenet121\"\n",
    "args.epoches = 500\n",
    "args.epoches_unary_threshold = 0\n",
    "args.image_h = 256\n",
    "args.image_w = 256\n",
    "args.img_extentions = [\"png\"]\n",
    "args.training_thresholds = [250,200,150,50,0,300]\n",
    "args.base_lr = 1\n",
    "args.lr = args.base_lr\n",
    "args.snapshot_interval = 5000\n",
    "args.debug = True\n",
    "args.gpu_num = 2\n",
    "growth_rate = (4*(2**(args.gpu_num)))\n",
    "args.display_interval = 50\n",
    "args.display_curindex = 0\n",
    "\n",
    "system_ = platform.system()\n",
    "system_dist, system_version, _ = platform.dist()\n",
    "if system_ == \"Darwin\": \n",
    "    args.train_dir = '/Volumes/Transcend/dataset/sintel2'\n",
    "    args.pretrained = False\n",
    "elif platform.dist() ==  ('debian', 'jessie/sid', ''):\n",
    "    args.train_dir = '/home/lwp/workspace/sintel2'\n",
    "    args.pretrained = True\n",
    "elif platform.dist() == ('debian', 'stretch/sid', ''):\n",
    "    args.train_dir = '/home/cad/lwp/workspace/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "\n",
    "if platform.system() == 'Linux': use_gpu = True\n",
    "else: use_gpu = False\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "    \n",
    "\n",
    "print(platform.dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyImageFolder(args.train_dir, 'train',\n",
    "                       transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ), random_crop=True, \n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "test_dataset = MyImageFolder(args.train_dir, 'test', \n",
    "                       transforms.Compose(\n",
    "        [transforms.CenterCrop((args.image_h, args.image_w)),\n",
    "         transforms.ToTensor()]\n",
    "    ), random_crop=False,\n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,1,True,num_workers=1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,1,True,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "[Defination](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)\n",
    "* DenseNet-121: num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16)\n",
    "    * First Convolution: 32M -> 16M -> 8M\n",
    "    * every transition: 8M -> 4M -> 2M (downsample 1/2, except the last block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "densenet = models.__dict__[args.arch](pretrained=args.pretrained)\n",
    "\n",
    "for param in densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu: densenet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = 6\n",
    "\n",
    "args.display_curindex = 0\n",
    "args.base_lr = 0.05\n",
    "args.display_interval = 20\n",
    "args.momentum = 0.9\n",
    "args.epoches = 120\n",
    "args.training_thresholds = [ss*4,ss*3,ss*2,ss*1,ss*0,ss*5]\n",
    "args.power = 0.5\n",
    "\n",
    "\n",
    "\n",
    "pretrained = PreTrainedModel(densenet)\n",
    "if use_gpu: \n",
    "    pretrained.cuda()\n",
    "\n",
    "\n",
    "net = GradientNet(growth_rate=growth_rate)\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "if use_gpu: \n",
    "    mse_losses = [nn.MSELoss().cuda()] * 6\n",
    "    test_losses = [nn.MSELoss().cuda()] * 6\n",
    "else:\n",
    "    mse_losses = [nn.MSELoss()] * 6\n",
    "    test_losses = [nn.MSELoss()] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [2017-11-21 17:40:12]\n",
      "lr 0.05\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.055675 merged: 0.000000\n",
      "epoch: 1 [2017-11-21 17:42:09]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.051149 merged: 0.000000\n",
      "epoch: 2 [2017-11-21 17:44:05]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.047816 merged: 0.000000\n",
      "epoch: 3 [2017-11-21 17:45:58]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.045797 merged: 0.000000\n",
      "epoch: 4 [2017-11-21 17:47:52]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.043717 merged: 0.000000\n",
      "epoch: 5 [2017-11-21 17:49:46]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.042538 merged: 0.000000\n",
      "epoch: 6 [2017-11-21 17:51:40]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.051502  1M: 0.046703 merged: 0.000000\n",
      "epoch: 7 [2017-11-21 17:53:51]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.043417  1M: 0.041166 merged: 0.000000\n",
      "epoch: 8 [2017-11-21 17:56:06]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.040211  1M: 0.036353 merged: 0.000000\n",
      "epoch: 9 [2017-11-21 17:58:19]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.034678  1M: 0.033623 merged: 0.000000\n",
      "epoch: 10 [2017-11-21 18:00:34]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.026206  1M: 0.028478 merged: 0.000000\n",
      "epoch: 11 [2017-11-21 18:02:49]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.024831  1M: 0.028140 merged: 0.000000\n",
      "epoch: 12 [2017-11-21 18:05:03]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.047170  2M: 0.029928  1M: 0.032065 merged: 0.000000\n",
      "epoch: 13 [2017-11-21 18:07:36]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.038411  2M: 0.026114  1M: 0.030081 merged: 0.000000\n",
      "epoch: 14 [2017-11-21 18:10:06]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.026960  2M: 0.021676  1M: 0.026853 merged: 0.000000\n",
      "epoch: 15 [2017-11-21 18:12:39]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.022625  2M: 0.019653  1M: 0.025259 merged: 0.000000\n",
      "epoch: 16 [2017-11-21 18:15:08]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.018582  2M: 0.018368  1M: 0.024535 merged: 0.000000\n",
      "epoch: 17 [2017-11-21 18:17:39]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.016654  2M: 0.018087  1M: 0.022356 merged: 0.000000\n",
      "epoch: 18 [2017-11-21 18:20:09]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.046222  4M: 0.021843  2M: 0.022111  1M: 0.027005 merged: 0.000000\n",
      "epoch: 19 [2017-11-21 18:22:53]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.035752  4M: 0.017887  2M: 0.019086  1M: 0.026210 merged: 0.000000\n",
      "epoch: 20 [2017-11-21 18:25:39]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.026925  4M: 0.017360  2M: 0.018221  1M: 0.025583 merged: 0.000000\n",
      "epoch: 21 [2017-11-21 18:28:26]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.020912  4M: 0.014752  2M: 0.016614  1M: 0.023896 merged: 0.000000\n",
      "epoch: 22 [2017-11-21 18:31:09]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.018442  4M: 0.013032  2M: 0.015421  1M: 0.021267 merged: 0.000000\n",
      "epoch: 23 [2017-11-21 18:33:56]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.016457  4M: 0.012792  2M: 0.015016  1M: 0.022399 merged: 0.000000\n",
      "epoch: 24 [2017-11-21 18:36:43]\n",
      "lr 1e-08\n",
      " 16M: 0.045104  8M: 0.021023  4M: 0.014820  2M: 0.017318  1M: 0.024788 merged: 0.000000\n",
      "epoch: 25 [2017-11-21 18:40:23]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.032688  8M: 0.017705  4M: 0.013232  2M: 0.016442  1M: 0.023403 merged: 0.000000\n",
      "epoch: 26 [2017-11-21 18:44:04]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.023367  8M: 0.015331  4M: 0.012391  2M: 0.015305  1M: 0.022479 merged: 0.000000\n",
      "epoch: 27 [2017-11-21 18:47:44]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.019320  8M: 0.014150  4M: 0.011535  2M: 0.014656  1M: 0.021483 merged: 0.000000\n",
      "epoch: 28 [2017-11-21 18:51:25]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.016134  8M: 0.012808  4M: 0.010884  2M: 0.013975  1M: 0.020820 merged: 0.000000\n",
      "epoch: 29 [2017-11-21 18:55:08]\n",
      "lr 1e-08\n",
      " 16M: 0.014639  8M: 0.012304  4M: 0.010830  2M: 0.013701  1M: 0.020554 merged: 0.000000\n",
      "epoch: 30 [2017-11-21 18:58:48]\n",
      "lr 0.05\n",
      " 16M: 0.019807  8M: 0.015587  4M: 0.013311  2M: 0.015262  1M: 0.022798 merged: 0.033898\n",
      "epoch: 31 [2017-11-21 19:04:30]\n",
      "lr 0.04971830761761256\n",
      " 16M: 0.021134  8M: 0.018020  4M: 0.017512  2M: 0.016947  1M: 0.023818 merged: 0.025660\n",
      "epoch: 32 [2017-11-21 19:10:12]\n",
      "lr 0.04943501011144937\n",
      " 16M: 0.018237  8M: 0.015457  4M: 0.015591  2M: 0.016087  1M: 0.023724 merged: 0.019927\n",
      "epoch: 33 [2017-11-21 19:15:51]\n",
      "lr 0.04915007972606608\n",
      " 16M: 0.015477  8M: 0.013243  4M: 0.014120  2M: 0.015379  1M: 0.022727 merged: 0.017099\n",
      "epoch: 34 [2017-11-21 19:21:29]\n",
      "lr 0.04886348789677424\n",
      " 16M: 0.014848  8M: 0.012905  4M: 0.013251  2M: 0.014775  1M: 0.021883 merged: 0.015844\n",
      "epoch: 35 [2017-11-21 19:27:08]\n",
      "lr 0.04857520521621862\n",
      " 16M: 0.013462  8M: 0.012393  4M: 0.013204  2M: 0.014309  1M: 0.022112 merged: 0.014391\n",
      "epoch: 36 [2017-11-21 19:32:50]\n",
      "lr 0.04828520139915856\n",
      " 16M: 0.013200  8M: 0.012010  4M: 0.012737  2M: 0.014412  1M: 0.021432 merged: 0.013891\n",
      "epoch: 37 [2017-11-21 19:38:28]\n",
      "lr 0.047993445245333805\n",
      " 16M: 0.012608  8M: 0.011597  4M: 0.011942  2M: 0.014167  1M: 0.021241 merged: 0.013088\n",
      "epoch: 38 [2017-11-21 19:44:13]\n",
      "lr 0.0476999046002862\n",
      " 16M: 0.011706  8M: 0.011229  4M: 0.011677  2M: 0.013875  1M: 0.020891 merged: 0.012449\n",
      "epoch: 39 [2017-11-21 19:50:00]\n",
      "lr 0.04740454631399772\n",
      " 16M: 0.011965  8M: 0.011265  4M: 0.011734  2M: 0.013606  1M: 0.020734 merged: 0.012459\n",
      "epoch: 40 [2017-11-21 19:55:46]\n",
      "lr 0.04710733619719444\n",
      " 16M: 0.011725  8M: 0.011536  4M: 0.011360  2M: 0.013133  1M: 0.020671 merged: 0.012590\n",
      "epoch: 41 [2017-11-21 20:01:29]\n",
      "lr 0.04680823897515326\n",
      " 16M: 0.011022  8M: 0.010751  4M: 0.011096  2M: 0.013028  1M: 0.019886 merged: 0.011521\n",
      "epoch: 42 [2017-11-21 20:07:13]\n",
      "lr 0.04650721823883479\n",
      " 16M: 0.010033  8M: 0.009896  4M: 0.010530  2M: 0.012490  1M: 0.019323 merged: 0.010513\n",
      "epoch: 43 [2017-11-21 20:12:58]\n",
      "lr 0.046204236393150765\n",
      " 16M: 0.009784  8M: 0.009687  4M: 0.009982  2M: 0.012326  1M: 0.018133 merged: 0.010449\n",
      "epoch: 44 [2017-11-21 20:18:42]\n",
      "lr 0.045899254602157845\n",
      " 16M: 0.010013  8M: 0.009571  4M: 0.010441  2M: 0.012549  1M: 0.019095 merged: 0.010364\n",
      "epoch: 45 [2017-11-21 20:24:25]\n",
      "lr 0.04559223273095164\n",
      " 16M: 0.010054  8M: 0.009621  4M: 0.010124  2M: 0.012044  1M: 0.018718 merged: 0.010388\n",
      "epoch: 46 [2017-11-21 20:30:09]\n",
      "lr 0.045283129284014914\n",
      " 16M: 0.009151  8M: 0.009214  4M: 0.009559  2M: 0.011636  1M: 0.018684 merged: 0.009585\n",
      "epoch: 47 [2017-11-21 20:35:53]\n",
      "lr 0.04497190133975169\n",
      " 16M: 0.009445  8M: 0.009208  4M: 0.009806  2M: 0.012031  1M: 0.018623 merged: 0.009707\n",
      "epoch: 48 [2017-11-21 20:41:40]\n",
      "lr 0.04465850448091506\n",
      " 16M: 0.008768  8M: 0.009005  4M: 0.009610  2M: 0.011840  1M: 0.018543 merged: 0.009370\n",
      "epoch: 49 [2017-11-21 20:47:22]\n",
      "lr 0.044342892720609255\n",
      " 16M: 0.008494  8M: 0.008759  4M: 0.009211  2M: 0.011316  1M: 0.017586 merged: 0.008974\n",
      "epoch: 50 [2017-11-21 20:53:04]\n",
      "lr 0.044025018423517\n",
      " 16M: 0.008457  8M: 0.008857  4M: 0.009342  2M: 0.011512  1M: 0.018494 merged: 0.008887\n",
      "epoch: 51 [2017-11-21 20:58:47]\n",
      "lr 0.04370483222197017\n",
      " 16M: 0.008515  8M: 0.008677  4M: 0.009156  2M: 0.011278  1M: 0.017398 merged: 0.008982\n",
      "epoch: 52 [2017-11-21 21:04:32]\n",
      "lr 0.043382282926444894\n",
      " 16M: 0.008377  8M: 0.008620  4M: 0.009288  2M: 0.011308  1M: 0.017929 merged: 0.008843\n",
      "epoch: 53 [2017-11-21 21:10:18]\n",
      "lr 0.04305731743002185\n",
      " 16M: 0.007963  8M: 0.008280  4M: 0.008792  2M: 0.010947  1M: 0.017299 merged: 0.008353\n",
      "epoch: 54 [2017-11-21 21:16:02]\n",
      "lr 0.04272988060630656\n",
      " 16M: 0.007722  8M: 0.008210  4M: 0.008816  2M: 0.011144  1M: 0.017131 merged: 0.008084\n",
      "epoch: 55 [2017-11-21 21:21:46]\n",
      "lr 0.04239991520025441\n",
      " 16M: 0.007985  8M: 0.008471  4M: 0.008935  2M: 0.010864  1M: 0.016959 merged: 0.008336\n",
      "epoch: 56 [2017-11-21 21:27:30]\n",
      "lr 0.0420673617112877\n",
      " 16M: 0.008147  8M: 0.008269  4M: 0.008704  2M: 0.010730  1M: 0.017090 merged: 0.008324\n",
      "epoch: 57 [2017-11-21 21:33:14]\n",
      "lr 0.041732158268029534\n",
      " 16M: 0.007632  8M: 0.008085  4M: 0.008720  2M: 0.010716  1M: 0.016842 merged: 0.007925\n",
      "epoch: 58 [2017-11-21 21:39:03]\n",
      "lr 0.041394240493907074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16M: 0.007454  8M: 0.007843  4M: 0.008522  2M: 0.010578  1M: 0.016308 merged: 0.007664\n",
      "epoch: 59 [2017-11-21 21:44:46]\n",
      "lr 0.041053541362798006\n",
      " 16M: 0.007308  8M: 0.007783  4M: 0.008319  2M: 0.010497  1M: 0.016683 merged: 0.007510\n",
      "epoch: 60 [2017-11-21 21:50:32]\n",
      "lr 0.04070999104380296\n",
      " 16M: 0.007049  8M: 0.007623  4M: 0.008295  2M: 0.010479  1M: 0.016176 merged: 0.007418\n",
      "epoch: 61 [2017-11-21 21:56:16]\n",
      "lr 0.04036351673412598\n",
      " 16M: 0.007351  8M: 0.007668  4M: 0.008283  2M: 0.010396  1M: 0.016145 merged: 0.007445\n",
      "epoch: 62 [2017-11-21 22:02:02]\n",
      "lr 0.04001404247893005\n",
      " 16M: 0.007179  8M: 0.007600  4M: 0.008291  2M: 0.010378  1M: 0.016245 merged: 0.007516\n",
      "epoch: 63 [2017-11-21 22:07:46]\n",
      "lr 0.03966148897690515\n",
      " 16M: 0.006966  8M: 0.007583  4M: 0.008215  2M: 0.010700  1M: 0.016246 merged: 0.007262\n",
      "epoch: 64 [2017-11-21 22:13:30]\n",
      "lr 0.03930577337013889\n",
      " 16M: 0.006774  8M: 0.007584  4M: 0.008147  2M: 0.010445  1M: 0.016849 merged: 0.006990\n",
      "epoch: 65 [2017-11-21 22:19:12]\n",
      "lr 0.038946809016712394\n",
      " 16M: 0.007006  8M: 0.007454  4M: 0.008043  2M: 0.010467  1M: 0.016380 merged: 0.007242\n",
      "epoch: 66 [2017-11-21 22:24:56]\n",
      "lr 0.03858450524425343\n",
      " 16M: 0.007057  8M: 0.007431  4M: 0.008170  2M: 0.009982  1M: 0.015781 merged: 0.007221\n",
      "epoch: 67 [2017-11-21 22:30:45]\n",
      "lr 0.03821876708246056\n",
      " 16M: 0.006661  8M: 0.007241  4M: 0.007813  2M: 0.010130  1M: 0.015858 merged: 0.006905\n",
      "epoch: 68 [2017-11-21 22:36:28]\n",
      "lr 0.03784949497236286\n",
      " 16M: 0.006431  8M: 0.007237  4M: 0.007769  2M: 0.009974  1M: 0.015775 merged: 0.006685\n",
      "epoch: 69 [2017-11-21 22:42:12]\n",
      "lr 0.03747658444979307\n",
      " 16M: 0.006657  8M: 0.007392  4M: 0.007897  2M: 0.010174  1M: 0.016333 merged: 0.006816\n",
      "epoch: 70 [2017-11-21 22:47:58]\n",
      "lr 0.0370999258002226\n",
      " 16M: 0.006597  8M: 0.007161  4M: 0.007815  2M: 0.010078  1M: 0.015936 merged: 0.006898\n",
      "epoch: 71 [2017-11-21 22:53:46]\n",
      "lr 0.03671940368172628\n",
      " 16M: 0.006318  8M: 0.007090  4M: 0.007576  2M: 0.009832  1M: 0.015603 merged: 0.006552\n",
      "epoch: 72 [2017-11-21 22:59:32]\n",
      "lr 0.03633489671240478\n",
      " 16M: 0.006316  8M: 0.006962  4M: 0.007516  2M: 0.009769  1M: 0.015557 merged: 0.006486\n",
      "epoch: 73 [2017-11-21 23:05:15]\n",
      "lr 0.03594627701808178\n",
      " 16M: 0.006533  8M: 0.007319  4M: 0.007838  2M: 0.009896  1M: 0.015926 merged: 0.006699\n",
      "epoch: 74 [2017-11-21 23:10:59]\n",
      "lr 0.035553409735498295\n",
      " 16M: 0.006393  8M: 0.007198  4M: 0.007772  2M: 0.009962  1M: 0.015648 merged: 0.006607\n",
      "epoch: 75 [2017-11-21 23:16:43]\n",
      "lr 0.03515615246553262\n",
      " 16M: 0.006381  8M: 0.007145  4M: 0.007612  2M: 0.009718  1M: 0.015365 merged: 0.006494\n",
      "epoch: 76 [2017-11-21 23:22:28]\n",
      "lr 0.03475435467016077\n",
      " 16M: 0.006009  8M: 0.006765  4M: 0.007495  2M: 0.009635  1M: 0.015615 merged: 0.006269\n",
      "epoch: 77 [2017-11-21 23:28:11]\n",
      "lr 0.034347857005916346\n",
      " 16M: 0.006128  8M: 0.006925  4M: 0.007472  2M: 0.009682  1M: 0.015352 merged: 0.006359\n",
      "epoch: 78 [2017-11-21 23:33:58]\n",
      "lr 0.0339364905854808\n",
      " 16M: 0.005944  8M: 0.006694  4M: 0.007350  2M: 0.009448  1M: 0.015038 merged: 0.006161\n",
      "epoch: 79 [2017-11-21 23:39:45]\n",
      "lr 0.03352007615769955\n",
      " 16M: 0.005885  8M: 0.006789  4M: 0.007466  2M: 0.009664  1M: 0.015218 merged: 0.006187\n",
      "epoch: 80 [2017-11-21 23:45:32]\n",
      "lr 0.03309842319473132\n",
      " 16M: 0.006021  8M: 0.006783  4M: 0.007357  2M: 0.009482  1M: 0.014993 merged: 0.006183\n",
      "epoch: 81 [2017-11-21 23:51:16]\n",
      "lr 0.03267132887314317\n",
      " 16M: 0.005921  8M: 0.006649  4M: 0.007229  2M: 0.009389  1M: 0.014879 merged: 0.006204\n",
      "epoch: 82 [2017-11-21 23:57:02]\n",
      "lr 0.03223857693349118\n",
      " 16M: 0.005743  8M: 0.006582  4M: 0.007194  2M: 0.009255  1M: 0.014664 merged: 0.006055\n",
      "epoch: 83 [2017-11-22 00:02:47]\n",
      "lr 0.0317999364001908\n",
      " 16M: 0.005821  8M: 0.006689  4M: 0.007204  2M: 0.009409  1M: 0.014965 merged: 0.006059\n",
      "epoch: 84 [2017-11-22 00:08:34]\n",
      "lr 0.031355160140170396\n",
      " 16M: 0.005750  8M: 0.006684  4M: 0.007259  2M: 0.009521  1M: 0.015370 merged: 0.006010\n",
      "epoch: 85 [2017-11-22 00:14:18]\n",
      "lr 0.03090398323477543\n",
      " 16M: 0.005569  8M: 0.006551  4M: 0.007056  2M: 0.009180  1M: 0.014743 merged: 0.005916\n",
      "epoch: 86 [2017-11-22 00:20:05]\n",
      "lr 0.030446121134470178\n",
      " 16M: 0.005582  8M: 0.006473  4M: 0.007113  2M: 0.009176  1M: 0.014678 merged: 0.005861\n",
      "epoch: 87 [2017-11-22 00:25:48]\n",
      "lr 0.02998126755983446\n",
      " 16M: 0.005762  8M: 0.006469  4M: 0.006984  2M: 0.008990  1M: 0.014518 merged: 0.005888\n",
      "epoch: 88 [2017-11-22 00:31:36]\n",
      "lr 0.029509092104873926\n",
      " 16M: 0.005489  8M: 0.006294  4M: 0.006910  2M: 0.009044  1M: 0.014433 merged: 0.005716\n",
      "epoch: 89 [2017-11-22 00:37:21]\n",
      "lr 0.029029237489356888\n",
      " 16M: 0.005282  8M: 0.006197  4M: 0.006795  2M: 0.008963  1M: 0.014579 merged: 0.005575\n",
      "epoch: 90 [2017-11-22 00:43:07]\n",
      "lr 0.028541316395237167\n",
      " 16M: 0.005464  8M: 0.006474  4M: 0.007055  2M: 0.009253  1M: 0.014843 merged: 0.005855\n",
      "epoch: 91 [2017-11-22 00:48:54]\n",
      "lr 0.028044907807525134\n",
      " 16M: 0.005385  8M: 0.006297  4M: 0.006805  2M: 0.009037  1M: 0.014503 merged: 0.005600\n",
      "epoch: 92 [2017-11-22 00:54:38]\n",
      "lr 0.027539552761294706\n",
      " 16M: 0.005684  8M: 0.006399  4M: 0.006901  2M: 0.009110  1M: 0.014584 merged: 0.005837\n",
      "epoch: 93 [2017-11-22 01:00:22]\n",
      "lr 0.027024749372597065\n",
      " 16M: 0.005467  8M: 0.006305  4M: 0.007033  2M: 0.009145  1M: 0.014756 merged: 0.005657\n",
      "epoch: 94 [2017-11-22 01:06:06]\n",
      "lr 0.026499947000159004\n",
      " 16M: 0.005517  8M: 0.006287  4M: 0.006867  2M: 0.008947  1M: 0.014254 merged: 0.005683\n",
      "epoch: 95 [2017-11-22 01:11:57]\n",
      "lr 0.02596453934447493\n",
      " 16M: 0.005177  8M: 0.006076  4M: 0.006589  2M: 0.008753  1M: 0.014494 merged: 0.005417\n",
      "epoch: 96 [2017-11-22 01:17:52]\n",
      "lr 0.025417856237895775\n",
      " 16M: 0.005282  8M: 0.006247  4M: 0.006707  2M: 0.008844  1M: 0.014395 merged: 0.005531\n",
      "epoch: 97 [2017-11-22 01:23:43]\n",
      "lr 0.02485915380880628\n",
      " 16M: 0.005147  8M: 0.006218  4M: 0.006845  2M: 0.009052  1M: 0.014635 merged: 0.005523\n",
      "epoch: 98 [2017-11-22 01:29:36]\n",
      "lr 0.02428760260810931\n",
      " 16M: 0.005105  8M: 0.006128  4M: 0.006697  2M: 0.008925  1M: 0.014142 merged: 0.005416\n",
      "epoch: 99 [2017-11-22 01:35:24]\n",
      "lr 0.02370227315699886\n",
      " 16M: 0.005174  8M: 0.006096  4M: 0.006761  2M: 0.008997  1M: 0.014309 merged: 0.005376\n",
      "epoch: 100 [2017-11-22 01:41:15]\n",
      "lr 0.023102118196575382\n",
      " 16M: 0.005067  8M: 0.006044  4M: 0.006547  2M: 0.008632  1M: 0.013752 merged: 0.005407\n",
      "epoch: 101 [2017-11-22 01:47:04]\n",
      "lr 0.022485950669875843\n",
      " 16M: 0.005128  8M: 0.006056  4M: 0.006765  2M: 0.008905  1M: 0.014418 merged: 0.005422\n",
      "epoch: 102 [2017-11-22 01:52:54]\n",
      "lr 0.021852416110985085\n",
      " 16M: 0.005126  8M: 0.006084  4M: 0.006592  2M: 0.008800  1M: 0.014171 merged: 0.005339\n",
      "epoch: 103 [2017-11-22 01:59:02]\n",
      "lr 0.021199957600127203\n",
      " 16M: 0.005144  8M: 0.006116  4M: 0.006709  2M: 0.008842  1M: 0.014105 merged: 0.005425\n",
      "epoch: 104 [2017-11-22 02:04:51]\n",
      "lr 0.020526770681399003\n",
      " 16M: 0.005073  8M: 0.005996  4M: 0.006618  2M: 0.008708  1M: 0.014205 merged: 0.005315\n",
      "epoch: 105 [2017-11-22 02:10:57]\n",
      "lr 0.019830744488452574\n",
      " 16M: 0.004969  8M: 0.005906  4M: 0.006528  2M: 0.008657  1M: 0.014137 merged: 0.005227\n",
      "epoch: 106 [2017-11-22 02:17:05]\n",
      "lr 0.01910938354123028\n",
      " 16M: 0.004951  8M: 0.005829  4M: 0.006476  2M: 0.008515  1M: 0.013639 merged: 0.005239\n",
      "epoch: 107 [2017-11-22 02:23:07]\n",
      "lr 0.01835970184086314\n",
      " 16M: 0.004884  8M: 0.005783  4M: 0.006412  2M: 0.008512  1M: 0.013776 merged: 0.005177\n",
      "epoch: 108 [2017-11-22 02:29:08]\n",
      "lr 0.01757807623276631\n",
      " 16M: 0.004855  8M: 0.005858  4M: 0.006514  2M: 0.008737  1M: 0.014217 merged: 0.005126\n",
      "epoch: 109 [2017-11-22 02:35:04]\n",
      "lr 0.016760038078849775\n",
      " 16M: 0.004887  8M: 0.005871  4M: 0.006476  2M: 0.008715  1M: 0.014171 merged: 0.005161\n",
      "epoch: 110 [2017-11-22 02:40:59]\n",
      "lr 0.0158999682000954\n",
      " 16M: 0.004715  8M: 0.005772  4M: 0.006399  2M: 0.008401  1M: 0.013414 merged: 0.004991\n",
      "epoch: 111 [2017-11-22 02:46:57]\n",
      "lr 0.01499063377991723\n",
      " 16M: 0.004855  8M: 0.005868  4M: 0.006481  2M: 0.008698  1M: 0.014188 merged: 0.005124\n",
      "epoch: 112 [2017-11-22 02:52:46]\n",
      "lr 0.014022453903762567\n",
      " 16M: 0.004684  8M: 0.005681  4M: 0.006341  2M: 0.008415  1M: 0.013682 merged: 0.004967\n",
      "epoch: 113 [2017-11-22 02:58:43]\n",
      "lr 0.012982269672237465\n",
      " 16M: 0.004655  8M: 0.005723  4M: 0.006278  2M: 0.008432  1M: 0.013551 merged: 0.004959\n",
      "epoch: 114 [2017-11-22 03:04:43]\n",
      "lr 0.01185113657849943\n",
      " 16M: 0.004686  8M: 0.005703  4M: 0.006300  2M: 0.008411  1M: 0.013412 merged: 0.004986\n",
      "epoch: 115 [2017-11-22 03:10:35]\n",
      "lr 0.010599978800063602\n",
      " 16M: 0.004697  8M: 0.005776  4M: 0.006379  2M: 0.008561  1M: 0.014098 merged: 0.004990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 116 [2017-11-22 03:16:27]\n",
      "lr 0.00917985092043157\n",
      " 16M: 0.004669  8M: 0.005690  4M: 0.006247  2M: 0.008355  1M: 0.013432 merged: 0.004938\n",
      "epoch: 117 [2017-11-22 03:22:29]\n",
      "lr 0.007495316889958615\n",
      " 16M: 0.004579  8M: 0.005637  4M: 0.006233  2M: 0.008283  1M: 0.013203 merged: 0.004886\n",
      "epoch: 118 [2017-11-22 03:28:34]\n",
      "lr 0.005299989400031801\n",
      " 16M: 0.004632  8M: 0.005648  4M: 0.006276  2M: 0.008437  1M: 0.013367 merged: 0.004893\n",
      "epoch: 119 [2017-11-22 03:34:22]\n",
      "lr 1e-08\n",
      " 16M: 0.004531  8M: 0.005638  4M: 0.006309  2M: 0.008412  1M: 0.013612 merged: 0.004876\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "writer = SummaryWriter(comment='-growth_rate_{}'.format(growth_rate))\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=args.base_lr, momentum=args.momentum)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, beg, end, reset_lr=None):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if reset_lr != None:\n",
    "            param_group['lr'] = reset_lr\n",
    "            continue\n",
    "        if epoch != 0: \n",
    "            # linear\n",
    "#             param_group['lr'] *= (end-epoch) / (end-beg)\n",
    "#             poly base_lr (1 - iter/max_iter) ^ (power)\n",
    "            param_group['lr'] = args.base_lr * (float(end-epoch)/(end-beg)) ** (args.power)\n",
    "            if param_group['lr'] < 1.0e-8: param_group['lr'] = 1.0e-8\n",
    "        print('lr', param_group['lr'])\n",
    "        \n",
    "# def findLargerInd(target, arr):\n",
    "#     res = list(filter(lambda x: x>target, arr))\n",
    "#     print('res',res)\n",
    "#     if len(res) == 0: return -1\n",
    "#     return res[0]\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    net.train()\n",
    "    print('epoch: {} [{}]'.format(epoch, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    \n",
    "    if epoch < args.training_thresholds[-1]: adjust_learning_rate(optimizer, epoch%ss, beg=0, end=ss-1)\n",
    "    else: adjust_learning_rate(optimizer, epoch, beg=args.training_thresholds[-1], end=args.epoches-1)\n",
    "    \n",
    "    run_losses = [0] * len(args.training_thresholds)\n",
    "    run_cnts   = [0.00001] * len(args.training_thresholds)\n",
    "    if (epoch in args.training_thresholds) == True: \n",
    "        adjust_learning_rate(optimizer, epoch, reset_lr=args.base_lr, beg=-1, end=-1)\n",
    "    writer.add_scalar('learning rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    for ind, data in enumerate(train_loader, 0):\n",
    "#         if  ind == 1 : break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        im = input_img[0,:,:,:].numpy(); im = im.transpose(1,2,0); im = im[:,:,::-1]*255\n",
    "        \n",
    "        if test_scene[0] == 'alley_1':\n",
    "            print('alley_1 yes')\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda()\n",
    "            gt_albedo = gt_albedo.cuda()\n",
    "            gt_shading = gt_shading.cuda()\n",
    "#         run_losses = [0] * len(mse_losses)\n",
    "#         run_cnts = [0.00001] * len(mse_losses)\n",
    "        if args.display_curindex % args.display_interval == 0:\n",
    "            cv2.imwrite('snapshot{}/input.png'.format(args.gpu_num), im)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_predict = net(ft_pretreained)\n",
    "        for i, threshold in enumerate(args.training_thresholds):\n",
    "#             threshold = args.training_thresholds[i]\n",
    "            if epoch >= threshold:\n",
    "#             if epoch >= 0:\n",
    "                if i == 5: s = 1\n",
    "                else: s = (2**(i+1))\n",
    "                gt = gt_albedo.cpu().data.numpy()\n",
    "                n,c,h,w = gt.shape\n",
    "                gt = gt[0,:,:,:]\n",
    "                gt = gt.transpose((1,2,0))\n",
    "                gt = cv2.resize(gt, (h//s, w//s))\n",
    "#                 gt = cv2.resize(gt, (h,w))\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    cv2.imwrite('snapshot{}/gt-{}-{}.png'.format(args.gpu_num, epoch, i), gt[:,:,::-1]*255)\n",
    "                gt = gt.transpose((2,0,1))\n",
    "                gt = gt[np.newaxis, :]\n",
    "                gt = Variable(torch.from_numpy(gt))\n",
    "                if use_gpu: gt = gt.cuda()\n",
    "                loss = mse_losses[i](ft_predict[i], gt)\n",
    "                loss_data = loss.data.cpu().numpy()\n",
    "                writer.add_scalar('{}th train iters loss'.format(i), loss_data, global_step=args.display_curindex)\n",
    "                ma_ = ft_predict[i].max().cpu().data.numpy()\n",
    "                mi_ = ft_predict[i].min().cpu().data.numpy()\n",
    "                #print('mi', mi_, 'ma', ma_)\n",
    "#                 writer.add_scalars('{}th train predict'.format(i), {'max': ma_, 'min': mi_}, global_step=args.display_curindex)\n",
    "#                 run_cnts[i] += 1\n",
    "                run_losses[i] += loss.data.cpu().numpy()[0]\n",
    "                loss.backward(retain_graph=True)\n",
    "                run_cnts[i] += 1\n",
    "#                 print('i = ', i, '; weig\\n', net.upsample01.weight[0,0,0:4,0:4].data.cpu().numpy())\n",
    "#                 print('i = ', i, '; grad\\n', net.upsample01.weight.grad[0,0,0:4,0:4].data.cpu().numpy())\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    im = ft_predict[i].cpu().data.numpy()[0].transpose((1,2,0)) * 255\n",
    "                    cv2.imwrite('snapshot{}/train-{}-{}.png'.format(args.gpu_num, epoch, i), im[:,:,::-1])\n",
    "        optimizer.step()\n",
    "        args.display_curindex += 1\n",
    "\n",
    "    \"\"\" every epoch \"\"\"\n",
    "#     loss_output = 'ind: ' + str(args.display_curindex)\n",
    "    loss_output = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,v in enumerate(run_losses):\n",
    "        if i == len(run_losses)-1: \n",
    "            loss_output += ' merged: %6f' % (run_losses[i] / run_cnts[i])\n",
    "            continue\n",
    "        loss_output += ' %2dM: %6f' % ((2**(4-i)), (run_losses[i] / run_cnts[i]))\n",
    "    print(loss_output)\n",
    "    # save at every epoch\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'args' : args,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, 'snapshot{}/snapshot-{}.pth.tar'.format(args.gpu_num, epoch))\n",
    "    \n",
    "    # test \n",
    "    test_losses_trainphase = [0] * len(args.training_thresholds)\n",
    "    test_cnts_trainphase   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "        \n",
    "        pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(ft_pretreained)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses_trainphase[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts_trainphase[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_train-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = [0] * len(args.training_thresholds)\n",
    "    test_cnts   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "#         if ind == 1: break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "            \n",
    "        pretrained.eval(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(ft_pretreained)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_test-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "    \n",
    "    writer.add_scalars('16M loss', {\n",
    "        'train 16M ': np.array([run_losses[0]/ run_cnts[0]]),\n",
    "        'test_trainphase 16M ': np.array([test_losses_trainphase[0]/ test_cnts_trainphase[0]]),\n",
    "        'test 16M ': np.array([test_losses[0]/ test_cnts[0]])\n",
    "    }, global_step=epoch)  \n",
    "    writer.add_scalars('8M loss', {\n",
    "        'train 8M ': np.array([run_losses[1]/ run_cnts[1]]),\n",
    "        'test_trainphase 8M ': np.array([test_losses_trainphase[1]/ test_cnts_trainphase[1]]),\n",
    "        'test 8M ': np.array([test_losses[1]/ test_cnts[1]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('4M loss', {\n",
    "        'train 4M ': np.array([run_losses[2]/ run_cnts[2]]),\n",
    "        'test_trainphase 4M ': np.array([test_losses_trainphase[2]/ test_cnts_trainphase[2]]),\n",
    "        'test 4M ': np.array([test_losses[2]/ test_cnts[2]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('2M loss', {\n",
    "        'train 2M ': np.array([run_losses[3]/ run_cnts[3]]),\n",
    "        'test_trainphase 2M ': np.array([test_losses_trainphase[3]/ test_cnts_trainphase[3]]),\n",
    "        'test 2M ': np.array([test_losses[3]/ test_cnts[3]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('1M loss', {\n",
    "        'train 1M ': np.array([run_losses[4]/ run_cnts[4]]),\n",
    "        'test_trainphase 1M ': np.array([test_losses_trainphase[4]/ test_cnts_trainphase[4]]),\n",
    "        'test 1M ': np.array([test_losses[4]/ test_cnts[4]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('merged loss', {\n",
    "        'train merged ': np.array([run_losses[5]/ run_cnts[5]]),\n",
    "        'test_trainphase merged ': np.array([test_losses_trainphase[5]/ test_cnts_trainphase[5]]),\n",
    "        'test merged ': np.array([test_losses[5]/ test_cnts[5]])\n",
    "    }, global_step=epoch) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"10240,10240\"), format='svg')\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.zeros(1,3,256,256))\n",
    "y = net(x.cuda())\n",
    "g = make_dot(y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.render('net') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
