{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, platform, datetime, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import functional as F\n",
    "# import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as denseinv\n",
    "from scipy import sparse\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import inv as spinv\n",
    "import scipy.misc\n",
    "\n",
    "from myimagefolder import MyImageFolder\n",
    "from mymodel import PreTrainedModel, GradientNet\n",
    "from myargs import Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('debian', 'jessie/sid', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Args()\n",
    "args.test_scene = 'alley_1'\n",
    "args.arch = \"densenet121\"\n",
    "args.epoches = 500\n",
    "args.epoches_unary_threshold = 0\n",
    "args.image_h = 256\n",
    "args.image_w = 256\n",
    "args.img_extentions = [\"png\"]\n",
    "args.training_thresholds = [250,200,150,50,0,300]\n",
    "args.base_lr = 1\n",
    "args.lr = args.base_lr\n",
    "args.snapshot_interval = 5000\n",
    "args.debug = True\n",
    "args.gpu_num = 1\n",
    "growth_rate = (4*(2**(args.gpu_num)))\n",
    "args.display_interval = 50\n",
    "args.display_curindex = 0\n",
    "\n",
    "system_ = platform.system()\n",
    "system_dist, system_version, _ = platform.dist()\n",
    "if system_ == \"Darwin\": \n",
    "    args.train_dir = '/Volumes/Transcend/dataset/sintel2'\n",
    "    args.pretrained = False\n",
    "elif platform.dist() ==  ('debian', 'jessie/sid', ''):\n",
    "    args.train_dir = '/home/lwp/workspace/sintel2'\n",
    "    args.pretrained = True\n",
    "elif platform.dist() == ('debian', 'stretch/sid', ''):\n",
    "    args.train_dir = '/home/cad/lwp/workspace/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "\n",
    "if platform.system() == 'Linux': use_gpu = True\n",
    "else: use_gpu = False\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "    \n",
    "\n",
    "print(platform.dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyImageFolder(args.train_dir, 'train',\n",
    "                       transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ), random_crop=True, \n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "test_dataset = MyImageFolder(args.train_dir, 'test', \n",
    "                       transforms.Compose(\n",
    "        [transforms.CenterCrop((args.image_h, args.image_w)),\n",
    "         transforms.ToTensor()]\n",
    "    ), random_crop=False,\n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,1,True,num_workers=1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,1,True,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "[Defination](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)\n",
    "* DenseNet-121: num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16)\n",
    "    * First Convolution: 32M -> 16M -> 8M\n",
    "    * every transition: 8M -> 4M -> 2M (downsample 1/2, except the last block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "densenet = models.__dict__[args.arch](pretrained=args.pretrained)\n",
    "\n",
    "for param in densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu: densenet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = 6\n",
    "\n",
    "args.display_curindex = 0\n",
    "args.base_lr = 0.05\n",
    "args.display_interval = 20\n",
    "args.momentum = 0.9\n",
    "args.epoches = 120\n",
    "args.training_thresholds = [ss*4,ss*3,ss*2,ss*1,ss*0,ss*5]\n",
    "args.power = 0.5\n",
    "\n",
    "\n",
    "\n",
    "pretrained = PreTrainedModel(densenet)\n",
    "if use_gpu: \n",
    "    pretrained.cuda()\n",
    "\n",
    "\n",
    "net = GradientNet(growth_rate=growth_rate)\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "if use_gpu: \n",
    "    mse_losses = [nn.MSELoss().cuda()] * 6\n",
    "    test_losses = [nn.MSELoss().cuda()] * 6\n",
    "else:\n",
    "    mse_losses = [nn.MSELoss()] * 6\n",
    "    test_losses = [nn.MSELoss()] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [2017-11-21 17:40:04]\n",
      "lr 0.05\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.055995 merged: 0.000000\n",
      "epoch: 1 [2017-11-21 17:41:57]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.049208 merged: 0.000000\n",
      "epoch: 2 [2017-11-21 17:43:50]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.047502 merged: 0.000000\n",
      "epoch: 3 [2017-11-21 17:45:41]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.045861 merged: 0.000000\n",
      "epoch: 4 [2017-11-21 17:47:33]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.042942 merged: 0.000000\n",
      "epoch: 5 [2017-11-21 17:49:26]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.041904 merged: 0.000000\n",
      "epoch: 6 [2017-11-21 17:51:19]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.048487  1M: 0.045408 merged: 0.000000\n",
      "epoch: 7 [2017-11-21 17:53:31]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.044846  1M: 0.044687 merged: 0.000000\n",
      "epoch: 8 [2017-11-21 17:55:43]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.039257  1M: 0.039106 merged: 0.000000\n",
      "epoch: 9 [2017-11-21 17:57:56]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.030131  1M: 0.034028 merged: 0.000000\n",
      "epoch: 10 [2017-11-21 18:00:10]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.024124  1M: 0.030004 merged: 0.000000\n",
      "epoch: 11 [2017-11-21 18:02:22]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.022335  1M: 0.028380 merged: 0.000000\n",
      "epoch: 12 [2017-11-21 18:04:34]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.048452  2M: 0.029604  1M: 0.034685 merged: 0.000000\n",
      "epoch: 13 [2017-11-21 18:07:05]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.038831  2M: 0.024342  1M: 0.030010 merged: 0.000000\n",
      "epoch: 14 [2017-11-21 18:09:35]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.029331  2M: 0.021477  1M: 0.028652 merged: 0.000000\n",
      "epoch: 15 [2017-11-21 18:12:05]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.022728  2M: 0.020328  1M: 0.026492 merged: 0.000000\n",
      "epoch: 16 [2017-11-21 18:14:32]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.019621  2M: 0.018477  1M: 0.024788 merged: 0.000000\n",
      "epoch: 17 [2017-11-21 18:17:00]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.019611  2M: 0.018279  1M: 0.023365 merged: 0.000000\n",
      "epoch: 18 [2017-11-21 18:19:28]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.047451  4M: 0.020998  2M: 0.020092  1M: 0.026975 merged: 0.000000\n",
      "epoch: 19 [2017-11-21 18:22:06]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.033968  4M: 0.019049  2M: 0.018897  1M: 0.026792 merged: 0.000000\n",
      "epoch: 20 [2017-11-21 18:24:44]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.024432  4M: 0.016376  2M: 0.017460  1M: 0.024029 merged: 0.000000\n",
      "epoch: 21 [2017-11-21 18:27:22]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.020982  4M: 0.015712  2M: 0.017136  1M: 0.024195 merged: 0.000000\n",
      "epoch: 22 [2017-11-21 18:30:01]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.018209  4M: 0.014467  2M: 0.016216  1M: 0.022361 merged: 0.000000\n",
      "epoch: 23 [2017-11-21 18:32:42]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.017454  4M: 0.014652  2M: 0.016416  1M: 0.022751 merged: 0.000000\n",
      "epoch: 24 [2017-11-21 18:35:19]\n",
      "lr 1e-08\n",
      " 16M: 0.043795  8M: 0.019385  4M: 0.015555  2M: 0.016618  1M: 0.023509 merged: 0.000000\n",
      "epoch: 25 [2017-11-21 18:38:30]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.031538  8M: 0.017654  4M: 0.014909  2M: 0.016924  1M: 0.023567 merged: 0.000000\n",
      "epoch: 26 [2017-11-21 18:41:44]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.024005  8M: 0.015513  4M: 0.013299  2M: 0.015760  1M: 0.022426 merged: 0.000000\n",
      "epoch: 27 [2017-11-21 18:44:53]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.020152  8M: 0.014826  4M: 0.012924  2M: 0.015241  1M: 0.021839 merged: 0.000000\n",
      "epoch: 28 [2017-11-21 18:48:04]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.016833  8M: 0.013471  4M: 0.012111  2M: 0.014342  1M: 0.020711 merged: 0.000000\n",
      "epoch: 29 [2017-11-21 18:51:17]\n",
      "lr 1e-08\n",
      " 16M: 0.014879  8M: 0.012797  4M: 0.012123  2M: 0.014540  1M: 0.021152 merged: 0.000000\n",
      "epoch: 30 [2017-11-21 18:54:28]\n",
      "lr 0.05\n",
      " 16M: 0.020367  8M: 0.016169  4M: 0.014902  2M: 0.016826  1M: 0.023762 merged: 0.035464\n",
      "epoch: 31 [2017-11-21 18:59:00]\n",
      "lr 0.04971830761761256\n",
      " 16M: 0.020922  8M: 0.016355  4M: 0.016300  2M: 0.016628  1M: 0.024189 merged: 0.023990\n",
      "epoch: 32 [2017-11-21 19:03:28]\n",
      "lr 0.04943501011144937\n",
      " 16M: 0.018079  8M: 0.016710  4M: 0.015471  2M: 0.015907  1M: 0.023090 merged: 0.020878\n",
      "epoch: 33 [2017-11-21 19:08:06]\n",
      "lr 0.04915007972606608\n",
      " 16M: 0.019073  8M: 0.015616  4M: 0.015997  2M: 0.016010  1M: 0.023063 merged: 0.020340\n",
      "epoch: 34 [2017-11-21 19:12:43]\n",
      "lr 0.04886348789677424\n",
      " 16M: 0.015314  8M: 0.014485  4M: 0.014335  2M: 0.015202  1M: 0.022275 merged: 0.016799\n",
      "epoch: 35 [2017-11-21 19:17:15]\n",
      "lr 0.04857520521621862\n",
      " 16M: 0.014447  8M: 0.013997  4M: 0.014213  2M: 0.015324  1M: 0.022012 merged: 0.016109\n",
      "epoch: 36 [2017-11-21 19:21:47]\n",
      "lr 0.04828520139915856\n",
      " 16M: 0.013456  8M: 0.013151  4M: 0.013592  2M: 0.015116  1M: 0.022733 merged: 0.014945\n",
      "epoch: 37 [2017-11-21 19:26:20]\n",
      "lr 0.047993445245333805\n",
      " 16M: 0.012919  8M: 0.012985  4M: 0.013323  2M: 0.014697  1M: 0.022284 merged: 0.014327\n",
      "epoch: 38 [2017-11-21 19:30:56]\n",
      "lr 0.0476999046002862\n",
      " 16M: 0.012300  8M: 0.012422  4M: 0.012745  2M: 0.014676  1M: 0.021050 merged: 0.013770\n",
      "epoch: 39 [2017-11-21 19:35:36]\n",
      "lr 0.04740454631399772\n",
      " 16M: 0.012560  8M: 0.012160  4M: 0.012594  2M: 0.014134  1M: 0.021295 merged: 0.013435\n",
      "epoch: 40 [2017-11-21 19:40:02]\n",
      "lr 0.04710733619719444\n",
      " 16M: 0.012160  8M: 0.012578  4M: 0.012764  2M: 0.014719  1M: 0.022176 merged: 0.013355\n",
      "epoch: 41 [2017-11-21 19:44:41]\n",
      "lr 0.04680823897515326\n",
      " 16M: 0.011780  8M: 0.012191  4M: 0.012009  2M: 0.014142  1M: 0.021355 merged: 0.013186\n",
      "epoch: 42 [2017-11-21 19:49:21]\n",
      "lr 0.04650721823883479\n",
      " 16M: 0.011209  8M: 0.011837  4M: 0.011685  2M: 0.013770  1M: 0.020569 merged: 0.012530\n",
      "epoch: 43 [2017-11-21 19:53:58]\n",
      "lr 0.046204236393150765\n",
      " 16M: 0.011181  8M: 0.011671  4M: 0.011691  2M: 0.013755  1M: 0.021060 merged: 0.012229\n",
      "epoch: 44 [2017-11-21 19:58:38]\n",
      "lr 0.045899254602157845\n",
      " 16M: 0.010676  8M: 0.011008  4M: 0.011100  2M: 0.013100  1M: 0.020258 merged: 0.011819\n",
      "epoch: 45 [2017-11-21 20:03:16]\n",
      "lr 0.04559223273095164\n",
      " 16M: 0.010824  8M: 0.011069  4M: 0.011148  2M: 0.013515  1M: 0.019915 merged: 0.011758\n",
      "epoch: 46 [2017-11-21 20:07:55]\n",
      "lr 0.045283129284014914\n",
      " 16M: 0.010232  8M: 0.010956  4M: 0.011264  2M: 0.013122  1M: 0.020118 merged: 0.011426\n",
      "epoch: 47 [2017-11-21 20:12:35]\n",
      "lr 0.04497190133975169\n",
      " 16M: 0.009937  8M: 0.010563  4M: 0.010629  2M: 0.012737  1M: 0.019574 merged: 0.010968\n",
      "epoch: 48 [2017-11-21 20:17:15]\n",
      "lr 0.04465850448091506\n",
      " 16M: 0.009511  8M: 0.010174  4M: 0.010408  2M: 0.012554  1M: 0.019134 merged: 0.010544\n",
      "epoch: 49 [2017-11-21 20:21:55]\n",
      "lr 0.044342892720609255\n",
      " 16M: 0.009822  8M: 0.010423  4M: 0.010898  2M: 0.012749  1M: 0.019048 merged: 0.010929\n",
      "epoch: 50 [2017-11-21 20:26:36]\n",
      "lr 0.044025018423517\n",
      " 16M: 0.009423  8M: 0.010081  4M: 0.010239  2M: 0.012439  1M: 0.018874 merged: 0.010301\n",
      "epoch: 51 [2017-11-21 20:31:14]\n",
      "lr 0.04370483222197017\n",
      " 16M: 0.009370  8M: 0.010180  4M: 0.010687  2M: 0.012647  1M: 0.019020 merged: 0.010385\n",
      "epoch: 52 [2017-11-21 20:35:55]\n",
      "lr 0.043382282926444894\n",
      " 16M: 0.009378  8M: 0.010039  4M: 0.010509  2M: 0.012487  1M: 0.018315 merged: 0.010281\n",
      "epoch: 53 [2017-11-21 20:40:33]\n",
      "lr 0.04305731743002185\n",
      " 16M: 0.009116  8M: 0.009893  4M: 0.010284  2M: 0.012288  1M: 0.018706 merged: 0.010153\n",
      "epoch: 54 [2017-11-21 20:45:12]\n",
      "lr 0.04272988060630656\n",
      " 16M: 0.008778  8M: 0.009522  4M: 0.009843  2M: 0.011868  1M: 0.018379 merged: 0.009746\n",
      "epoch: 55 [2017-11-21 20:49:51]\n",
      "lr 0.04239991520025441\n",
      " 16M: 0.008698  8M: 0.009464  4M: 0.009695  2M: 0.011860  1M: 0.017860 merged: 0.009616\n",
      "epoch: 56 [2017-11-21 20:54:27]\n",
      "lr 0.0420673617112877\n",
      " 16M: 0.008595  8M: 0.009620  4M: 0.009876  2M: 0.011942  1M: 0.018114 merged: 0.009515\n",
      "epoch: 57 [2017-11-21 20:59:08]\n",
      "lr 0.041732158268029534\n",
      " 16M: 0.008562  8M: 0.009618  4M: 0.009906  2M: 0.011831  1M: 0.017615 merged: 0.009451\n",
      "epoch: 58 [2017-11-21 21:03:46]\n",
      "lr 0.041394240493907074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16M: 0.008132  8M: 0.009243  4M: 0.009540  2M: 0.011491  1M: 0.017138 merged: 0.009114\n",
      "epoch: 59 [2017-11-21 21:08:24]\n",
      "lr 0.041053541362798006\n",
      " 16M: 0.008658  8M: 0.009830  4M: 0.010032  2M: 0.011929  1M: 0.017485 merged: 0.009530\n",
      "epoch: 60 [2017-11-21 21:13:05]\n",
      "lr 0.04070999104380296\n",
      " 16M: 0.008116  8M: 0.009349  4M: 0.009548  2M: 0.011531  1M: 0.017049 merged: 0.008917\n",
      "epoch: 61 [2017-11-21 21:17:43]\n",
      "lr 0.04036351673412598\n",
      " 16M: 0.008053  8M: 0.009119  4M: 0.009494  2M: 0.011436  1M: 0.017350 merged: 0.009002\n",
      "epoch: 62 [2017-11-21 21:22:27]\n",
      "lr 0.04001404247893005\n",
      " 16M: 0.008018  8M: 0.009149  4M: 0.009547  2M: 0.011417  1M: 0.017619 merged: 0.008744\n",
      "epoch: 63 [2017-11-21 21:27:06]\n",
      "lr 0.03966148897690515\n",
      " 16M: 0.007811  8M: 0.008922  4M: 0.009133  2M: 0.011309  1M: 0.017150 merged: 0.008391\n",
      "epoch: 64 [2017-11-21 21:31:46]\n",
      "lr 0.03930577337013889\n",
      " 16M: 0.007552  8M: 0.008802  4M: 0.009011  2M: 0.010948  1M: 0.016830 merged: 0.008419\n",
      "epoch: 65 [2017-11-21 21:36:23]\n",
      "lr 0.038946809016712394\n",
      " 16M: 0.007681  8M: 0.009039  4M: 0.009109  2M: 0.011232  1M: 0.016828 merged: 0.008506\n",
      "epoch: 66 [2017-11-21 21:41:06]\n",
      "lr 0.03858450524425343\n",
      " 16M: 0.007883  8M: 0.009017  4M: 0.009297  2M: 0.011578  1M: 0.017765 merged: 0.008560\n",
      "epoch: 67 [2017-11-21 21:45:45]\n",
      "lr 0.03821876708246056\n",
      " 16M: 0.007678  8M: 0.008862  4M: 0.009133  2M: 0.011373  1M: 0.017198 merged: 0.008393\n",
      "epoch: 68 [2017-11-21 21:50:25]\n",
      "lr 0.03784949497236286\n",
      " 16M: 0.007425  8M: 0.008540  4M: 0.008909  2M: 0.011024  1M: 0.016697 merged: 0.008099\n",
      "epoch: 69 [2017-11-21 21:55:03]\n",
      "lr 0.03747658444979307\n",
      " 16M: 0.007353  8M: 0.008662  4M: 0.009101  2M: 0.011229  1M: 0.016583 merged: 0.008099\n",
      "epoch: 70 [2017-11-21 21:59:43]\n",
      "lr 0.0370999258002226\n",
      " 16M: 0.007404  8M: 0.008656  4M: 0.009127  2M: 0.011348  1M: 0.017087 merged: 0.008082\n",
      "epoch: 71 [2017-11-21 22:04:23]\n",
      "lr 0.03671940368172628\n",
      " 16M: 0.007379  8M: 0.008850  4M: 0.009163  2M: 0.011231  1M: 0.016854 merged: 0.008012\n",
      "epoch: 72 [2017-11-21 22:09:02]\n",
      "lr 0.03633489671240478\n",
      " 16M: 0.007133  8M: 0.008404  4M: 0.009158  2M: 0.011203  1M: 0.016930 merged: 0.007868\n",
      "epoch: 73 [2017-11-21 22:13:41]\n",
      "lr 0.03594627701808178\n",
      " 16M: 0.006851  8M: 0.008095  4M: 0.008435  2M: 0.010474  1M: 0.015930 merged: 0.007480\n",
      "epoch: 74 [2017-11-21 22:18:21]\n",
      "lr 0.035553409735498295\n",
      " 16M: 0.007025  8M: 0.008375  4M: 0.008695  2M: 0.010765  1M: 0.016427 merged: 0.007671\n",
      "epoch: 75 [2017-11-21 22:22:59]\n",
      "lr 0.03515615246553262\n",
      " 16M: 0.007068  8M: 0.008327  4M: 0.008917  2M: 0.010802  1M: 0.016393 merged: 0.007686\n",
      "epoch: 76 [2017-11-21 22:27:38]\n",
      "lr 0.03475435467016077\n",
      " 16M: 0.006979  8M: 0.008222  4M: 0.008562  2M: 0.010369  1M: 0.016270 merged: 0.007562\n",
      "epoch: 77 [2017-11-21 22:32:14]\n",
      "lr 0.034347857005916346\n",
      " 16M: 0.006771  8M: 0.008192  4M: 0.008595  2M: 0.010678  1M: 0.016294 merged: 0.007535\n",
      "epoch: 78 [2017-11-21 22:36:54]\n",
      "lr 0.0339364905854808\n",
      " 16M: 0.006910  8M: 0.008207  4M: 0.008588  2M: 0.010660  1M: 0.016237 merged: 0.007476\n",
      "epoch: 79 [2017-11-21 22:41:30]\n",
      "lr 0.03352007615769955\n",
      " 16M: 0.006855  8M: 0.008021  4M: 0.008550  2M: 0.010652  1M: 0.016013 merged: 0.007367\n",
      "epoch: 80 [2017-11-21 22:46:10]\n",
      "lr 0.03309842319473132\n",
      " 16M: 0.006872  8M: 0.008018  4M: 0.008469  2M: 0.010544  1M: 0.016150 merged: 0.007444\n",
      "epoch: 81 [2017-11-21 22:50:47]\n",
      "lr 0.03267132887314317\n",
      " 16M: 0.006795  8M: 0.007987  4M: 0.008419  2M: 0.010524  1M: 0.015931 merged: 0.007229\n",
      "epoch: 82 [2017-11-21 22:55:27]\n",
      "lr 0.03223857693349118\n",
      " 16M: 0.006724  8M: 0.007937  4M: 0.008335  2M: 0.010357  1M: 0.016114 merged: 0.007186\n",
      "epoch: 83 [2017-11-21 23:00:05]\n",
      "lr 0.0317999364001908\n",
      " 16M: 0.006579  8M: 0.007766  4M: 0.008259  2M: 0.010181  1M: 0.015681 merged: 0.007140\n",
      "epoch: 84 [2017-11-21 23:04:46]\n",
      "lr 0.031355160140170396\n",
      " 16M: 0.006695  8M: 0.007963  4M: 0.008512  2M: 0.010491  1M: 0.015962 merged: 0.007212\n",
      "epoch: 85 [2017-11-21 23:09:25]\n",
      "lr 0.03090398323477543\n",
      " 16M: 0.006528  8M: 0.007860  4M: 0.008247  2M: 0.010128  1M: 0.015707 merged: 0.006984\n",
      "epoch: 86 [2017-11-21 23:14:06]\n",
      "lr 0.030446121134470178\n",
      " 16M: 0.006522  8M: 0.007888  4M: 0.008336  2M: 0.010463  1M: 0.015935 merged: 0.007030\n",
      "epoch: 87 [2017-11-21 23:18:43]\n",
      "lr 0.02998126755983446\n",
      " 16M: 0.006458  8M: 0.007861  4M: 0.008274  2M: 0.010236  1M: 0.015679 merged: 0.007031\n",
      "epoch: 88 [2017-11-21 23:23:21]\n",
      "lr 0.029509092104873926\n",
      " 16M: 0.006396  8M: 0.007775  4M: 0.008074  2M: 0.010104  1M: 0.015398 merged: 0.006918\n",
      "epoch: 89 [2017-11-21 23:28:00]\n",
      "lr 0.029029237489356888\n",
      " 16M: 0.006454  8M: 0.007690  4M: 0.008157  2M: 0.010113  1M: 0.015606 merged: 0.006845\n",
      "epoch: 90 [2017-11-21 23:32:38]\n",
      "lr 0.028541316395237167\n",
      " 16M: 0.006321  8M: 0.007520  4M: 0.008120  2M: 0.009958  1M: 0.015134 merged: 0.006812\n",
      "epoch: 91 [2017-11-21 23:37:20]\n",
      "lr 0.028044907807525134\n",
      " 16M: 0.006357  8M: 0.007666  4M: 0.008267  2M: 0.010184  1M: 0.015639 merged: 0.006907\n",
      "epoch: 92 [2017-11-21 23:42:00]\n",
      "lr 0.027539552761294706\n",
      " 16M: 0.006152  8M: 0.007438  4M: 0.007977  2M: 0.009926  1M: 0.015432 merged: 0.006640\n",
      "epoch: 93 [2017-11-21 23:46:41]\n",
      "lr 0.027024749372597065\n",
      " 16M: 0.006301  8M: 0.007520  4M: 0.008076  2M: 0.010019  1M: 0.015418 merged: 0.006712\n",
      "epoch: 94 [2017-11-21 23:51:18]\n",
      "lr 0.026499947000159004\n",
      " 16M: 0.006146  8M: 0.007474  4M: 0.007977  2M: 0.009927  1M: 0.015171 merged: 0.006580\n",
      "epoch: 95 [2017-11-21 23:55:55]\n",
      "lr 0.02596453934447493\n",
      " 16M: 0.006099  8M: 0.007440  4M: 0.008044  2M: 0.010029  1M: 0.015382 merged: 0.006594\n",
      "epoch: 96 [2017-11-22 00:00:35]\n",
      "lr 0.025417856237895775\n",
      " 16M: 0.006109  8M: 0.007476  4M: 0.008056  2M: 0.009982  1M: 0.015486 merged: 0.006579\n",
      "epoch: 97 [2017-11-22 00:05:13]\n",
      "lr 0.02485915380880628\n",
      " 16M: 0.006020  8M: 0.007349  4M: 0.007912  2M: 0.009797  1M: 0.015213 merged: 0.006568\n",
      "epoch: 98 [2017-11-22 00:09:53]\n",
      "lr 0.02428760260810931\n",
      " 16M: 0.005929  8M: 0.007248  4M: 0.007668  2M: 0.009375  1M: 0.014535 merged: 0.006404\n",
      "epoch: 99 [2017-11-22 00:14:30]\n",
      "lr 0.02370227315699886\n",
      " 16M: 0.006016  8M: 0.007357  4M: 0.007963  2M: 0.010013  1M: 0.015286 merged: 0.006467\n",
      "epoch: 100 [2017-11-22 00:19:09]\n",
      "lr 0.023102118196575382\n",
      " 16M: 0.005904  8M: 0.007227  4M: 0.007808  2M: 0.009704  1M: 0.014684 merged: 0.006322\n",
      "epoch: 101 [2017-11-22 00:23:45]\n",
      "lr 0.022485950669875843\n",
      " 16M: 0.005937  8M: 0.007199  4M: 0.007791  2M: 0.009800  1M: 0.014820 merged: 0.006439\n",
      "epoch: 102 [2017-11-22 00:28:23]\n",
      "lr 0.021852416110985085\n",
      " 16M: 0.005990  8M: 0.007184  4M: 0.007706  2M: 0.009673  1M: 0.015073 merged: 0.006259\n",
      "epoch: 103 [2017-11-22 00:33:01]\n",
      "lr 0.021199957600127203\n",
      " 16M: 0.005933  8M: 0.007350  4M: 0.007885  2M: 0.009868  1M: 0.014922 merged: 0.006460\n",
      "epoch: 104 [2017-11-22 00:37:38]\n",
      "lr 0.020526770681399003\n",
      " 16M: 0.005778  8M: 0.007074  4M: 0.007645  2M: 0.009695  1M: 0.014899 merged: 0.006190\n",
      "epoch: 105 [2017-11-22 00:42:18]\n",
      "lr 0.019830744488452574\n",
      " 16M: 0.005837  8M: 0.007091  4M: 0.007598  2M: 0.009707  1M: 0.014686 merged: 0.006235\n",
      "epoch: 106 [2017-11-22 00:46:56]\n",
      "lr 0.01910938354123028\n",
      " 16M: 0.005763  8M: 0.007094  4M: 0.007551  2M: 0.009564  1M: 0.014670 merged: 0.006199\n",
      "epoch: 107 [2017-11-22 00:51:33]\n",
      "lr 0.01835970184086314\n",
      " 16M: 0.005781  8M: 0.007085  4M: 0.007592  2M: 0.009541  1M: 0.014583 merged: 0.006194\n",
      "epoch: 108 [2017-11-22 00:56:11]\n",
      "lr 0.01757807623276631\n",
      " 16M: 0.005616  8M: 0.007006  4M: 0.007477  2M: 0.009344  1M: 0.014410 merged: 0.006046\n",
      "epoch: 109 [2017-11-22 01:00:51]\n",
      "lr 0.016760038078849775\n",
      " 16M: 0.005742  8M: 0.007224  4M: 0.007863  2M: 0.009841  1M: 0.014801 merged: 0.006254\n",
      "epoch: 110 [2017-11-22 01:05:31]\n",
      "lr 0.0158999682000954\n",
      " 16M: 0.005707  8M: 0.007007  4M: 0.007674  2M: 0.009679  1M: 0.014936 merged: 0.006065\n",
      "epoch: 111 [2017-11-22 01:10:15]\n",
      "lr 0.01499063377991723\n",
      " 16M: 0.005694  8M: 0.007067  4M: 0.007600  2M: 0.009624  1M: 0.014604 merged: 0.006068\n",
      "epoch: 112 [2017-11-22 01:15:00]\n",
      "lr 0.014022453903762567\n",
      " 16M: 0.005659  8M: 0.007024  4M: 0.007680  2M: 0.009784  1M: 0.014936 merged: 0.006125\n",
      "epoch: 113 [2017-11-22 01:19:47]\n",
      "lr 0.012982269672237465\n",
      " 16M: 0.005747  8M: 0.007480  4M: 0.007547  2M: 0.009502  1M: 0.014569 merged: 0.006181\n",
      "epoch: 114 [2017-11-22 01:24:33]\n",
      "lr 0.01185113657849943\n",
      " 16M: 0.005554  8M: 0.007107  4M: 0.007574  2M: 0.009640  1M: 0.014577 merged: 0.006061\n",
      "epoch: 115 [2017-11-22 01:29:26]\n",
      "lr 0.010599978800063602\n",
      " 16M: 0.005524  8M: 0.006950  4M: 0.007436  2M: 0.009481  1M: 0.014535 merged: 0.005935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 116 [2017-11-22 01:34:12]\n",
      "lr 0.00917985092043157\n",
      " 16M: 0.005487  8M: 0.006940  4M: 0.007450  2M: 0.009431  1M: 0.014525 merged: 0.005896\n",
      "epoch: 117 [2017-11-22 01:38:50]\n",
      "lr 0.007495316889958615\n",
      " 16M: 0.005439  8M: 0.006842  4M: 0.007492  2M: 0.009540  1M: 0.014767 merged: 0.005811\n",
      "epoch: 118 [2017-11-22 01:43:29]\n",
      "lr 0.005299989400031801\n",
      " 16M: 0.005485  8M: 0.006830  4M: 0.007374  2M: 0.009378  1M: 0.014383 merged: 0.005881\n",
      "epoch: 119 [2017-11-22 01:48:18]\n",
      "lr 1e-08\n",
      " 16M: 0.005347  8M: 0.006710  4M: 0.007235  2M: 0.009123  1M: 0.014188 merged: 0.005748\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "writer = SummaryWriter(comment='-growth_rate_{}'.format(growth_rate))\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=args.base_lr, momentum=args.momentum)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, beg, end, reset_lr=None):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if reset_lr != None:\n",
    "            param_group['lr'] = reset_lr\n",
    "            continue\n",
    "        if epoch != 0: \n",
    "            # linear\n",
    "#             param_group['lr'] *= (end-epoch) / (end-beg)\n",
    "#             poly base_lr (1 - iter/max_iter) ^ (power)\n",
    "            param_group['lr'] = args.base_lr * (float(end-epoch)/(end-beg)) ** (args.power)\n",
    "            if param_group['lr'] < 1.0e-8: param_group['lr'] = 1.0e-8\n",
    "        print('lr', param_group['lr'])\n",
    "        \n",
    "# def findLargerInd(target, arr):\n",
    "#     res = list(filter(lambda x: x>target, arr))\n",
    "#     print('res',res)\n",
    "#     if len(res) == 0: return -1\n",
    "#     return res[0]\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    net.train()\n",
    "    print('epoch: {} [{}]'.format(epoch, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    \n",
    "    if epoch < args.training_thresholds[-1]: adjust_learning_rate(optimizer, epoch%ss, beg=0, end=ss-1)\n",
    "    else: adjust_learning_rate(optimizer, epoch, beg=args.training_thresholds[-1], end=args.epoches-1)\n",
    "    \n",
    "    run_losses = [0] * len(args.training_thresholds)\n",
    "    run_cnts   = [0.00001] * len(args.training_thresholds)\n",
    "    if (epoch in args.training_thresholds) == True: \n",
    "        adjust_learning_rate(optimizer, epoch, reset_lr=args.base_lr, beg=-1, end=-1)\n",
    "    writer.add_scalar('learning rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    for ind, data in enumerate(train_loader, 0):\n",
    "#         if  ind == 1 : break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        im = input_img[0,:,:,:].numpy(); im = im.transpose(1,2,0); im = im[:,:,::-1]*255\n",
    "        \n",
    "        if test_scene[0] == 'alley_1':\n",
    "            print('alley_1 yes')\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda()\n",
    "            gt_albedo = gt_albedo.cuda()\n",
    "            gt_shading = gt_shading.cuda()\n",
    "#         run_losses = [0] * len(mse_losses)\n",
    "#         run_cnts = [0.00001] * len(mse_losses)\n",
    "        if args.display_curindex % args.display_interval == 0:\n",
    "            cv2.imwrite('snapshot{}/input.png'.format(args.gpu_num), im)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_predict = net(ft_pretreained)\n",
    "        for i, threshold in enumerate(args.training_thresholds):\n",
    "#             threshold = args.training_thresholds[i]\n",
    "            if epoch >= threshold:\n",
    "#             if epoch >= 0:\n",
    "                if i == 5: s = 1\n",
    "                else: s = (2**(i+1))\n",
    "                gt = gt_albedo.cpu().data.numpy()\n",
    "                n,c,h,w = gt.shape\n",
    "                gt = gt[0,:,:,:]\n",
    "                gt = gt.transpose((1,2,0))\n",
    "                gt = cv2.resize(gt, (h//s, w//s))\n",
    "#                 gt = cv2.resize(gt, (h,w))\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    cv2.imwrite('snapshot{}/gt-{}-{}.png'.format(args.gpu_num, epoch, i), gt[:,:,::-1]*255)\n",
    "                gt = gt.transpose((2,0,1))\n",
    "                gt = gt[np.newaxis, :]\n",
    "                gt = Variable(torch.from_numpy(gt))\n",
    "                if use_gpu: gt = gt.cuda()\n",
    "                loss = mse_losses[i](ft_predict[i], gt)\n",
    "                loss_data = loss.data.cpu().numpy()\n",
    "                writer.add_scalar('{}th train iters loss'.format(i), loss_data, global_step=args.display_curindex)\n",
    "                ma_ = ft_predict[i].max().cpu().data.numpy()\n",
    "                mi_ = ft_predict[i].min().cpu().data.numpy()\n",
    "                #print('mi', mi_, 'ma', ma_)\n",
    "#                 writer.add_scalars('{}th train predict'.format(i), {'max': ma_, 'min': mi_}, global_step=args.display_curindex)\n",
    "#                 run_cnts[i] += 1\n",
    "                run_losses[i] += loss.data.cpu().numpy()[0]\n",
    "                loss.backward(retain_graph=True)\n",
    "                run_cnts[i] += 1\n",
    "#                 print('i = ', i, '; weig\\n', net.upsample01.weight[0,0,0:4,0:4].data.cpu().numpy())\n",
    "#                 print('i = ', i, '; grad\\n', net.upsample01.weight.grad[0,0,0:4,0:4].data.cpu().numpy())\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    im = ft_predict[i].cpu().data.numpy()[0].transpose((1,2,0)) * 255\n",
    "                    cv2.imwrite('snapshot{}/train-{}-{}.png'.format(args.gpu_num, epoch, i), im[:,:,::-1])\n",
    "        optimizer.step()\n",
    "        args.display_curindex += 1\n",
    "\n",
    "    \"\"\" every epoch \"\"\"\n",
    "#     loss_output = 'ind: ' + str(args.display_curindex)\n",
    "    loss_output = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,v in enumerate(run_losses):\n",
    "        if i == len(run_losses)-1: \n",
    "            loss_output += ' merged: %6f' % (run_losses[i] / run_cnts[i])\n",
    "            continue\n",
    "        loss_output += ' %2dM: %6f' % ((2**(4-i)), (run_losses[i] / run_cnts[i]))\n",
    "    print(loss_output)\n",
    "    # save at every epoch\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'args' : args,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, 'snapshot{}/snapshot-{}.pth.tar'.format(args.gpu_num, epoch))\n",
    "    \n",
    "    # test \n",
    "    test_losses_trainphase = [0] * len(args.training_thresholds)\n",
    "    test_cnts_trainphase   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "        \n",
    "        pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(ft_pretreained)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses_trainphase[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts_trainphase[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_train-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = [0] * len(args.training_thresholds)\n",
    "    test_cnts   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "#         if ind == 1: break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "            \n",
    "        pretrained.eval(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(ft_pretreained)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_test-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "    \n",
    "    writer.add_scalars('16M loss', {\n",
    "        'train 16M ': np.array([run_losses[0]/ run_cnts[0]]),\n",
    "        'test_trainphase 16M ': np.array([test_losses_trainphase[0]/ test_cnts_trainphase[0]]),\n",
    "        'test 16M ': np.array([test_losses[0]/ test_cnts[0]])\n",
    "    }, global_step=epoch)  \n",
    "    writer.add_scalars('8M loss', {\n",
    "        'train 8M ': np.array([run_losses[1]/ run_cnts[1]]),\n",
    "        'test_trainphase 8M ': np.array([test_losses_trainphase[1]/ test_cnts_trainphase[1]]),\n",
    "        'test 8M ': np.array([test_losses[1]/ test_cnts[1]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('4M loss', {\n",
    "        'train 4M ': np.array([run_losses[2]/ run_cnts[2]]),\n",
    "        'test_trainphase 4M ': np.array([test_losses_trainphase[2]/ test_cnts_trainphase[2]]),\n",
    "        'test 4M ': np.array([test_losses[2]/ test_cnts[2]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('2M loss', {\n",
    "        'train 2M ': np.array([run_losses[3]/ run_cnts[3]]),\n",
    "        'test_trainphase 2M ': np.array([test_losses_trainphase[3]/ test_cnts_trainphase[3]]),\n",
    "        'test 2M ': np.array([test_losses[3]/ test_cnts[3]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('1M loss', {\n",
    "        'train 1M ': np.array([run_losses[4]/ run_cnts[4]]),\n",
    "        'test_trainphase 1M ': np.array([test_losses_trainphase[4]/ test_cnts_trainphase[4]]),\n",
    "        'test 1M ': np.array([test_losses[4]/ test_cnts[4]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('merged loss', {\n",
    "        'train merged ': np.array([run_losses[5]/ run_cnts[5]]),\n",
    "        'test_trainphase merged ': np.array([test_losses_trainphase[5]/ test_cnts_trainphase[5]]),\n",
    "        'test merged ': np.array([test_losses[5]/ test_cnts[5]])\n",
    "    }, global_step=epoch) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"10240,10240\"), format='svg')\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.zeros(1,3,256,256))\n",
    "y = net(x.cuda())\n",
    "g = make_dot(y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g.render('net') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
