{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, platform, datetime, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch import functional as F\n",
    "# import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv as denseinv\n",
    "from scipy import sparse\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import inv as spinv\n",
    "import scipy.misc\n",
    "\n",
    "from myimagefolder import MyImageFolder\n",
    "from mymodel import GradientNet\n",
    "from myargs import Args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('debian', 'jessie/sid', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Args()\n",
    "args.test_scene = 'alley_1'\n",
    "args.arch = \"densenet121\"\n",
    "args.epoches = 500\n",
    "args.epoches_unary_threshold = 0\n",
    "args.image_h = 256\n",
    "args.image_w = 256\n",
    "args.img_extentions = [\"png\"]\n",
    "args.training_thresholds = [250,200,150,50,0,300]\n",
    "args.base_lr = 1\n",
    "args.lr = args.base_lr\n",
    "args.snapshot_interval = 5000\n",
    "args.debug = True\n",
    "args.gpu_num = 3\n",
    "# growth_rate = (4*(2**(args.gpu_num)))\n",
    "# transition_scale=(2*(2**(args.gpu_num+1)))\n",
    "transition_scale=2\n",
    "pretrained_scale=4\n",
    "growth_rate = 32\n",
    "args.display_interval = 50\n",
    "args.display_curindex = 0\n",
    "\n",
    "system_ = platform.system()\n",
    "system_dist, system_version, _ = platform.dist()\n",
    "if system_ == \"Darwin\": \n",
    "    args.train_dir = '/Volumes/Transcend/dataset/sintel2'\n",
    "    args.pretrained = False\n",
    "elif platform.dist() ==  ('debian', 'jessie/sid', ''):\n",
    "    args.train_dir = '/home/lwp/workspace/sintel2'\n",
    "    args.pretrained = True\n",
    "elif platform.dist() == ('debian', 'stretch/sid', ''):\n",
    "    args.train_dir = '/home/cad/lwp/workspace/dataset/sintel2'\n",
    "    args.pretrained = True\n",
    "\n",
    "if platform.system() == 'Linux': use_gpu = True\n",
    "else: use_gpu = False\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.gpu_num)\n",
    "    \n",
    "\n",
    "print(platform.dist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# My DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyImageFolder(args.train_dir, 'train',\n",
    "                       transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ), random_crop=True, \n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "test_dataset = MyImageFolder(args.train_dir, 'test', \n",
    "                       transforms.Compose(\n",
    "        [transforms.CenterCrop((args.image_h, args.image_w)),\n",
    "         transforms.ToTensor()]\n",
    "    ), random_crop=False,\n",
    "    img_extentions=args.img_extentions, test_scene=args.test_scene, image_h=args.image_h, image_w=args.image_w)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,1,True,num_workers=1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,1,True,num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "[Defination](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)\n",
    "* DenseNet-121: num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16)\n",
    "    * First Convolution: 32M -> 16M -> 8M\n",
    "    * every transition: 8M -> 4M -> 2M (downsample 1/2, except the last block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "densenet = models.__dict__[args.arch](pretrained=args.pretrained)\n",
    "\n",
    "for param in densenet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if use_gpu: densenet.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 6\n",
    "\n",
    "args.display_curindex = 0\n",
    "args.base_lr = 0.05\n",
    "args.display_interval = 20\n",
    "args.momentum = 0.9\n",
    "args.epoches = 120\n",
    "args.training_thresholds = [ss*4,ss*3,ss*2,ss*1,ss*0,ss*5]\n",
    "args.power = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# pretrained = PreTrainedModel(densenet)\n",
    "# if use_gpu: \n",
    "#     pretrained.cuda()\n",
    "\n",
    "\n",
    "net = GradientNet(densenet=densenet, growth_rate=growth_rate, transition_scale=transition_scale, pretrained_scale=pretrained_scale)\n",
    "if use_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "if use_gpu: \n",
    "    mse_losses = [nn.MSELoss().cuda()] * 6\n",
    "    test_losses = [nn.MSELoss().cuda()] * 6\n",
    "else:\n",
    "    mse_losses = [nn.MSELoss()] * 6\n",
    "    test_losses = [nn.MSELoss()] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 [2017-11-23 15:43:54]\n",
      "lr 0.05\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.057529 merged: 0.000000\n",
      "epoch: 1 [2017-11-23 15:46:02]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.052053 merged: 0.000000\n",
      "epoch: 2 [2017-11-23 15:48:10]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.047733 merged: 0.000000\n",
      "epoch: 3 [2017-11-23 15:50:17]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.044509 merged: 0.000000\n",
      "epoch: 4 [2017-11-23 15:52:24]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.038485 merged: 0.000000\n",
      "epoch: 5 [2017-11-23 15:54:30]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.000000  1M: 0.033386 merged: 0.000000\n",
      "epoch: 6 [2017-11-23 15:56:38]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.052660  1M: 0.041762 merged: 0.000000\n",
      "epoch: 7 [2017-11-23 15:59:08]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.050439  1M: 0.037072 merged: 0.000000\n",
      "epoch: 8 [2017-11-23 16:02:08]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.046301  1M: 0.032468 merged: 0.000000\n",
      "epoch: 9 [2017-11-23 16:04:56]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.041922  1M: 0.027672 merged: 0.000000\n",
      "epoch: 10 [2017-11-23 16:07:31]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.035036  1M: 0.025137 merged: 0.000000\n",
      "epoch: 11 [2017-11-23 16:10:04]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.000000  2M: 0.029670  1M: 0.024691 merged: 0.000000\n",
      "epoch: 12 [2017-11-23 16:12:47]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.049411  2M: 0.042442  1M: 0.030162 merged: 0.000000\n",
      "epoch: 13 [2017-11-23 16:15:49]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.040240  2M: 0.035206  1M: 0.027575 merged: 0.000000\n",
      "epoch: 14 [2017-11-23 16:18:32]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.033164  2M: 0.029420  1M: 0.025735 merged: 0.000000\n",
      "epoch: 15 [2017-11-23 16:21:19]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.026806  2M: 0.027602  1M: 0.024130 merged: 0.000000\n",
      "epoch: 16 [2017-11-23 16:24:45]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.022783  2M: 0.023555  1M: 0.022533 merged: 0.000000\n",
      "epoch: 17 [2017-11-23 16:28:14]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.000000  4M: 0.020267  2M: 0.023194  1M: 0.021651 merged: 0.000000\n",
      "epoch: 18 [2017-11-23 16:31:15]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.047238  4M: 0.026660  2M: 0.027711  1M: 0.025505 merged: 0.000000\n",
      "epoch: 19 [2017-11-23 16:34:21]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.000000  8M: 0.036079  4M: 0.023929  2M: 0.025823  1M: 0.026577 merged: 0.000000\n",
      "epoch: 20 [2017-11-23 16:37:22]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.000000  8M: 0.024981  4M: 0.020728  2M: 0.022587  1M: 0.023673 merged: 0.000000\n",
      "epoch: 21 [2017-11-23 16:40:24]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.000000  8M: 0.021695  4M: 0.017605  2M: 0.019461  1M: 0.020909 merged: 0.000000\n",
      "epoch: 22 [2017-11-23 16:43:37]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.000000  8M: 0.018775  4M: 0.015763  2M: 0.018946  1M: 0.020876 merged: 0.000000\n",
      "epoch: 23 [2017-11-23 16:46:34]\n",
      "lr 1e-08\n",
      " 16M: 0.000000  8M: 0.018228  4M: 0.015985  2M: 0.018831  1M: 0.019734 merged: 0.000000\n",
      "epoch: 24 [2017-11-23 16:49:33]\n",
      "lr 1e-08\n",
      " 16M: 0.042900  8M: 0.022284  4M: 0.019680  2M: 0.022466  1M: 0.022870 merged: 0.000000\n",
      "epoch: 25 [2017-11-23 16:52:44]\n",
      "lr 0.044721359549995794\n",
      " 16M: 0.031403  8M: 0.019927  4M: 0.017176  2M: 0.023608  1M: 0.023303 merged: 0.000000\n",
      "epoch: 26 [2017-11-23 16:55:57]\n",
      "lr 0.038729833462074176\n",
      " 16M: 0.024404  8M: 0.017497  4M: 0.015574  2M: 0.020273  1M: 0.021179 merged: 0.000000\n",
      "epoch: 27 [2017-11-23 16:59:08]\n",
      "lr 0.0316227766016838\n",
      " 16M: 0.020854  8M: 0.016100  4M: 0.014514  2M: 0.019134  1M: 0.020583 merged: 0.000000\n",
      "epoch: 28 [2017-11-23 17:02:21]\n",
      "lr 0.022360679774997897\n",
      " 16M: 0.017761  8M: 0.014226  4M: 0.013097  2M: 0.016901  1M: 0.018803 merged: 0.000000\n",
      "epoch: 29 [2017-11-23 17:05:35]\n",
      "lr 1e-08\n",
      " 16M: 0.018023  8M: 0.014519  4M: 0.012807  2M: 0.016458  1M: 0.019170 merged: 0.000000\n",
      "epoch: 30 [2017-11-23 17:08:50]\n",
      "lr 0.05\n",
      " 16M: 0.022868  8M: 0.022000  4M: 0.017339  2M: 0.020371  1M: 0.024010 merged: 0.035861\n",
      "epoch: 31 [2017-11-23 17:13:47]\n",
      "lr 0.04971830761761256\n",
      " 16M: 0.022604  8M: 0.022733  4M: 0.018687  2M: 0.021476  1M: 0.022745 merged: 0.025234\n",
      "epoch: 32 [2017-11-23 17:18:56]\n",
      "lr 0.04943501011144937\n",
      " 16M: 0.021031  8M: 0.019526  4M: 0.017373  2M: 0.020456  1M: 0.022177 merged: 0.021963\n",
      "epoch: 33 [2017-11-23 17:24:06]\n",
      "lr 0.04915007972606608\n",
      " 16M: 0.019810  8M: 0.018614  4M: 0.016391  2M: 0.020616  1M: 0.022471 merged: 0.020040\n",
      "epoch: 34 [2017-11-23 17:29:16]\n",
      "lr 0.04886348789677424\n",
      " 16M: 0.017681  8M: 0.016625  4M: 0.014983  2M: 0.018017  1M: 0.021331 merged: 0.017452\n",
      "epoch: 35 [2017-11-23 17:34:27]\n",
      "lr 0.04857520521621862\n",
      " 16M: 0.016507  8M: 0.016445  4M: 0.015553  2M: 0.017710  1M: 0.020075 merged: 0.016852\n",
      "epoch: 36 [2017-11-23 17:39:40]\n",
      "lr 0.04828520139915856\n",
      " 16M: 0.016738  8M: 0.016614  4M: 0.014675  2M: 0.017438  1M: 0.019103 merged: 0.016523\n",
      "epoch: 37 [2017-11-23 17:44:51]\n",
      "lr 0.047993445245333805\n",
      " 16M: 0.015796  8M: 0.016929  4M: 0.014143  2M: 0.016568  1M: 0.019933 merged: 0.015665\n",
      "epoch: 38 [2017-11-23 17:50:03]\n",
      "lr 0.0476999046002862\n",
      " 16M: 0.015200  8M: 0.016161  4M: 0.012921  2M: 0.015584  1M: 0.018465 merged: 0.014890\n",
      "epoch: 39 [2017-11-23 17:55:11]\n",
      "lr 0.04740454631399772\n",
      " 16M: 0.015028  8M: 0.015367  4M: 0.013945  2M: 0.015838  1M: 0.018709 merged: 0.015163\n",
      "epoch: 40 [2017-11-23 18:00:23]\n",
      "lr 0.04710733619719444\n",
      " 16M: 0.014390  8M: 0.014877  4M: 0.013382  2M: 0.015524  1M: 0.018772 merged: 0.013994\n",
      "epoch: 41 [2017-11-23 18:05:34]\n",
      "lr 0.04680823897515326\n",
      " 16M: 0.013029  8M: 0.013763  4M: 0.012212  2M: 0.014315  1M: 0.017482 merged: 0.013051\n",
      "epoch: 42 [2017-11-23 18:10:44]\n",
      "lr 0.04650721823883479\n",
      " 16M: 0.013787  8M: 0.014112  4M: 0.012355  2M: 0.014678  1M: 0.018127 merged: 0.013009\n",
      "epoch: 43 [2017-11-23 18:15:54]\n",
      "lr 0.046204236393150765\n",
      " 16M: 0.013288  8M: 0.013789  4M: 0.012527  2M: 0.015176  1M: 0.018391 merged: 0.012761\n",
      "epoch: 44 [2017-11-23 18:21:05]\n",
      "lr 0.045899254602157845\n",
      " 16M: 0.012557  8M: 0.012827  4M: 0.011943  2M: 0.015878  1M: 0.017626 merged: 0.012073\n",
      "epoch: 45 [2017-11-23 18:26:16]\n",
      "lr 0.04559223273095164\n",
      " 16M: 0.012608  8M: 0.012716  4M: 0.011517  2M: 0.014587  1M: 0.017417 merged: 0.012257\n",
      "epoch: 46 [2017-11-23 18:31:28]\n",
      "lr 0.045283129284014914\n",
      " 16M: 0.013142  8M: 0.012630  4M: 0.012355  2M: 0.014734  1M: 0.017555 merged: 0.012003\n",
      "epoch: 47 [2017-11-23 18:36:36]\n",
      "lr 0.04497190133975169\n",
      " 16M: 0.011955  8M: 0.012775  4M: 0.011551  2M: 0.013994  1M: 0.017096 merged: 0.011469\n",
      "epoch: 48 [2017-11-23 18:41:47]\n",
      "lr 0.04465850448091506\n",
      " 16M: 0.011482  8M: 0.011843  4M: 0.011256  2M: 0.013310  1M: 0.016649 merged: 0.011009\n",
      "epoch: 49 [2017-11-23 18:46:58]\n",
      "lr 0.044342892720609255\n",
      " 16M: 0.011265  8M: 0.011793  4M: 0.011177  2M: 0.013884  1M: 0.016859 merged: 0.010755\n",
      "epoch: 50 [2017-11-23 18:52:08]\n",
      "lr 0.044025018423517\n",
      " 16M: 0.011383  8M: 0.011811  4M: 0.010549  2M: 0.014723  1M: 0.016572 merged: 0.010640\n",
      "epoch: 51 [2017-11-23 18:57:17]\n",
      "lr 0.04370483222197017\n",
      " 16M: 0.011692  8M: 0.012029  4M: 0.010874  2M: 0.014425  1M: 0.016516 merged: 0.010671\n",
      "epoch: 52 [2017-11-23 19:02:29]\n",
      "lr 0.043382282926444894\n",
      " 16M: 0.011165  8M: 0.011122  4M: 0.010002  2M: 0.013397  1M: 0.015719 merged: 0.010046\n",
      "epoch: 53 [2017-11-23 19:07:40]\n",
      "lr 0.04305731743002185\n",
      " 16M: 0.011283  8M: 0.011163  4M: 0.010006  2M: 0.013578  1M: 0.016116 merged: 0.010307\n",
      "epoch: 54 [2017-11-23 19:13:06]\n",
      "lr 0.04272988060630656\n",
      " 16M: 0.010535  8M: 0.010700  4M: 0.009819  2M: 0.012870  1M: 0.016240 merged: 0.009513\n",
      "epoch: 55 [2017-11-23 19:18:37]\n",
      "lr 0.04239991520025441\n",
      " 16M: 0.010293  8M: 0.010418  4M: 0.009553  2M: 0.012348  1M: 0.015533 merged: 0.009165\n",
      "epoch: 56 [2017-11-23 19:24:09]\n",
      "lr 0.0420673617112877\n",
      " 16M: 0.010109  8M: 0.010140  4M: 0.009116  2M: 0.012568  1M: 0.015862 merged: 0.009150\n",
      "epoch: 57 [2017-11-23 19:29:39]\n",
      "lr 0.041732158268029534\n",
      " 16M: 0.009993  8M: 0.010111  4M: 0.009303  2M: 0.012421  1M: 0.015503 merged: 0.008965\n",
      "epoch: 58 [2017-11-23 19:35:12]\n",
      "lr 0.041394240493907074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16M: 0.009499  8M: 0.009896  4M: 0.009043  2M: 0.012145  1M: 0.015190 merged: 0.008711\n",
      "epoch: 59 [2017-11-23 19:40:43]\n",
      "lr 0.041053541362798006\n",
      " 16M: 0.009537  8M: 0.009896  4M: 0.009136  2M: 0.012512  1M: 0.015569 merged: 0.008738\n",
      "epoch: 60 [2017-11-23 19:46:14]\n",
      "lr 0.04070999104380296\n",
      " 16M: 0.009603  8M: 0.009982  4M: 0.009000  2M: 0.012128  1M: 0.015766 merged: 0.008674\n",
      "epoch: 61 [2017-11-23 19:51:43]\n",
      "lr 0.04036351673412598\n",
      " 16M: 0.009977  8M: 0.010122  4M: 0.009281  2M: 0.012157  1M: 0.015210 merged: 0.008935\n",
      "epoch: 62 [2017-11-23 19:57:14]\n",
      "lr 0.04001404247893005\n",
      " 16M: 0.009393  8M: 0.009777  4M: 0.008825  2M: 0.011753  1M: 0.015355 merged: 0.008367\n",
      "epoch: 63 [2017-11-23 20:02:44]\n",
      "lr 0.03966148897690515\n",
      " 16M: 0.009161  8M: 0.009564  4M: 0.008530  2M: 0.011601  1M: 0.015045 merged: 0.008183\n",
      "epoch: 64 [2017-11-23 20:08:13]\n",
      "lr 0.03930577337013889\n",
      " 16M: 0.008812  8M: 0.009290  4M: 0.008442  2M: 0.011383  1M: 0.014810 merged: 0.007977\n",
      "epoch: 65 [2017-11-23 20:13:44]\n",
      "lr 0.038946809016712394\n",
      " 16M: 0.008878  8M: 0.009216  4M: 0.008462  2M: 0.011321  1M: 0.014739 merged: 0.007848\n",
      "epoch: 66 [2017-11-23 20:19:13]\n",
      "lr 0.03858450524425343\n",
      " 16M: 0.008581  8M: 0.009077  4M: 0.008197  2M: 0.011027  1M: 0.014407 merged: 0.007745\n",
      "epoch: 67 [2017-11-23 20:24:44]\n",
      "lr 0.03821876708246056\n",
      " 16M: 0.008340  8M: 0.008750  4M: 0.008012  2M: 0.011278  1M: 0.014469 merged: 0.007490\n",
      "epoch: 68 [2017-11-23 20:30:14]\n",
      "lr 0.03784949497236286\n",
      " 16M: 0.008798  8M: 0.009118  4M: 0.008236  2M: 0.011350  1M: 0.014636 merged: 0.007879\n",
      "epoch: 69 [2017-11-23 20:35:45]\n",
      "lr 0.03747658444979307\n",
      " 16M: 0.008355  8M: 0.008611  4M: 0.007987  2M: 0.010985  1M: 0.014474 merged: 0.007421\n",
      "epoch: 70 [2017-11-23 20:41:14]\n",
      "lr 0.0370999258002226\n",
      " 16M: 0.009986  8M: 0.009059  4M: 0.008087  2M: 0.010877  1M: 0.014205 merged: 0.008135\n",
      "epoch: 71 [2017-11-23 20:46:44]\n",
      "lr 0.03671940368172628\n",
      " 16M: 0.009011  8M: 0.010389  4M: 0.008711  2M: 0.011151  1M: 0.014612 merged: 0.008282\n",
      "epoch: 72 [2017-11-23 20:52:12]\n",
      "lr 0.03633489671240478\n",
      " 16M: 0.008661  8M: 0.009530  4M: 0.008196  2M: 0.010868  1M: 0.014212 merged: 0.007759\n",
      "epoch: 73 [2017-11-23 20:57:40]\n",
      "lr 0.03594627701808178\n",
      " 16M: 0.008116  8M: 0.008763  4M: 0.007732  2M: 0.010563  1M: 0.013721 merged: 0.007283\n",
      "epoch: 74 [2017-11-23 21:03:09]\n",
      "lr 0.035553409735498295\n",
      " 16M: 0.007874  8M: 0.008501  4M: 0.007748  2M: 0.010780  1M: 0.013769 merged: 0.007070\n",
      "epoch: 75 [2017-11-23 21:08:38]\n",
      "lr 0.03515615246553262\n",
      " 16M: 0.008911  8M: 0.009414  4M: 0.008152  2M: 0.011163  1M: 0.014159 merged: 0.008066\n",
      "epoch: 76 [2017-11-23 21:14:07]\n",
      "lr 0.03475435467016077\n",
      " 16M: 0.008237  8M: 0.008750  4M: 0.007804  2M: 0.011255  1M: 0.014265 merged: 0.007364\n",
      "epoch: 77 [2017-11-23 21:19:34]\n",
      "lr 0.034347857005916346\n",
      " 16M: 0.007930  8M: 0.008453  4M: 0.007734  2M: 0.010667  1M: 0.013982 merged: 0.007036\n",
      "epoch: 78 [2017-11-23 21:24:59]\n",
      "lr 0.0339364905854808\n",
      " 16M: 0.007845  8M: 0.008258  4M: 0.007609  2M: 0.010933  1M: 0.014286 merged: 0.006898\n",
      "epoch: 79 [2017-11-23 21:30:26]\n",
      "lr 0.03352007615769955\n",
      " 16M: 0.007917  8M: 0.008248  4M: 0.007614  2M: 0.010551  1M: 0.014096 merged: 0.007014\n",
      "epoch: 80 [2017-11-23 21:35:52]\n",
      "lr 0.03309842319473132\n",
      " 16M: 0.007855  8M: 0.008164  4M: 0.007562  2M: 0.010581  1M: 0.013688 merged: 0.006881\n",
      "epoch: 81 [2017-11-23 21:41:18]\n",
      "lr 0.03267132887314317\n",
      " 16M: 0.008005  8M: 0.008115  4M: 0.008120  2M: 0.010602  1M: 0.014096 merged: 0.007187\n",
      "epoch: 82 [2017-11-23 21:46:46]\n",
      "lr 0.03223857693349118\n",
      " 16M: 0.007870  8M: 0.008094  4M: 0.007843  2M: 0.010590  1M: 0.013892 merged: 0.006835\n",
      "epoch: 83 [2017-11-23 21:52:12]\n",
      "lr 0.0317999364001908\n",
      " 16M: 0.007525  8M: 0.007908  4M: 0.007426  2M: 0.011133  1M: 0.014104 merged: 0.006713\n",
      "epoch: 84 [2017-11-23 21:57:37]\n",
      "lr 0.031355160140170396\n",
      " 16M: 0.007569  8M: 0.007786  4M: 0.007255  2M: 0.010367  1M: 0.013363 merged: 0.006599\n",
      "epoch: 85 [2017-11-23 22:03:03]\n",
      "lr 0.03090398323477543\n",
      " 16M: 0.007491  8M: 0.007936  4M: 0.007239  2M: 0.012398  1M: 0.013354 merged: 0.006629\n",
      "epoch: 86 [2017-11-23 22:08:39]\n",
      "lr 0.030446121134470178\n",
      " 16M: 0.007505  8M: 0.007898  4M: 0.007264  2M: 0.011187  1M: 0.013620 merged: 0.006584\n",
      "epoch: 87 [2017-11-23 22:13:58]\n",
      "lr 0.02998126755983446\n",
      " 16M: 0.007424  8M: 0.007758  4M: 0.007256  2M: 0.010742  1M: 0.013346 merged: 0.006449\n",
      "epoch: 88 [2017-11-23 22:19:46]\n",
      "lr 0.029509092104873926\n",
      " 16M: 0.007204  8M: 0.007525  4M: 0.007055  2M: 0.010578  1M: 0.013270 merged: 0.006289\n",
      "epoch: 89 [2017-11-23 22:25:15]\n",
      "lr 0.029029237489356888\n",
      " 16M: 0.007809  8M: 0.007911  4M: 0.007236  2M: 0.010552  1M: 0.013333 merged: 0.006710\n",
      "epoch: 90 [2017-11-23 22:30:43]\n",
      "lr 0.028541316395237167\n",
      " 16M: 0.007328  8M: 0.007439  4M: 0.006980  2M: 0.010294  1M: 0.013264 merged: 0.006326\n",
      "epoch: 91 [2017-11-23 22:36:12]\n",
      "lr 0.028044907807525134\n",
      " 16M: 0.007217  8M: 0.007531  4M: 0.007042  2M: 0.010861  1M: 0.013415 merged: 0.006340\n",
      "epoch: 92 [2017-11-23 22:42:08]\n",
      "lr 0.027539552761294706\n",
      " 16M: 0.007116  8M: 0.007329  4M: 0.006855  2M: 0.010319  1M: 0.013064 merged: 0.006180\n",
      "epoch: 93 [2017-11-23 22:47:36]\n",
      "lr 0.027024749372597065\n",
      " 16M: 0.006946  8M: 0.007394  4M: 0.006868  2M: 0.010244  1M: 0.012990 merged: 0.006099\n",
      "epoch: 94 [2017-11-23 22:53:05]\n",
      "lr 0.026499947000159004\n",
      " 16M: 0.007324  8M: 0.007675  4M: 0.007085  2M: 0.010337  1M: 0.013587 merged: 0.006440\n",
      "epoch: 95 [2017-11-23 22:58:35]\n",
      "lr 0.02596453934447493\n",
      " 16M: 0.006931  8M: 0.007394  4M: 0.006797  2M: 0.009870  1M: 0.012910 merged: 0.006179\n",
      "epoch: 96 [2017-11-23 23:04:04]\n",
      "lr 0.025417856237895775\n",
      " 16M: 0.006912  8M: 0.007295  4M: 0.006809  2M: 0.009900  1M: 0.013311 merged: 0.006007\n",
      "epoch: 97 [2017-11-23 23:09:31]\n",
      "lr 0.02485915380880628\n",
      " 16M: 0.006924  8M: 0.007132  4M: 0.006716  2M: 0.009860  1M: 0.012932 merged: 0.005838\n",
      "epoch: 98 [2017-11-23 23:14:58]\n",
      "lr 0.02428760260810931\n",
      " 16M: 0.006658  8M: 0.006927  4M: 0.006499  2M: 0.009529  1M: 0.012825 merged: 0.005800\n",
      "epoch: 99 [2017-11-23 23:20:27]\n",
      "lr 0.02370227315699886\n",
      " 16M: 0.006812  8M: 0.007136  4M: 0.006739  2M: 0.009923  1M: 0.013113 merged: 0.005914\n",
      "epoch: 100 [2017-11-23 23:25:56]\n",
      "lr 0.023102118196575382\n",
      " 16M: 0.006817  8M: 0.007076  4M: 0.006812  2M: 0.009929  1M: 0.013181 merged: 0.005981\n",
      "epoch: 101 [2017-11-23 23:31:24]\n",
      "lr 0.022485950669875843\n",
      " 16M: 0.006725  8M: 0.007144  4M: 0.006630  2M: 0.009669  1M: 0.012667 merged: 0.005832\n",
      "epoch: 102 [2017-11-23 23:36:52]\n",
      "lr 0.021852416110985085\n",
      " 16M: 0.006482  8M: 0.006971  4M: 0.006546  2M: 0.009565  1M: 0.012909 merged: 0.005610\n",
      "epoch: 103 [2017-11-23 23:42:23]\n",
      "lr 0.021199957600127203\n",
      " 16M: 0.006648  8M: 0.007026  4M: 0.006626  2M: 0.009598  1M: 0.012959 merged: 0.005697\n",
      "epoch: 104 [2017-11-23 23:47:52]\n",
      "lr 0.020526770681399003\n",
      " 16M: 0.006526  8M: 0.006929  4M: 0.006607  2M: 0.009517  1M: 0.012723 merged: 0.005658\n",
      "epoch: 105 [2017-11-23 23:53:25]\n",
      "lr 0.019830744488452574\n",
      " 16M: 0.006467  8M: 0.006852  4M: 0.006539  2M: 0.009604  1M: 0.012652 merged: 0.005638\n",
      "epoch: 106 [2017-11-23 23:58:45]\n",
      "lr 0.01910938354123028\n",
      " 16M: 0.006632  8M: 0.006940  4M: 0.006541  2M: 0.009548  1M: 0.012591 merged: 0.005638\n",
      "epoch: 107 [2017-11-24 00:04:20]\n",
      "lr 0.01835970184086314\n",
      " 16M: 0.006379  8M: 0.006690  4M: 0.006362  2M: 0.009203  1M: 0.012056 merged: 0.005523\n",
      "epoch: 108 [2017-11-24 00:09:53]\n",
      "lr 0.01757807623276631\n",
      " 16M: 0.006335  8M: 0.006705  4M: 0.006440  2M: 0.009411  1M: 0.012206 merged: 0.005532\n",
      "epoch: 109 [2017-11-24 00:15:27]\n",
      "lr 0.016760038078849775\n",
      " 16M: 0.006317  8M: 0.006546  4M: 0.006282  2M: 0.009211  1M: 0.012166 merged: 0.005460\n",
      "epoch: 110 [2017-11-24 00:20:55]\n",
      "lr 0.0158999682000954\n",
      " 16M: 0.006179  8M: 0.006510  4M: 0.006246  2M: 0.009012  1M: 0.012147 merged: 0.005397\n",
      "epoch: 111 [2017-11-24 00:26:15]\n",
      "lr 0.01499063377991723\n",
      " 16M: 0.006241  8M: 0.006531  4M: 0.006288  2M: 0.009170  1M: 0.012308 merged: 0.005377\n",
      "epoch: 112 [2017-11-24 00:31:37]\n",
      "lr 0.014022453903762567\n",
      " 16M: 0.006141  8M: 0.006460  4M: 0.006098  2M: 0.008873  1M: 0.011901 merged: 0.005283\n",
      "epoch: 113 [2017-11-24 00:36:56]\n",
      "lr 0.012982269672237465\n",
      " 16M: 0.006144  8M: 0.006443  4M: 0.006141  2M: 0.008988  1M: 0.012026 merged: 0.005310\n",
      "epoch: 114 [2017-11-24 00:42:07]\n",
      "lr 0.01185113657849943\n",
      " 16M: 0.006250  8M: 0.006622  4M: 0.006385  2M: 0.009280  1M: 0.012732 merged: 0.005416\n",
      "epoch: 115 [2017-11-24 00:47:20]\n",
      "lr 0.010599978800063602\n",
      " 16M: 0.006018  8M: 0.006301  4M: 0.006067  2M: 0.008857  1M: 0.012034 merged: 0.005194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 116 [2017-11-24 00:52:34]\n",
      "lr 0.00917985092043157\n",
      " 16M: 0.006267  8M: 0.006608  4M: 0.006253  2M: 0.009163  1M: 0.012380 merged: 0.005394\n",
      "epoch: 117 [2017-11-24 00:57:47]\n",
      "lr 0.007495316889958615\n",
      " 16M: 0.005957  8M: 0.006325  4M: 0.006119  2M: 0.009010  1M: 0.012262 merged: 0.005183\n",
      "epoch: 118 [2017-11-24 01:03:06]\n",
      "lr 0.005299989400031801\n",
      " 16M: 0.005933  8M: 0.006285  4M: 0.006095  2M: 0.008876  1M: 0.012189 merged: 0.005155\n",
      "epoch: 119 [2017-11-24 01:08:24]\n",
      "lr 1e-08\n",
      " 16M: 0.005949  8M: 0.006343  4M: 0.006142  2M: 0.008976  1M: 0.012232 merged: 0.005150\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "writer = SummaryWriter(comment='-pretrained_scale_{}'.format(pretrained_scale))\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=args.base_lr, momentum=args.momentum)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, beg, end, reset_lr=None):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if reset_lr != None:\n",
    "            param_group['lr'] = reset_lr\n",
    "            continue\n",
    "        if epoch != 0: \n",
    "            # linear\n",
    "#             param_group['lr'] *= (end-epoch) / (end-beg)\n",
    "#             poly base_lr (1 - iter/max_iter) ^ (power)\n",
    "            param_group['lr'] = args.base_lr * (float(end-epoch)/(end-beg)) ** (args.power)\n",
    "            if param_group['lr'] < 1.0e-8: param_group['lr'] = 1.0e-8\n",
    "        print('lr', param_group['lr'])\n",
    "        \n",
    "# def findLargerInd(target, arr):\n",
    "#     res = list(filter(lambda x: x>target, arr))\n",
    "#     print('res',res)\n",
    "#     if len(res) == 0: return -1\n",
    "#     return res[0]\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    net.train()\n",
    "    print('epoch: {} [{}]'.format(epoch, datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    \n",
    "    if epoch < args.training_thresholds[-1]: adjust_learning_rate(optimizer, epoch%ss, beg=0, end=ss-1)\n",
    "    else: adjust_learning_rate(optimizer, epoch, beg=args.training_thresholds[-1], end=args.epoches-1)\n",
    "    \n",
    "    run_losses = [0] * len(args.training_thresholds)\n",
    "    run_cnts   = [0.00001] * len(args.training_thresholds)\n",
    "    if (epoch in args.training_thresholds) == True: \n",
    "        adjust_learning_rate(optimizer, epoch, reset_lr=args.base_lr, beg=-1, end=-1)\n",
    "    writer.add_scalar('learning rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    for ind, data in enumerate(train_loader, 0):\n",
    "#         if  ind == 1 : break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        im = input_img[0,:,:,:].numpy(); im = im.transpose(1,2,0); im = im[:,:,::-1]*255\n",
    "        \n",
    "        if test_scene[0] == 'alley_1':\n",
    "            print('alley_1 yes')\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda()\n",
    "            gt_albedo = gt_albedo.cuda()\n",
    "            gt_shading = gt_shading.cuda()\n",
    "#         run_losses = [0] * len(mse_losses)\n",
    "#         run_cnts = [0.00001] * len(mse_losses)\n",
    "        if args.display_curindex % args.display_interval == 0:\n",
    "            cv2.imwrite('snapshot{}/input.png'.format(args.gpu_num), im)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "#         pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_predict = net(input_img)\n",
    "        for i, threshold in enumerate(args.training_thresholds):\n",
    "#             threshold = args.training_thresholds[i]\n",
    "            if epoch >= threshold:\n",
    "#             if epoch >= 0:\n",
    "                if i == 5: s = 1\n",
    "                else: s = (2**(i+1))\n",
    "                gt = gt_albedo.cpu().data.numpy()\n",
    "                n,c,h,w = gt.shape\n",
    "                gt = gt[0,:,:,:]\n",
    "                gt = gt.transpose((1,2,0))\n",
    "                gt = cv2.resize(gt, (h//s, w//s))\n",
    "#                 gt = cv2.resize(gt, (h,w))\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    cv2.imwrite('snapshot{}/gt-{}-{}.png'.format(args.gpu_num, epoch, i), gt[:,:,::-1]*255)\n",
    "                gt = gt.transpose((2,0,1))\n",
    "                gt = gt[np.newaxis, :]\n",
    "                gt = Variable(torch.from_numpy(gt))\n",
    "                if use_gpu: gt = gt.cuda()\n",
    "                loss = mse_losses[i](ft_predict[i], gt)\n",
    "                loss_data = loss.data.cpu().numpy()\n",
    "                writer.add_scalar('{}th train iters loss'.format(i), loss_data, global_step=args.display_curindex)\n",
    "                ma_ = ft_predict[i].max().cpu().data.numpy()\n",
    "                mi_ = ft_predict[i].min().cpu().data.numpy()\n",
    "                #print('mi', mi_, 'ma', ma_)\n",
    "#                 writer.add_scalars('{}th train predict'.format(i), {'max': ma_, 'min': mi_}, global_step=args.display_curindex)\n",
    "#                 run_cnts[i] += 1\n",
    "                run_losses[i] += loss.data.cpu().numpy()[0]\n",
    "                loss.backward(retain_graph=True)\n",
    "                run_cnts[i] += 1\n",
    "#                 print('i = ', i, '; weig\\n', net.upsample01.weight[0,0,0:4,0:4].data.cpu().numpy())\n",
    "#                 print('i = ', i, '; grad\\n', net.upsample01.weight.grad[0,0,0:4,0:4].data.cpu().numpy())\n",
    "                if args.display_curindex % args.display_interval == 0:\n",
    "                    im = ft_predict[i].cpu().data.numpy()[0].transpose((1,2,0)) * 255\n",
    "                    cv2.imwrite('snapshot{}/train-{}-{}.png'.format(args.gpu_num, epoch, i), im[:,:,::-1])\n",
    "        optimizer.step()\n",
    "        args.display_curindex += 1\n",
    "\n",
    "    \"\"\" every epoch \"\"\"\n",
    "#     loss_output = 'ind: ' + str(args.display_curindex)\n",
    "    loss_output = ''\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,v in enumerate(run_losses):\n",
    "        if i == len(run_losses)-1: \n",
    "            loss_output += ' merged: %6f' % (run_losses[i] / run_cnts[i])\n",
    "            continue\n",
    "        loss_output += ' %2dM: %6f' % ((2**(4-i)), (run_losses[i] / run_cnts[i]))\n",
    "    print(loss_output)\n",
    "    # save at every epoch\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'args' : args,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, 'snapshot{}/snapshot-{}.pth.tar'.format(args.gpu_num, epoch))\n",
    "    \n",
    "    # test \n",
    "    test_losses_trainphase = [0] * len(args.training_thresholds)\n",
    "    test_cnts_trainphase   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "        \n",
    "#         pretrained.train(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(input_img)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses_trainphase[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts_trainphase[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_train-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "\n",
    "    \n",
    "    net.eval()\n",
    "    test_losses = [0] * len(args.training_thresholds)\n",
    "    test_cnts   = [0.00001] * len(args.training_thresholds)   \n",
    "    for ind, data in enumerate(test_loader, 0):\n",
    "#         if ind == 1: break\n",
    "        input_img, gt_albedo, gt_shading, test_scene, img_path = data\n",
    "        input_img = Variable(input_img)\n",
    "        gt_albedo = Variable(gt_albedo)\n",
    "        gt_shading = Variable(gt_shading)\n",
    "        if use_gpu:\n",
    "            input_img = input_img.cuda(args.gpu_num)\n",
    "            \n",
    "#         pretrained.eval(); ft_pretreained = pretrained(input_img)\n",
    "        ft_test = net(input_img)\n",
    "            \n",
    "        for i,v in enumerate(ft_test):\n",
    "            if epoch < args.training_thresholds[i]: continue\n",
    "            if i == 5: s = 1\n",
    "            else: s = (2**(i+1))\n",
    "            gt = gt_albedo.data.numpy()\n",
    "            n,c,h,w = gt.shape\n",
    "            gt = gt[0,:,:,:]\n",
    "            gt = gt.transpose((1,2,0))\n",
    "            gt = cv2.resize(gt, (h//s, w//s))\n",
    "#             gt = cv2.resize(gt, (h,w))\n",
    "            \n",
    "            gt = gt.transpose((2,0,1))\n",
    "            gt = gt[np.newaxis, :]\n",
    "            gt = Variable(torch.from_numpy(gt))\n",
    "            if use_gpu: gt = gt.cuda()\n",
    "\n",
    "            loss = mse_losses[i](ft_test[i], gt)\n",
    "            \n",
    "            test_losses[i] += loss.data.cpu().numpy()[0]\n",
    "            test_cnts[i] += 1\n",
    "            v = v[0].cpu().data.numpy()\n",
    "            v = v.transpose(1,2,0)\n",
    "            if ind == 0: cv2.imwrite('snapshot{}/test-phase_test-{}-{}.png'.format(args.gpu_num, epoch, i), v[:,:,::-1]*255)\n",
    "    \n",
    "    writer.add_scalars('16M loss', {\n",
    "        'train 16M ': np.array([run_losses[0]/ run_cnts[0]]),\n",
    "        'test_trainphase 16M ': np.array([test_losses_trainphase[0]/ test_cnts_trainphase[0]]),\n",
    "        'test 16M ': np.array([test_losses[0]/ test_cnts[0]])\n",
    "    }, global_step=epoch)  \n",
    "    writer.add_scalars('8M loss', {\n",
    "        'train 8M ': np.array([run_losses[1]/ run_cnts[1]]),\n",
    "        'test_trainphase 8M ': np.array([test_losses_trainphase[1]/ test_cnts_trainphase[1]]),\n",
    "        'test 8M ': np.array([test_losses[1]/ test_cnts[1]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('4M loss', {\n",
    "        'train 4M ': np.array([run_losses[2]/ run_cnts[2]]),\n",
    "        'test_trainphase 4M ': np.array([test_losses_trainphase[2]/ test_cnts_trainphase[2]]),\n",
    "        'test 4M ': np.array([test_losses[2]/ test_cnts[2]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('2M loss', {\n",
    "        'train 2M ': np.array([run_losses[3]/ run_cnts[3]]),\n",
    "        'test_trainphase 2M ': np.array([test_losses_trainphase[3]/ test_cnts_trainphase[3]]),\n",
    "        'test 2M ': np.array([test_losses[3]/ test_cnts[3]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('1M loss', {\n",
    "        'train 1M ': np.array([run_losses[4]/ run_cnts[4]]),\n",
    "        'test_trainphase 1M ': np.array([test_losses_trainphase[4]/ test_cnts_trainphase[4]]),\n",
    "        'test 1M ': np.array([test_losses[4]/ test_cnts[4]])\n",
    "    }, global_step=epoch) \n",
    "    writer.add_scalars('merged loss', {\n",
    "        'train merged ': np.array([run_losses[5]/ run_cnts[5]]),\n",
    "        'test_trainphase merged ': np.array([test_losses_trainphase[5]/ test_cnts_trainphase[5]]),\n",
    "        'test merged ': np.array([test_losses[5]/ test_cnts[5]])\n",
    "    }, global_step=epoch) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def make_dot(var, params=None):\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\n",
    "    saved for backward in torch.autograd.Function\n",
    "    Args:\n",
    "        var: output Variable\n",
    "        params: dict of (name, Variable) to add names to node that\n",
    "            require grad (TODO: make optional)\n",
    "    \"\"\"\n",
    "    if params is not None:\n",
    "        assert isinstance(params.values()[0], Variable)\n",
    "        param_map = {id(v): k for k, v in params.items()}\n",
    "\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"10240,10240\"), format='svg')\n",
    "    seen = set()\n",
    "\n",
    "    def size_to_str(size):\n",
    "        return '('+(', ').join(['%d' % v for v in size])+')'\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if torch.is_tensor(var):\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
    "            elif hasattr(var, 'variable'):\n",
    "                u = var.variable\n",
    "                name = param_map[id(u)] if params is not None else ''\n",
    "                node_name = '%s\\n %s' % (name, size_to_str(u.size()))\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'next_functions'):\n",
    "                for u in var.next_functions:\n",
    "                    if u[0] is not None:\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\n",
    "                        add_nodes(u[0])\n",
    "            if hasattr(var, 'saved_tensors'):\n",
    "                for t in var.saved_tensors:\n",
    "                    dot.edge(str(id(t)), str(id(var)))\n",
    "                    add_nodes(t)\n",
    "    add_nodes(var.grad_fn)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.zeros(1,3,256,256))\n",
    "y = net(x.cuda())\n",
    "g = make_dot(y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Tsvg', '-O', 'net-pretrained_scale_4'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, quiet)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTARTUPINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a8aee31c1e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net-pretrained_scale_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/graphviz/files.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mrendered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/graphviz/backend.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, quiet)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Tsvg', '-O', 'net-pretrained_scale_4'], make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "g.render('net-pretrained_scale_{}'.format(pretrained_scale)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
